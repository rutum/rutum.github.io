<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="http://localhost:4000/assets/images/lightning-border.png">
<title>The Foundations of Language Models | Rutu Mulkar</title>

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>The Foundations of Language Models | Rutu Mulkar</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="The Foundations of Language Models" />
<meta name="author" content="rutum" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post describes how to document, reproduce and manage your ML experiments at scale" />
<meta property="og:description" content="This post describes how to document, reproduce and manage your ML experiments at scale" />
<link rel="canonical" href="http://localhost:4000/blog/2021/language-models/" />
<meta property="og:url" content="http://localhost:4000/blog/2021/language-models/" />
<meta property="og:site_name" content="Rutu Mulkar" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-04-24T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="The Foundations of Language Models" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"rutum"},"description":"This post describes how to document, reproduce and manage your ML experiments at scale","@type":"BlogPosting","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"},"name":"rutum"},"headline":"The Foundations of Language Models","dateModified":"2021-04-24T00:00:00-07:00","datePublished":"2021-04-24T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2021/language-models/"},"url":"http://localhost:4000/blog/2021/language-models/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Lato:wght@100;300;&family=Mukta:wght@300;500&family=Roboto:wght@100;300&display=swap" rel="stylesheet">
<link href="/assets/css/screen.css" rel="stylesheet">

<link href="/assets/css/main.css" rel="stylesheet">

<script src="/assets/js/jquery.min.js"></script>

<!-- Google analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-31232401-1', 'auto');
ga('send', 'pageview');
</script>
<style>
    ::-moz-selection { /* Code for Firefox */
        color: #fbfff1;
        background: #3066be;
    }
    
    ::selection {
        color: #fbfff1;
        background: #3066be;
    }
    </style>

</head>

<!--  -->


<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<!-- <link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet"> -->
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="http://localhost:4000/"> 
        <h2> <img class="d-inline-block align-text-center" src="http://localhost:4000/assets/images/lightning.png"/></h2>
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">
        <!-- Begin Menu -->
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                <a class="nav-link" href="/index.html">Syntax and Semantics</a>
                </li>
                <li class="nav-item">
                     <a class="nav-link" href="/books/index.html"> Book Summaries</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="/_pages/about/index.html">About</a>
                </li>
                <li class="nav-item">
                    <script src="/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search f-dlex" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<!-- <form class="d-flex">
    <input class="form-control me-2" type="search" placeholder="Search" aria-label="Search">
    <button class="btn btn-outline-success" type="submit">Search</button>
  </form> -->
  
<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="/assets/js/lunrsearchengine.js"></script>
                </li>
            </ul>
        <!-- End Menu -->
    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->


<div class="site-content">

    <div class="container">

        <!-- Site Title
        ================================================== -->
        <div class="mainheading">
        <!--     <h1 class="sitetitle">Rutu Mulkar</h1>
            <p class="lead">
                
            </p> -->
        </div>
        <!-- Content
        ================================================== -->

        <div class="main-content">
            <!-- reading progress-bar -->
<div class="progress_container">
    <progress class="progress_read" id="myBar" value="0"></progress>
</div>

<!-- Begin Article
    ================================================== -->
    <div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-1 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=The Foundations of Language Models&url=http://localhost:4000/blog/2021/language-models/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=http://localhost:4000/blog/2021/language-models/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/blog/2021/language-models/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
</div>

        </div>

        <!-- Post -->
        <!--  -->

        <div class="col-md-8 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Post Title -->
                <h1 class="posttitle">The Foundations of Language Models</h1>
                <!-- Reading time -->
                <!-- <span class="bluecolor smtext" title="Estimated read time">
    
    
        Approximate time to read: 12 min
    
</span> -->
            </div>
            <!-- Post Date -->
            <p>
                <span class="post-date"><time class="post-date" datetime="2021-04-24">Apr 24th, 2021</time> </span>            
                <br>
                <span>
                    <span class="tag"><a href="http://localhost:4000/categories#"> language models</a></span>
                </span> 

            </p>
            
            

            <!-- Rating -->
            <!--  -->
            
            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                <!-- End Toc -->
                <p><a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a> are models that are trained to predict the next word, given a set of words that are already uttered or written. e.g. Consider the sentence:  <em>“Don’t eat that because it looks…“</em></p>

<p>The next word following this will most likely be “disgusting”, or “bad”, but will probably not be “table” or “chair”. <a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a> are models that assign probabilities to sequences of words to be able to predict the next word given a sequence of words.</p>

<p>The probability of a word $w$ given some history $h$ is $p(w|h)$.</p>

<p>In this post I am going to discuss the following concepts:</p>

<ul>
  <li><a href="#n-grams">N-Grams</a>
    <ul>
      <li><a href="#working-example">Working Example</a></li>
      <li><a href="#practical-issues">Practical Issues</a></li>
    </ul>
  </li>
  <li><a href="#problems-with-language-models">Problems with language Models</a></li>
  <li><a href="#smoothing">Smoothing</a>
    <ul>
      <li><a href="#laplace-smoothing">Laplace Smoothing</a></li>
      <li><a href="#add-k-smoothing">Add-k smoothing</a></li>
      <li><a href="#backoff-and-interpolation">Backoff and Interpolation</a></li>
    </ul>
  </li>
  <li><a href="#evaluating-language-models">Evaluating Language Models</a>
    <ul>
      <li><a href="#intrinsic-evaluation">Intrinsic Evaluation</a></li>
      <li><a href="#extrinsic-evaluation">Extrinsic Evaluation</a>
        <ul>
          <li><a href="#perplexity">Perplexity</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#n-gram-efficiency-considerations">N-Gram Efficiency considerations</a></li>
  <li><a href="#references">References</a></li>
</ul>

<h2 id="n-grams">N-Grams</h2>
<p>The probability of a word $w$ given the history $h$ is defined as:</p>

\[p(w|h)\]

<p>as $h$ is several tokens long, we can rephrase it as the probability of the ${n+1}^{th}$ word $w_{n+1}$ depends on the words $w_1, w_2 \cdots w_n$.</p>

\[p(w_{n+1}|w_1, w_2, w_3 \cdots w_n)\]

<p>One way to answer this is using relative frequency counts. i.e. count the number of times we see $w_1, w_2, w_3 \cdots w_n$ and the number of times we see $w_1, w_2, w_3 \ldots w_n$ followed by $w_{n+1}$.</p>

<p><a href="https://www.mathsisfun.com/data/relative-frequency.html">Relative frequency</a> is defined as the ratio of an observed sequence to the observed sequence followed by a suffix. Using the concept of relative frequency we can get:</p>

\[P(w_{n+1}|w_1 w_2 w_3 \ldots w_n) = \frac{C(w_1 w_2 w_3 \cdots w_n w_{n+1})}{C(w_1 w_2 w_3 \ldots w_n)}\]

<p>Where $C(x_1, x_2)$ is the count or the number of times we see a pattern of token $x_1$ followed by $x_2$.</p>

<p>This approach of getting probabilities from counts works well in many cases, but if we wanted to know the joint probability of an entire sequence of words $p(w_1, w_2, w_3 \ldots w_n)$, we would have to compute - out of all the possible combinations of size $n$ how many are this exact sequence $w_1, w_2, w_3 \ldots w_n$. It fails when the size of the sequence is very long.</p>

<p>Even the entire web isn’t big enough to compute such probabilities, because there are not enough examples for every word combination even on the world wide web). For this reason, we will have to introduce clever ways of computing probability.</p>

<p>Let’s decompose this joint probability into a conditional probabilities using the chain rule of probability.</p>

<p><a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">Chain Rule of Probability</a> helps us decompose a joint probability into a conditional probability of a word given previous words. <a href="https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/">Joint Probability</a> of $n$ words is the probability of $n$ words occurring together. Using the chain rule, we can break down Equation the joint probability of a sequence of tokens into conditional probabilities. Here is how we do it:</p>

\[\begin{eqnarray}
P(w_1 w_2 w_3 \cdots w_n w_{n+1}) &amp;=&amp; P(w_1) P(w_2|w_1) P(w_3|w_1 w_2) \ldots P(w_n|w_1\ldots w_{n-1}) \nonumber \\ 
&amp;=&amp; \prod_{k=1}^{n+1} P(w_k|w_1 \ldots w_k)
\end{eqnarray}\]

<p>The chain rule still has the constraint of needing the probability of a long sequence of previous words. One idea is to approximate this using the Markov Assumption.</p>

<p><a href="https://en.wikipedia.org/wiki/Markov_property">Markov Assumption</a> says that the probability of a word only depends on the previous word and not the entire sequence of tokens preceding it. According to this assumption, we can predict a word without looking too much into the previous history of words. So, instead of working with the exact probabilities of a long sequence of preceding words, we can use a small window of preceding words. This is where we introduce a <a href="https://en.wikipedia.org/wiki/Bigram">bigram model</a> (that uses the preceding word only) a <a href="https://en.wikipedia.org/wiki/Trigram">trigram model</a> (that uses the preceding two words) to predict the probability of a sequence of tokens.</p>

<p>Using the bigram model we can simplify the probability of a sequence of tokens to the following:</p>

\[P(w_1, w_2, w_3 \cdots w_n, w_{n+1}) = \prod_{k=1}^n P(w_k|w_{k-1})\]

<p>Similarly using the trigram model we can simplify the probability of a sequence of tokens to the following:</p>

\[P(w_1, w_2, w_3 \cdots w_n, w_{n+1}) = \prod_{k=1}^n P(w_k|w_{k-1} w_{k-2})\]

<p>Now that we have simplified the RHS, we need to compute the probabilities.</p>

<p><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#:~:text=In%20statistics%2C%20maximum%20likelihood%20estimation,observed%20data%20is%20most%20probable.">Maximum Likelihood Estimation (MLE)</a> is an intuitive way of measuring the parameters of an N-gram model, by computing the counts of words or tokens that exist together, normalized by the total counts so that the output is a probability that is between 0 and 1. Relative frequency is one way of measuring the MLE. Relative frequency can be computed using equation (7) below and Figure (1) explains the equation:</p>

\[P(w_n|w_{n-1}) = \frac{C(w_{n-1}w_n)}{\sum_w C(w_{n-1}w)}\]

<p align="center">
    <img src="http://localhost:4000/assets/images/relative-frequency.png" alt="Relative Frequency" width="500" />
    <br />
    <em class="image-label">Fig 1: Relative Frequency Explained</em>
</p>

<h3 id="working-example">Working Example</h3>
<p>Let’s build a small language model using bigrams, and use our model to predict the probability of a new sentence. For our toy example, consider a tiny corpus of the following sentences:</p>

<blockquote>
  <p>&lt;s&gt; I am Sam &lt;\s&gt;</p>

  <p>&lt;s&gt; Sam I am &lt;\s&gt;</p>

  <p>&lt;s&gt; I do not like that Sam I am &lt;\s&gt;</p>
</blockquote>

<p>Here &lt;s&gt; represents the start of a sentence and &lt;\s&gt; represents the end of the sentence.</p>

<p>This corpus has the following lexicon or unique words:</p>

<table>
  <tbody>
    <tr>
      <td><strong>token</strong></td>
      <td><strong>frequency</strong></td>
    </tr>
    <tr>
      <td>I</td>
      <td>4</td>
    </tr>
    <tr>
      <td>am</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Sam</td>
      <td>3</td>
    </tr>
    <tr>
      <td>do</td>
      <td>1</td>
    </tr>
    <tr>
      <td>not</td>
      <td>1</td>
    </tr>
    <tr>
      <td>like</td>
      <td>1</td>
    </tr>
    <tr>
      <td>that</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>Computing the conditional probabilities</p>

\[P(I|&lt;s&gt;) = \frac{2}{4}\]

\[P(Sam|&lt;s&gt;) = \frac{1}{3}\]

\[P(am|I) = \frac{3}{4}\]

<p>We can continue computing probabilities for all different possibilities of bigrams, then for any new sentence such as  the following:</p>

<blockquote>
  <p>I like Sam</p>
</blockquote>

<p>We can compute the join probability of the tokens of this sentence from the conditional probability of the bigrams $P(like|I)$ and $P(Sam|like)$.</p>

<h3 id="practical-issues">Practical Issues</h3>
<ul>
  <li>Always use Log probabilities. Multiplication in linear space is addition in log space, and we will avoid numerical underflow</li>
  <li>It is typical to use trigrams instead of bigrams (although we illustrated bigrams in the example above)</li>
  <li>There are often unknown words in the sentence we want to predict the probability of, and we need to handle that.</li>
  <li>We often won’t find instances of joint probability in our corpus, and we need to account for that. E.g. in the above example we do not have an instance of ‘sam like’, and so the conditional probability of $P(Sam|like)$ will be $0$.</li>
</ul>

<h2 id="problems-with-language-models">Problems with language Models</h2>
<p>Language Models face the issue of <a href="https://en.wikipedia.org/wiki/Language_model">sparsity</a> - which means that the training corpus is limited and some perfectly acceptable English word sequences are bound to be missing from it. This means that it is possible to have several n-grams with a probability of $0$, but should actually have a non-zero probability.</p>

<p>Another issue with language models is that the vocabulary the the language model is trained on might not have seen words from the test dataset - introducing the issue of unknown words or <a href="https://groups.csail.mit.edu/sls/publications/2000/03105.pdf">out of vocabulary words(OOV)</a>. One way to deal with OOV words is to replace all words with a frequency below a certain threshold by ‘UNK’. Other ways to deal with this is to use smoothing and discounting techniques.</p>

<h2 id="smoothing">Smoothing</h2>
<p>There will always be unknown words in the test dataset that the language model will have to work with that sparsity. To keep the language model from assigning zero probabilities to unseen events, we will shave off some probability mass from some more frequent events and give it to the events that we have never seen. This process is called <a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">smoothing</a> or <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1086/handouts/cs224n-lecture2-language-models-slides.pdf">discounting</a>.</p>

<p>A few types of smoothing are:</p>
<ul>
  <li>add-1 smoothing (or <a href="****">Laplace smoothing</a>)</li>
  <li>add-k smoothing</li>
  <li>backoff and interpolation</li>
  <li>stupid backoff</li>
  <li>Kneser ney smoothing</li>
</ul>

<h3 id="laplace-smoothing">Laplace Smoothing</h3>
<p>Laplace smoothing involves adding $1$ to all of the bigram (or n-gram) counts before we normalize them into probabilities.</p>

\[P(w_i) = \frac{c_i}{N}\]

<p>If we add $1$ to each probability, and there are $V$ words in the vocabulary, we will need to add $V$ to the denominator as we add $1$ to each numerator.</p>

\[P_{laplace}(w_i) = \frac{c_i}{N+V} \label{laplace}\]

<p>In equation \ref{laplace}, $w_i$ is the $i^{th}$ word, $N$ is a normalizer (total number of words) and $V$ is the vocabulary size.</p>

<p>But instead of adding to the numerator and denominator, a better way to is change the numerator only, and show how it affects smoothing by describing an adjusted count $c^*$.</p>

\[c_i^* = (c_i + 1) \frac{N}{N+V}\]

<p>Now we can convert each adjusted count into a probability by dividing it by $N$ (the total number of tokens)</p>

<p>Although Laplace smoothing isn’t the best type of smoothing as it gives away a lot of probability mass to infrequent terms, it is still used and is practical for classification.</p>

<p>** Question: How can you use Language Models for classification? **</p>

\[P(w_n|w_{n-1}) = \frac{C(w_{n-1} w_n)}{C(w_{n-1})}\]

<p>Using Laplace Smoothing, this becomes:</p>

\[p^*_{Laplace}(w_n|w_{n-1}) = \frac{C(w_{n-1} w_n) + 1}{\sum_w C(w_{n-1}w) + V}\]

<h3 id="add-k-smoothing">Add-k smoothing</h3>
<p>Add-K smoothing is an alternative to add $1$ smoothing, where we move a bit less of the probability mass from the seen events to the unseen events.</p>

\[p^*_{add-k}(w_n|w_{n-1}) = \frac{C(w_{n-1} w_n) + k}{\sum_w C(w_{n-1}w) + kV}\]

<p>Add-K requires us to have a method for choosing our k (0.5? 0.1? 0.05?) e.g. one can optimize over a dev set or some other data source.</p>

<h3 id="backoff-and-interpolation">Backoff and Interpolation</h3>
<p><a href="https://en.wikipedia.org/wiki/Katz%27s_back-off_model">Backoff</a> is an approach for smoothing using which we only backoff to a lower order n-gram when we have zero evidence for a higher level n-gram.</p>

<p>So, we use a trigram if the evidence is sufficient, but if such a trigram does not exist, we backoff to a bigram, and if the bigram does not exist, we backoff to a unigram.</p>

<p><a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">Interpolation</a> is an approach is using a mixture of probability estimates from all the n-gram estimators. For instance, if we are looking at trigrams, we would compute its probability by combining the trigram, bigram and unigram counts.</p>

<p>Interpolation for a trigram can be defined by the following formula:</p>

\[\hat{p}(w_n|w_{n-2}w_{n-1}) = \lambda_1 p(w_n|w_{n-2}w_{n-1}) + \lambda_2 p(w_n|w_{n-1}) + \lambda_3 p(w_n)\]

<p>where $\sum_i \lambda_i = 1$</p>

<p>The values of $\lambda$ can be computed by optimizing over a heldout dataset. <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">EM Algorithm (Expectation Maximization Algorithm)</a> is an iterative learning algorithm that converges on locally optimal $\lambda$s</p>

<h2 id="evaluating-language-models">Evaluating Language Models</h2>
<p>There are 2 ways to evaluate language models:</p>
<ul>
  <li>intrinsic evaluation: Evaluation of the model as a measure of how much it improves the application that it is used in.</li>
  <li>extrinsic evaluation: Measure of the quality of the model independent of any application</li>
</ul>

<h3 id="intrinsic-evaluation">Intrinsic Evaluation</h3>
<p>For intrinsic evaluation of a language model we need to have:</p>
<ul>
  <li>training dataset</li>
  <li>test dataset</li>
  <li>held out dataset</li>
</ul>

<p>We use the language model to compute scores on the test dataset, and use the heldout and training dataset to optimize out language model.</p>

<h3 id="extrinsic-evaluation">Extrinsic Evaluation</h3>
<p>To compute extrinsic evaluation of a language model, we compute the effect of a new language model on the final end to end product that it is integrated with. Good scores during intrinsic evaluation does not always mean better scores during extrinsic evaluation, which is why both the types of evaluation are important.</p>

<h4 id="perplexity">Perplexity</h4>
<p><a href="https://en.wikipedia.org/wiki/Perplexity">Perplexity</a> is the measure of computation of the probabilities learned from the training dataset and applied on the test dataset. Perplexity is represented as $PP$ and is measured as the inverse probability of the test set, normalized by the number of words.</p>

\[\begin{eqnarray}
PP(w) &amp;=&amp; P(w_1 w_2 w_3 \ldots w_n)^{-\frac{1}{n}} \nonumber \\
&amp;=&amp; \sqrt[n]{\frac{1}{P(w_1 w_2 w_3 \ldots w_n)}} \nonumber \\
&amp;=&amp; \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i|w_1 w_2 \ldots w_{i-1})}}
\end{eqnarray}\]

<p>To compute perplexity of a bigram, we can simplify Equation (11) to the following:</p>

\[PP(w) = \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i|w_{i-1})}}\]

<p>Similarly, to compute perplexity of a bigram, we can simplify Equation (11) to the following:</p>

\[PP(w) = \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i|w_{i-1} w_{i-2})}}\]

<blockquote>
  <p>Minimizing perplexity is equivalent to maximizing the test set probability. If the perplexity is low, it means that the training data captures the probability of the test set really well.</p>
</blockquote>

<p>Another way to think about perplexity is to think of it as the weighted average branching factor of the language. The <a href="https://towardsdatascience.com/perplexity-in-language-models-87a196019a94">branching factor</a> of a language is the number of possible next words that can follow any word.</p>

<p>Intrinsic improvement in perplexity does not guarantee an extrinsic improvement in the performance of the language processing task.</p>

<h2 id="n-gram-efficiency-considerations">N-Gram Efficiency considerations</h2>
<p>When a language model uses large sets of n-grams, it is important to store the efficiently. Below are some ways to store LMs efficiently:</p>

<ul>
  <li>Words: storing words in 64 bit hash representations, and the actual words are stored on disc as string</li>
  <li>Probabilities: 4-8 bits instead of 8 byte float</li>
  <li>n-grams: Stored in reverse <a href="https://en.wikipedia.org/wiki/Trie">tries</a></li>
  <li>Approximate language models can be created using techniques such as <a href="https://en.wikipedia.org/wiki/Bloom_filter">bloom filters</a></li>
  <li>n-grams can be shrunk by pruning i.e. only storing n-grams with counts greater than some threshold.</li>
  <li>Efficient Language Models such as <a href="https://github.com/kpu/kenlm">KenLM</a>
    <ul>
      <li>Use sorted Arrays</li>
      <li>Efficiently combined probabilities and backoffs into a single value</li>
      <li>Use merge sorts to efficiently build probability tables in a minimal number of passes through a large corpus</li>
    </ul>
  </li>
</ul>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf">Chapter 3, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition</a> by Daniel Jurafsky, James H. Martin</li>
  <li><a href="https://en.wikipedia.org/wiki/Language_model">Language Models</a>, Wikipedia</li>
  <li><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1086/handouts/cs224n-lecture2-language-models-slides.pdf">Lecture 2, Language Models</a>, CS224n Stanford NLP</li>
  <li><a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">NLP Lunch Tutorial: Smoothing</a>, Bill MacCartney, Stanford NLP</li>
  <li><a href="https://www.mathsisfun.com/data/relative-frequency.html">Relative Frequency</a>, mathisfun.com</li>
</ul>

            </div>
        </div>
        <!-- End Post -->

        <!-- Related posts list-->
        <aside class="col-md-3 flex-md-unordered">
            <div class="related-posts sticky-top sticky-top-offset">
    <p class="related">Related posts:</p>
    <ul>
        
            
                
                    <li><a href="/blog/2023/ml-to-llm/">Machine Learning, Deep Learning and Large Language Models</a></li>
                
            
        
            
                
                    <li><a href="/blog/2023/Important-books-for-AI/">Important books to read for AI</a></li>
                
            
        
            
                
                    <li><a href="/blog/2021/probability-theory/">Probability Theory for Natural Language Processing</a></li>
                
            
        
            
                
                    <li class="active"><a href="/blog/2021/language-models/">The Foundations of Language Models</a></li>
                
            
        
            
                
                    <li><a href="/blog/2021/logistic-regression/">The Comprehensive Guide to Logistic Regression</a></li>
                
            
        
            
                
                    <li><a href="/blog/2021/byte-pair-encoding/">What is Byte-Pair Encoding for Tokenization?</a></li>
                
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
                
                    <li><a href="/blog/2019/manageml/">Managing Machine Learning Experiments</a></li>
                
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
                
                    <li><a href="/blog/2017/what-is-nlp/">What is Natural Language Processing (NLP)?</a></li>
                
            
        
            
                
                    <li><a href="/blog/2016/NLP-ML/">Natural Language Processing vs. Machine Learning vs. Deep Learning</a></li>
                
            
        
            
                
                    <li><a href="/blog/2015/word2vec/">Online Word2Vec for Gensim</a></li>
                
            
        
            
                
                    <li><a href="/blog/2015/statistics-for-everyone/">Understanding your Data - Basic Statistics</a></li>
                
            
        
            
                
                    <li><a href="/blog/2014/all-about-that-bayes-intro-to-probability/">An Introduction to Probability</a></li>
                
            
        
            
                
                    <li><a href="/blog/2014/build-your-own-search-engine/">Build your own search Engine</a></li>
                
            
        
            
                
                    <li><a href="/blog/2014/core-of-lucene/">The Math behind Lucene</a></li>
                
            
        
    </ul>
</div>
        </aside>

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'datasciencetoolbox'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->



<script>
    // Load document before calculating window height
            var winHeight = $(window).height(),
                docHeight = $(document).height(),
                progressBar = $('#myBar'),
                max, value;

            /* Set the max scrollable area */
            max = docHeight - winHeight;
            //progressBar.attr('max', max);

            $(document).on('scroll', function () {
                value = $(window).scrollTop()/max;
                console.log(value);
                progressBar.attr('value', value);
            });
</script>
        </div>
    </div>

    <!-- Begin Footer
    ================================================== -->
    <footer class="footer">
        <div class="container">
            <div class="row">
                <div class="col-md-12 col-sm-12 text-center text-lg-center">
                    Copyright © 2023 Rutu Mulkar. All Rights Reserved.
                </div>
            </div>
        </div>
    </footer>
    <!-- End Footer
    ================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="/assets/js/mediumish.js"></script>



<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//datasciencetoolbox.disqus.com/count.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: 
        {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: false,
        }
        
    });
</script>

  
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    CommonHTML: { linebreaks: { automatic: true } },
    "HTML-CSS": { scale: 100, linebreaks: { automatic: true } },
    TeX: { equationNumbers: {autoNumber: "all"} } });
</script>

<!-- Category filter -->
<script src="/assets/js/categorySearch.js"></script>
</body>
</html>
