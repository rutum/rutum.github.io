<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="http://localhost:4000/assets/images/lightning-border.png">
<title>Probability Theory for Natural Language Processing | Rutu Mulkar</title>

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Probability Theory for Natural Language Processing | Rutu Mulkar</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Probability Theory for Natural Language Processing" />
<meta name="author" content="rutum" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A lot of work in Natural Language Processing (NLP) such a creation of Language Models is based on probability theory. For the purpose of NLP, knowing about probabilities of words can help us predict the next word, understanding the rarity of words, analyzing and knowing when to ignore common words with respect to a given context - e.g. articles such as “the” and “a” have a very high probability of occurring in any document and add less information to the overall semantics, where as the probability of “supercalifragilisticexpialidocious” is incredibly low, but having it in a sentence provides much more semantics." />
<meta property="og:description" content="A lot of work in Natural Language Processing (NLP) such a creation of Language Models is based on probability theory. For the purpose of NLP, knowing about probabilities of words can help us predict the next word, understanding the rarity of words, analyzing and knowing when to ignore common words with respect to a given context - e.g. articles such as “the” and “a” have a very high probability of occurring in any document and add less information to the overall semantics, where as the probability of “supercalifragilisticexpialidocious” is incredibly low, but having it in a sentence provides much more semantics." />
<link rel="canonical" href="http://localhost:4000/blog/2021/probability-theory/" />
<meta property="og:url" content="http://localhost:4000/blog/2021/probability-theory/" />
<meta property="og:site_name" content="Rutu Mulkar" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-08T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Probability Theory for Natural Language Processing" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"rutum"},"description":"A lot of work in Natural Language Processing (NLP) such a creation of Language Models is based on probability theory. For the purpose of NLP, knowing about probabilities of words can help us predict the next word, understanding the rarity of words, analyzing and knowing when to ignore common words with respect to a given context - e.g. articles such as “the” and “a” have a very high probability of occurring in any document and add less information to the overall semantics, where as the probability of “supercalifragilisticexpialidocious” is incredibly low, but having it in a sentence provides much more semantics.","@type":"BlogPosting","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"},"name":"rutum"},"headline":"Probability Theory for Natural Language Processing","dateModified":"2021-05-08T00:00:00-07:00","datePublished":"2021-05-08T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2021/probability-theory/"},"url":"http://localhost:4000/blog/2021/probability-theory/","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?family=Lato:wght@100;300;&family=Mukta:wght@300;500&family=Roboto:wght@100;300&display=swap" rel="stylesheet">
<link href="/assets/css/screen.css" rel="stylesheet">

<link href="/assets/css/main.css" rel="stylesheet">

<script src="/assets/js/jquery.min.js"></script>

<!-- Google analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-31232401-1', 'auto');
ga('send', 'pageview');
</script>
<style>
    ::-moz-selection { /* Code for Firefox */
        color: #fbfff1;
        background: #3066be;
    }
    
    ::selection {
        color: #fbfff1;
        background: #3066be;
    }
    </style>

</head>

<!--  -->


<body class="layout-post">
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<!-- <link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet"> -->
		<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>


<!-- Begin Menu Navigation
================================================== -->
<nav class="navbar navbar-expand-lg navbar-light bg-white fixed-top mediumnavigation nav-down">

    <div class="container pr-0">

    <!-- Begin Logo -->
    <a class="navbar-brand" href="http://localhost:4000/"> 
        <h2> <img class="d-inline-block align-text-center" src="http://localhost:4000/assets/images/lightning.png"/></h2>
    </a>
    <!-- End Logo -->

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarMediumish" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarMediumish">
        <!-- Begin Menu -->
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                <a class="nav-link" href="/index.html">Syntax and Semantics</a>
                </li>
                <li class="nav-item">
                     <a class="nav-link" href="/books/index.html"> Book Summaries</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="/_pages/about/index.html">About</a>
                </li>
                <li class="nav-item">
                    <script src="/assets/js/lunr.js"></script>


<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>


<form class="bd-search f-dlex" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <input type="text" class="form-control launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
</form>

<!-- <form class="d-flex">
    <input class="form-control me-2" type="search" placeholder="Search" aria-label="Search">
    <button class="btn btn-outline-success" type="submit">Search</button>
  </form> -->
  
<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="/assets/js/lunrsearchengine.js"></script>
                </li>
            </ul>
        <!-- End Menu -->
    </div>

    </div>
</nav>
<!-- End Navigation
================================================== -->


<div class="site-content">

    <div class="container">

        <!-- Site Title
        ================================================== -->
        <div class="mainheading">
        <!--     <h1 class="sitetitle">Rutu Mulkar</h1>
            <p class="lead">
                
            </p> -->
        </div>
        <!-- Content
        ================================================== -->

        <div class="main-content">
            <!-- reading progress-bar -->
<div class="progress_container">
    <progress class="progress_read" id="myBar" value="0"></progress>
</div>

<!-- Begin Article
    ================================================== -->
    <div class="container">
    <div class="row">

        <!-- Post Share -->
        <div class="col-md-1 pl-0">
            <div class="share sticky-top sticky-top-offset">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=Probability Theory for Natural Language Processing&url=http://localhost:4000/blog/2021/probability-theory/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=http://localhost:4000/blog/2021/probability-theory/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/blog/2021/probability-theory/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

    </ul>
    
    <div class="sep">
    </div>
    <ul>
        <li>
        <a class="small smoothscroll" href="#disqus_thread"></a>
        </li>
    </ul>
    
</div>

        </div>

        <!-- Post -->
        <!--  -->

        <div class="col-md-8 flex-first flex-md-unordered">
            <div class="mainheading">

                <!-- Post Title -->
                <h1 class="posttitle">Probability Theory for Natural Language Processing</h1>
                <!-- Reading time -->
                <!-- <span class="bluecolor smtext" title="Estimated read time">
    
    
        Approximate time to read: 9 min
    
</span> -->
            </div>
            <!-- Post Date -->
            <p>
                <span class="post-date"><time class="post-date" datetime="2021-05-08">May 8th, 2021</time> </span>            
                <br>
                <span>
                    <span class="tag"><a href="http://localhost:4000/categories#"> probability</a></span>
                </span> 

            </p>
            
            

            <!-- Rating -->
            <!--  -->
            
            <!-- Adsense if enabled from _config.yml (change your pub id and slot) -->
            
            <!-- End Adsense -->

            
            <!-- End Featured Image -->

            <!-- Post Content -->
            <div class="article-post">
                <!-- Toc if any -->
                
                <!-- End Toc -->
                <p>A lot of work in <a href="https://rutumulkar.com/blog/2017/what-is-nlp/">Natural Language Processing (NLP)</a> such a creation of <a href="https://rutumulkar.com/blog/2021/language-models/">Language Models</a> is based on <a href="https://en.wikipedia.org/wiki/Probability_theory">probability theory</a>. For the purpose of NLP, knowing about probabilities of words can help us predict the next word, understanding the rarity of words, analyzing and knowing when to ignore common words with respect to a given context - e.g. articles such as “the” and “a” have a very high probability of occurring in any document and add less information to the overall semantics, where as the probability of “supercalifragilisticexpialidocious” is incredibly low, but having it in a sentence provides much more semantics.</p>

<p>Probability theory deals with predicting how likely something will happen. Below are some concepts to know and understand about probability. 
In this post I will discuss the following:</p>

<ul>
  <li><a href="#experiment-trial">Experiment (Trial)</a></li>
  <li><a href="#foundations">Foundations</a>
    <ul>
      <li><a href="#sample-space">Sample space</a></li>
      <li><a href="#event-space">Event Space</a></li>
      <li><a href="#disjoint-sets">Disjoint Sets</a></li>
      <li><a href="#well-founded-probability-space">Well Founded Probability Space</a></li>
    </ul>
  </li>
  <li><a href="#probability-theory">Probability Theory</a>
    <ul>
      <li><a href="#conditional-probability-and-independence">Conditional Probability and Independence</a></li>
      <li><a href="#chain-rule">Chain Rule</a></li>
      <li><a href="#independence-of-events">Independence of Events</a></li>
      <li><a href="#bayes-theorem">Bayes Theorem</a></li>
      <li><a href="#random-variables">Random Variables</a></li>
      <li><a href="#probability-mass-function">Probability Mass Function</a></li>
      <li><a href="#expectation">Expectation</a></li>
      <li><a href="#variance">Variance</a></li>
      <li><a href="#standard-deviation">Standard Deviation</a></li>
      <li><a href="#joint-probability-mass-function">Joint Probability Mass Function</a></li>
      <li><a href="#marginal-distribution">Marginal Distribution</a></li>
      <li><a href="#relative-frequency">Relative Frequency</a></li>
    </ul>
  </li>
  <li><a href="#distributions">Distributions</a>
    <ul>
      <li><a href="#continuous-distribution">Continuous distribution</a></li>
    </ul>
  </li>
  <li><a href="#maximum-likelihood-estimate">Maximum Likelihood Estimate</a>
    <ul>
      <li><a href="#bayesian-updating">Bayesian Updating</a></li>
      <li><a href="#likelihood-ratio">Likelihood Ratio</a></li>
    </ul>
  </li>
  <li><a href="#derivation">Derivation</a></li>
  <li><a href="#bayes-optimal-decision">Bayes Optimal Decision</a></li>
</ul>

<hr />

<h1 id="experiment-trial">Experiment (Trial)</h1>
<p>An experiment (also known as a trial) is a process by which an observation is made. Rolling a dice is a trial, observing the weather is a trial. The outcome of a trial is called an event.</p>

<h1 id="foundations">Foundations</h1>
<h2 id="sample-space">Sample space</h2>
<p>The sample space is a collection of all the basic outcomes or events of our experiment.  Sample space can be discrete or continuous. In NLP, the sample space of a given dataset can be the exhausting combinations of words that can occur together as bigrams. In a trial with 2 dice, the sample space is all the combinations in which the dice can have an outcome. A sample space is represented as $\Omega$. $\phi$ represents an event that can never happen.</p>

<h2 id="event-space">Event Space</h2>
<p>An event space is the set of all the possible subsets of the sample space. The size of an event space is $2^{\Omega}$. An event space is represented as $\mathcal{F}$.</p>

<h2 id="disjoint-sets">Disjoint Sets</h2>
<p>Disjoint sets are sets that do not share any events with each other. E.g. in NLP if 2 datasets have no common vocabulary or vocabulary sequences, they are disjoint from one another. $A_j$ and $A_k$ are disjoint sets if they satisfy the following condition:</p>

\[A_j \in \mathcal{F} (A_j \cap A_k = \phi, j \ne k)\]

\[P(\cup_{j=1}^{\inf}A_j) = \sum_{j=1}^{\inf} P(A_j)\]

<h2 id="well-founded-probability-space">Well Founded Probability Space</h2>
<p>A well founded probability space contains:</p>
<ul>
  <li>Sample space $\Omega$</li>
  <li>a $\sigma$ field of events $\mathcal{F}$</li>
  <li>A probability function (where all the individual probabilities sum to $1$)</li>
</ul>

<h1 id="probability-theory">Probability Theory</h1>
<h2 id="conditional-probability-and-independence">Conditional Probability and Independence</h2>
<p>Conditional Probability is the heart of <a href="">Naive Bayes</a> algorithm. Conditional Probability measures the probability of an event given another event has occurred.</p>

<p align="center">
    <img src="https://rutumulkar.com/assets/images/cond_prob.png" alt="Image of Conditional Probability" width="400" />
    <br />
    <em class="image-label">Fig 1: Conditional Probability</em>
</p>

\[P(A|B) = \frac{P(A \cap B)}{P(B)}\]

<p>A simpler way of understanding conditional probability is the following: If event $B$ has definitely happened, how likely is it for event $A$ to also happen? This is answered by the fraction of times $A$ happens when $B$ happens.</p>

<h2 id="chain-rule">Chain Rule</h2>
<p>Chain rule of probabilities is used for Markov Models. According to chain rule, if we have 2 events $A$ and $B$, the probability of $A \cap B$ can be written as below:</p>

\[P(A \cap B) = P(A|B) P(B)\]

<p>When events A and B occur together they are written as $P(A \cap B)$ or $P(A B)$</p>

<p>For $n$ events, $A_1, A_2, \ldots, A_n$, the chain rule is:</p>

\[P(A_1 \cap A_2 \ldots \cap A_n) = P(A_1) P(A_2|A_1) \ldots P(A_n|\cap_{i=1}^{n-1}A_i)\]

<p>Another notation is:</p>

\[P(A_1 A_2 \ldots A_n) = P(A_1) P(A_2|A_1) \ldots P(A_n|\prod_{i=1}^{n-1}A_i)\]

<p>Applying this to NLP, we can compute the probability of the sentence “Pete is happy” by computing:</p>

\[P(Pete, is, happy) = P(Pete) P(is| Pete) P(happy| is, Pete)\]

<p>For very long sentences, such joint probabilities are very hard to compute, e.g. the probability of the sentence “I am happy because I ate salad for lunch on Wednesday” depends on the frequency of this exact sentence in the corpus. It is quite possible that this exact sentence does not exist in our training dataset, cut the component words do exist. In which case it is helpful to make an independence assumption.</p>

<h2 id="independence-of-events">Independence of Events</h2>

<p>Two events $A$ and $B$ are independent of each other if $P(A B) = P(A) P(B)$. That means that there is no overlap between the two events with respect to Figure 1.</p>

<p>When applying the independence assumption, $P(A|B) = P(A)$ because the presence of $B$ does not affect the probability of $A$ at all.</p>

<p>Two events $A$ and $B$ are conditionally independent given an event $C$ where $P(C) &gt; 0$ if:</p>

\[P(A \cap B|C) = P(A|C) P(B|C)\]

<h2 id="bayes-theorem">Bayes Theorem</h2>
<p>According to Bayes theorem:</p>

\[P(B|A) = \frac{P(A|B) P(B)}{P(A)}\]

<p>The denominator is a normalizing factor, and it helps produce a probability function.</p>

<p>If we are simply interested in which event is most likely given A, we can ignore the normalizing constant because it is the same value for all events.</p>

<p><strong>Bayes theorem is central to the noisy channel model</strong></p>

<h2 id="random-variables">Random Variables</h2>
<p>Random Variables map outcomes of random processes to numbers. Random variables are different from regular variables. Random variables can have many values, but regular variables can be solved for a small set of values. The probability $P(random|condition)$ helps with the mathematics notation of probability.</p>

<p>The value held by a random variable can be discrete of continuous. Discrete is separate values e.g. 1, 2, 3, where as continuous is when the random variable can held any value within an interval e.g. between 0 and 1.</p>

<h2 id="probability-mass-function">Probability Mass Function</h2>
<p>Probability Mass Function (or PMF) of a discrete random variable $X$ provides the probabilities $P(X=x)$ for all the possible values of $x$.</p>

<h2 id="expectation">Expectation</h2>
<p>Expectation tells us what is the most likely outcome to expect. Expectation is the mean or average of the random variable:</p>

\[E(X) = \sum_{x}xP(x)\]

<p>Calculating Expectation is central to Information Theory.</p>

<h2 id="variance">Variance</h2>
<p>The variance of a random variable is a measure of whether the values of the random variable tend to be consistent over trials or to vary a lot. Variance is represented as $\sigma^2$</p>

\[\begin{eqnarray}
var(X) &amp;=&amp; E((X-E(X))^2 \nonumber\\
 &amp;=&amp; E(X^2)-E^2(X))
\end{eqnarray}\]

<h2 id="standard-deviation">Standard Deviation</h2>
<p>Standard Deviation is the square root of variance. It is represented as $\sigma$.</p>

<h2 id="joint-probability-mass-function">Joint Probability Mass Function</h2>
<p>The joint probability of two events $x$ and $y$ is represented as:</p>

\[P(x,y) = P(X=y, Y=y)\]

<h2 id="marginal-distribution">Marginal Distribution</h2>
<p>Marginal PMFs total up the probability masses for the values of each variable separately.</p>

\[P_X(x) = \sum_y P(x,y)\]

\[P_Y(y) = \sum_x P(x,y)\]

<h2 id="relative-frequency">Relative Frequency</h2>
<p>The proportion of times a certain outcome occurs is called the relative frequency of the outcome.</p>

<p>$P$ - probability function</p>

<p>$p$ - probability mass function</p>

<p>We take a parametric approach to estimate the probability function in language.</p>

<p>Non-parametric approaches are used in classification, or when the underlying distribution of the data is unknown.</p>

<h1 id="distributions">Distributions</h1>
<p>The types of functions for probability mass functions are called distributions</p>

<ul>
  <li><strong>Discrete Distributions</strong>: Binomial discrete distributions are a series of trails with 2 outcomes only.
    <ul>
      <li>Multi-nominal Distribution: A special case of binomial distribution is <strong>Multi-nominal Distribution</strong> where each trial has more than 2 basic outcomes</li>
      <li>Bernoulli distribution: A special case of discrete distributions where there is only one trial.</li>
    </ul>
  </li>
  <li><strong>Continuous distributions</strong> Continuous distributions are also known as Normal distributions</li>
</ul>

<h2 id="continuous-distribution">Continuous distribution</h2>
<p>A continuous distribution is also known as a Normal Distribution or a Bell Curve. We also refer to them as Gaussians (often used in clustering). A Gaussian is represented as:</p>

\[n(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} e^{\frac{-(x-\mu)}{2\sigma^2}}\]

<p>Where $\sigma$ is the standard deviation and $\mu$ is the mean.</p>

<p>It is also the probability density of observing a single data point $x$, that is generated from a gaussian distribution.</p>

<h1 id="maximum-likelihood-estimate">Maximum Likelihood Estimate</h1>
<p>Maximum Likelihood Estimate or MLE helps us identify which values of $\mu$ and $\sigma$ should we use that produce a curve that explains or covers all the data points. It is the way to determine the most likely outcome to a set of trials.</p>

<h2 id="bayesian-updating">Bayesian Updating</h2>
<p>The process of using prior to get posterior, posterior becomes the new prior, as new data comes.</p>

\[P(\Theta|data) = P(data|\Theta) \text{ } P(\Theta)\]

<p>Here $\Theta$ is what we are interested in and what we are trying to estimate. It represented the set of parameters. If we are trying to estimate the parameters of a gaussian distribution then $\Theta$ represents both the mean $\mu$ and the standard deviation $\sigma$ and $\Theta = \{\mu, \sigma\}$</p>

<p>Here $P(\Theta|data)$ is the posterior and $P(\Theta)$ is the prior.</p>

<p>Prior belief is computed as the following:</p>

\[P(s|\mu_m) = m^i (1-m)^j\]

<p>Here:</p>
<ul>
  <li>$i$ = outcome of 1 counts</li>
  <li>$j$ = outcome of 2 counts</li>
  <li>$\mu_m$ is the model that asserts the outcome</li>
  <li>$s$ is a sequence of observations</li>
</ul>

<h2 id="likelihood-ratio">Likelihood Ratio</h2>
<p>Ratio computed between 2 models to see which of them is more likely to occur.</p>

<p>People often take log likelihood ratio and see it the result is $&gt;$ 1 or $&lt;$ 1</p>

<p>The ratio to determine which theory is most likely to occur given a sequence of events $s$.</p>

<p>Let $v$ be the first theory and $\mu$ be the seond theory.</p>

\[\frac{P(\mu|s)}{P(v|s)} = \frac{P(s|\mu) P(\mu)}{P(s|v)P(v)}\]

<p>If the ratio is $&gt;1$ we prefer theory $\mu$ (the numerator).
If the ratio is $&lt;1$ we prefer theory $v$ (the denominator).</p>

<h1 id="derivation">Derivation</h1>
<p>Derivation is the process of finding the maxima and minima of functions.</p>

<p>Computing the partial derivative WRT $\mu$ and then setting the equation to $0$ gives us the MLE fo $\mu$</p>

<h1 id="bayes-optimal-decision">Bayes Optimal Decision</h1>
<p>When we pick the best decision theory out of all the available theories that could explain the data, we make the Bayes Optimal Decision.</p>

            </div>
        </div>
        <!-- End Post -->

        <!-- Related posts list-->
        <aside class="col-md-3 flex-md-unordered">
            <div class="related-posts sticky-top sticky-top-offset">
    <p class="related">Related posts:</p>
    <ul>
        
            
                
                    <li><a href="/blog/2023/ml-to-llm/">Machine Learning, Deep Learning and Large Language Models</a></li>
                
            
        
            
                
                    <li><a href="/blog/2023/Important-books-for-AI/">Important books to read for AI</a></li>
                
            
        
            
                
                    <li class="active"><a href="/blog/2021/probability-theory/">Probability Theory for Natural Language Processing</a></li>
                
            
        
            
                
                    <li><a href="/blog/2021/language-models/">The Foundations of Language Models</a></li>
                
            
        
            
                
                    <li><a href="/blog/2021/logistic-regression/">The Comprehensive Guide to Logistic Regression</a></li>
                
            
        
            
                
                    <li><a href="/blog/2021/byte-pair-encoding/">What is Byte-Pair Encoding for Tokenization?</a></li>
                
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
                
                    <li><a href="/blog/2019/manageml/">Managing Machine Learning Experiments</a></li>
                
            
        
            
        
            
        
            
        
            
        
            
        
            
        
            
                
                    <li><a href="/blog/2017/what-is-nlp/">What is Natural Language Processing (NLP)?</a></li>
                
            
        
            
                
                    <li><a href="/blog/2016/NLP-ML/">Natural Language Processing vs. Machine Learning vs. Deep Learning</a></li>
                
            
        
            
                
                    <li><a href="/blog/2015/word2vec/">Online Word2Vec for Gensim</a></li>
                
            
        
            
                
                    <li><a href="/blog/2015/statistics-for-everyone/">Understanding your Data - Basic Statistics</a></li>
                
            
        
            
                
                    <li><a href="/blog/2014/all-about-that-bayes-intro-to-probability/">An Introduction to Probability</a></li>
                
            
        
            
                
                    <li><a href="/blog/2014/build-your-own-search-engine/">Build your own search Engine</a></li>
                
            
        
            
                
                    <li><a href="/blog/2014/core-of-lucene/">The Math behind Lucene</a></li>
                
            
        
    </ul>
</div>
        </aside>

    </div>
</div>
<!-- End Article
================================================== -->

<!-- Begin Comments
================================================== -->

    <div class="container">
        <div id="comments" class="row justify-content-center mb-5">
            <div class="col-md-8">
                <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'datasciencetoolbox'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>

            </div>
        </div>
    </div>

<!--End Comments
================================================== -->

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->



<script>
    // Load document before calculating window height
            var winHeight = $(window).height(),
                docHeight = $(document).height(),
                progressBar = $('#myBar'),
                max, value;

            /* Set the max scrollable area */
            max = docHeight - winHeight;
            //progressBar.attr('max', max);

            $(document).on('scroll', function () {
                value = $(window).scrollTop()/max;
                console.log(value);
                progressBar.attr('value', value);
            });
</script>
        </div>
    </div>

    <!-- Begin Footer
    ================================================== -->
    <footer class="footer">
        <div class="container">
            <div class="row">
                <div class="col-md-12 col-sm-12 text-center text-lg-center">
                    Copyright © 2023 Rutu Mulkar. All Rights Reserved.
                </div>
            </div>
        </div>
    </footer>
    <!-- End Footer
    ================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts
================================================== -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<script src="/assets/js/mediumish.js"></script>



<script src="/assets/js/ie10-viewport-bug-workaround.js"></script> 


<script id="dsq-count-scr" src="//datasciencetoolbox.disqus.com/count.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: 
        {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: false,
        }
        
    });
</script>

  
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    CommonHTML: { linebreaks: { automatic: true } },
    "HTML-CSS": { scale: 100, linebreaks: { automatic: true } },
    TeX: { equationNumbers: {autoNumber: "all"} } });
</script>

<!-- Category filter -->
<script src="/assets/js/categorySearch.js"></script>
</body>
</html>
