
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "http://localhost:4000/_pages/about/",
    "title": "About me",
    "body": "							Summary: 																 My name is Rutu Mulkar and I am a Sr. Manager in Applied Science at Amazon. Previously, I was the Founder of Hunchera and the founder of Ticary Solutions (acquired in the summer of 2019). 			 I have a Ph. D. in Natural Language Processing, during which I contributed to IBM Watson that defeated humans at Jeopardy! (news story). 			I love languages - both Natural Languages and Programming languages. I speak English, Hindi and Gujarati fluently and bits of Korean and Spanish. 			I started out as a Computer Engineer and later I got my Masters and Doctorate in Computer Science from University of Southern California (USC).  I was a Postdoctoral Research Fellow at the San Diego Supercomputer Center, at University of California, San Diego (UCSD). 			 																																																																Program Committee and Organizer: 						IJCAI-ECAI-18, Stockholm, Sweden, July 13-19, 2018			Association for the Advancement of Artificial Intelligence AAAI 2017			COLING 2014 Information Retrieval and Question Answering Track			The 5th International Symposium on Semantic Mining in Biomedicine (SMBM), 2012			International Workshop on Machine Reading for Social Media Content Analytics(IMRSMCA'12) 			Information Retrieval and Knowledge Discovery in Biomedical Text symposium, at the AAAI 2012 Fall Symposia series. 			International Journal on Semantic Web and Information Systems (IJSWIS), Special Edition on  Web Scale Knowledge Extraction , 2012, Ed. Aditya Kalyanpur, James Fan, Chris Welty			Natural Language Processing in the Web Era, Volume 6. 2, December 2012, Ed. Roberto Basili, Bernardo Magnini			Journal on BMC Medical Informatics and Decision Making 2012			Empirical Methods in Natural Language Processing (EMNLP-2011)			Webscale Knowledge Extraction co-located with KCAP 2011			Learning by Reading in the Real World (LERREW 2011) co-located with Conference of the Italian Association for Artificial Intelligence (AI*IA 2011)			Learning by Reading and its Applications in Intelligent Question-Answering (FAM-LbR/KRAQ 2011), co-located with IJCAI 2011			Conference on Intelligent Text Processing and Computational Linguistics CICLing 2011			Association of Computational Linguistics (ACL 2011)			Association for the Advancement of Artificial Intelligence (AAAI 2011)			Student Summit @ Machine Reading Phase 3 Kickoff meeting 2011			Artificial Intelligence Journal (AIJ 2010)			Workshop on Formalisms and Methodology for Learning by Reading (FAM-LbR 2010), co-located with NAACL 2010			Workshop on Learning by Reading and its Applications in Intelligent Question-Answering (FAM-LbR/KRAQ 2011), co-located with IJCAI 2011			Workshop on Formalisms and Methodology for Learning by Reading (FAM-LbR 2010), co-located with NAACL 2010								"
    }, {
    "id": 2,
    "url": "http://localhost:4000/book-list",
    "title": "Reading List",
    "body": "Here is a list of books that I encourage everyone to read (cover to cover, or a summary) at least once in their lives. Career:  Rise by Patty Azarello Deep Work: Rules for Focused Success in a Distracted World, by Cal Newport Good to Great, by Jim Colins Getting to Yes, by Roger Fisher and William Ury Thinking Fast and Slow, by Daniel Kahneman So Good They Cant Ignore You, by Cal Newport I’m OK - You’re OK, by Thomas A. Harris, MDPhilosophy:  The Subtle Art of Not Giving a F*ck, by Mark Manson Rising Strong, by Brené Brown Braving the Wilderness, by Brené Brown Quiet - The Power of Introverts in a World That Can’t Stop Talking, by Susan Cain Influence - The Psychology of Persuasion, by Robert B. Cialdini Blink: The Power of Thinking Without Thinking, by Malcolm Gladwell Curious: The Desire to Know and Why Your Future Depends On It, by Ian Leslie The Storytelling Animal: How Stories Make Us Human, by Jonathan Gottschall Rework, by Jason Fried and David Heinemeier Hansson (DHH) Evil Plans, by Hugh Macleod How to Win Friends and Influence People, by Dale Carnegie The Power of Less: The Fine Art of Limiting Yourself to the Essential…in Business and in Life, by Leo BabautaTime and Space:  A Universe from Nothing, by Lawrence M. Krauss A Brief History of Time, by Stephen Hawking"
    }, {
    "id": 3,
    "url": "http://localhost:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 4,
    "url": "http://localhost:4000/",
    "title": "Syntax and Semantics",
    "body": "  Syntax and Semantics: I am maintaining a list of interesting foundational NLP concepts here.                                                                 Jul 13th, 2023                                              llm                    ml                    deep-learning                               Machine Learning, Deep Learning and Large Language Models            :       Starting the early 2000s, the improvements in hardware to support deep learning networks has lead to a leap in modern deep learning approaches. Deep Learning (Hinton et al. 2006), (Bengio. . . :                       Approximate time to read: 41 min                                                                            Jun 9th, 2023                                              nlp                    deep-learning                    ai                    machine-learning                               Important books to read for AI            :       Here is a list of books that I highly recommend to bolster your foundational knowledge in ML and AI::                       Approximate time to read: 1 min                                                                            May 8th, 2021                                              probability                               Probability Theory for Natural Language Processing            :       A lot of work in Natural Language Processing (NLP) such a creation of Language Models is based on probability theory. For the purpose of NLP, knowing about probabilities of words. . . :                       Approximate time to read: 9 min                                                                            Apr 24th, 2021                                              language models                               The Foundations of Language Models            :       Language Models are models that are trained to predict the next word, given a set of words that are already uttered or written. e. g. Consider the sentence: “Don’t eat that. . . :                       Approximate time to read: 12 min                                                                            Apr 23rd, 2021                                              nlp                    classification                               The Comprehensive Guide to Logistic Regression            :       In Natural Language Processing (NLP) Logistic Regression is the baseline supervised ML algorithm for classification. It also has a very close relationship with neural networks (If you are new to. . . :                       Approximate time to read: 16 min                                                                            Jan 28th, 2021                                              tokenization                               What is Byte-Pair Encoding for Tokenization?            :       Tokenization is the concept of dividing text into tokens - words (unigrams), or groups of words (n-grams) or even characters. Morphology traditionally defines morphemes as the smallest semantic unit. e. g. . . . :                       Approximate time to read: 3 min                                                                                                                                        Jul 24th, 2019                                              machine learning                    experiment management                               Managing Machine Learning Experiments            :       I run Machine Learning experiments for a living and I run an average of 50 experiments per stage of a project. For each experiment I write code for training models,. . . :                       Approximate time to read: 3 min                                                                                                                                        Dec 12th, 2017                                              nlp introduction                               What is Natural Language Processing (NLP)?            :       Last year I wrote a highly popular blog post about Natural Language Processing, Machine Learning, and Deep Learning. :                       Approximate time to read: 7 min                                                                            Jun 8th, 2016                                              nlp introduction                    machine learning                    deep learning                               Natural Language Processing vs. Machine Learning vs. Deep Learning            :       NLP, Machine Learning and Deep Learning are all parts of Artificial Intelligence, which is a part of the greater field of Computer Science. The following image visually illustrates CS, AI. . . :                       Approximate time to read: 4 min                                                                            Aug 22nd, 2015                                              word2vec                    representation learning                               Online Word2Vec for Gensim            :       Word2Vec [1] is a technique for creating vectors of word representations to capture the syntax and semantics of words. The vectors used to represent the words have several interesting features,. . . :                       Approximate time to read: 14 min                                                                            May 10th, 2015                                              statistics                               Understanding your Data - Basic Statistics            :       Have you ever had to deal with a lot of data, and don’t know where to start? If yes, then this post is for you. In this post I will. . . :                       Approximate time to read: 10 min                                                                            Jul 7th, 2014                                              probability                               An Introduction to Probability            :       This post is an introduction to probability theory. Probability theory is the backbone of AI, and the this post attempts to cover these fundamentals, and bring us to Naive Bayes,. . . :                       Approximate time to read: 11 min                                                                            May 20th, 2014                                              information retrieval                    lucene                               Build your own search Engine            :       In this post, I will take you through the steps for calculating the $tf \times idf$ values for all the words in a given document. To implement this, we use. . . :                       Approximate time to read: 6 min                                                                            Apr 20th, 2014                                              information retrieval                    search                    lucene                               The Math behind Lucene            :       Lucene is an open source search engine, that one can use on top of custom data and create your own search engine - like your own personal google. In this. . . :                       Approximate time to read: 6 min                           "
    }, {
    "id": 5,
    "url": "http://localhost:4000/books/",
    "title": "Book Summaries",
    "body": "1234567891011&lt;div class= section-title &gt;  &lt;h2&gt;&lt;span&gt;Book Summaries&lt;/span&gt;&lt;/h2&gt;&lt;/div&gt; &lt;!-- &lt;div class= text-center flex-wrap mt-4  role= group  aria-label= Categories &gt;&lt;button id= anyCategory  class= categoryButton btn btn-outline-secondary btn-sm shadow-none mb-1 clicked  type= button  onclick= showAllCategories() &gt;All&lt;/button&gt;  &lt;button id= categoryButton  class= categoryButton btn btn-outline-secondary btn-sm shadow-none mb-1  type= button  onclick= filterCategories(this. value)  value= Books &gt;Books&lt;/button&gt;  &lt;button id= categoryButton  class= categoryButton btn btn-outline-secondary btn-sm shadow-none mb-1  type= button  onclick= filterCategories(this. value)  value= blog &gt;blog&lt;/button&gt;&lt;/div&gt; –&gt; I am maintining a list of interesting non-fiction books here. &lt;/p&gt;                                                                                                                                                                                                     Approximate time to read: 3 min                  Willpower              :       In this book Roy Baumeister and John Tierney talk about Willpower, why it is important, how successful people have implemented the power of willpower in their own lives, and how. . . :                                                                                                                                                                Approximate time to read: 11 min                  The Hard Thing about Hard Things              :       This book is one of the first books to read as an entrepreneur. Ben Horowitz candidly talks about the struggles, and hard things about starting, running and growing companies. Here. . . :                                                                                                                                                                Approximate time to read: 17 min                  How to Win Friends and Influence People              :       This book is a timeless classic by Dale Carnegie. In this book Carnegie goes into great depth about how to win hearts of everyone around you - employees, employers, students,. . . :                                                                                                                                                                Approximate time to read: 10 min                  Start with Why              :       This book is by Simon Sinek. In this book Sinek talks about finding the elusive - “why” - which is the primordial reason for developing brand loyalty, making friends, selling,. . . :                                                                                                                                                                Approximate time to read: 13 min                  How to Fail at Almost Everything and Still Win Big              :       This book is by Scott Adams - the Author of Dilbert comics. Adams does a phenomenal job of break down the psychology behind failing and winning, and simple techniques to. . . :                                                                                                                                                                Approximate time to read: 5 min                  So Good They Can't Ignore You              :       Author’s profile and book description: So Good that they can’t ignore you, by Cal Newport:                                                                                                                                                                          Approximate time to read: 7 min                  A Universe from Nothing              :       Lawrence Krauss answers questions about the universe - where it came from, what was there before, what will the future bring, and if it all really came from nothing. :                                                                                                                                                                Approximate time to read: 7 min                  Rising Strong              :       In Rising Strong, Brené Brown talks about how we often fall when we try, and goes into great detail about how we can change the narratives in our mind, and. . . :                                                                                                                                                                Approximate time to read: 11 min                  Braving the Wilderness              :       This book is about our struggle to try to “belong” to something, what the belonging means. This book inspires the reader to belong to themselves, and giving yourselves that place. . . :                                                                                                                                                                Approximate time to read: 7 min                  Influence - The Psychology of Persuasion              :       This book is about how our brain loves shortcuts, and other people can make us follow them by pushing our emotional buttons. These manipulators are everywhere in the industry, they’re. . . :                                                                                                                                                                Approximate time to read: 7 min                  Quiet - The Power of Introverts in a World That Can't Stop Talking              :       Quiet - The Power of Introverts in a World That Can’t Stop Talking, by Susan Cain:                                                                                                                                                                Approximate time to read: 10 min                  The Subtle Art of Not Giving a F*ck              :       This book is about how we need to be careful about what we give importance to in our lives. How do we pick and chose the right things to give. . . :                                                                                                       "
    }, {
    "id": 6,
    "url": "http://localhost:4000/nlp-bytes",
    "title": "NLP Bytes",
    "body": "Large Language Models (LLMs):  Knowledge Distillation Transformer Architecture Decoder Only Models Multimodal LLM Text to ImageVector Semantics and Embeddings:  Distributional hypothesis Vector Semantics Contextualized Word Representations Self-supervised Learning Lexical SemanticsLexical Semantics:  Lemma Word Forms Word Sense Synonymy Propositional Meaning Word Similarity Relatedness Semantic Field Topic Models Hypernymy Autonymy Meronymy Semantic Frames and Roles ConnotationsVectors:  Vector semantics Co-occurrence Matrix Term Document Matrix Vector Space Vector for a document Information Retrieval Term - term matrix One Hot Encoding TF*IDF PMI (Pointwise Mutual Information) PPMI (Positive Pointwise Mutual Information) Laplace SmoothingCosine Similarity:  What is Cosine similarity Issues with raw dot product Normalized Dot Product Unit VectorEmbeddings:  Sparse vs Dense vectors Word2Vec and Logistic Regression Word2Vec (Skip Gram with Negative Sampling SGNS) Word2Vec (Cumulative Bag of Words CBOW) Semantic Properties of Embeddings Relationships:     First Order Co-Occurrence - Syntagmatic Association   Second Order Co-occurrence - Paradigmatic Association   Relevant Concepts:  GloVe Embeddings - method based on ratios of word co-occurrence probabilities Fasttext - computing word embeddings on bag or character (or subwords) LSI (Latent Semantic Indexing) LDA (Latent Dirichlet Allocation) LSA (Latent Semantic Analysis) SVD (Singular Value Decomposition) PLSI (Probabilistic Latent Semantic Indexing) NMF (Non-Negative Matrix Factorization) Contextual Embeddings - ELMO, BERT. The representation of a word is contextual - a function of the entire sentence"
    }, {
    "id": 7,
    "url": "http://localhost:4000/publications",
    "title": "Publications",
    "body": "  Publications:   2014       (Book Chapter) Ovchinnikova E. , Montazeri N. , Alexandrov T. , Hobbs J. R. , McCord M. C. , Mulkar-Mehta R. (2014) Abductive Reasoning with a Large Knowledge Base for Discourse Processing. In: Bunt H. , Bos J. , Pulman S. (eds) Computing Meaning. Text, Speech and Language Technology, vol 47. Springer, Dordrecht. https://doi. org/10. 1007/978-94-007-7284-7_7     2013       (Book Chapter) Hobbs, J. R. , &amp; Mulkar-Mehta, R. (2013). Toward a Formal Theory of Information Structure. In Evolution of Semantic Systems (pp. 101-126). Springer, Berlin, Heidelberg.      2012       (Book) Mulkar-Mehta, R. , Granular Causality: Discovering Causality in Natural Language, LAP LAMBERT Academic Publishing (2012), (ISBN 10:3847324470, ISBN 13:978-3847324478) Available from Amazon. com     (Conference Paper) Silgard, E. , Tharp, M. , &amp; Mulkar-Mehta, R. (2012). Temporal Relation Extraction from Medical Discharge Summaries. 2012 IEEE Second International Conference on Healthcare Informatics, Imaging and Systems Biology, 132-132, (HISB 2012), UCSD, La Jolla, CA     2011       Mulkar-Mehta, R. , Welty, C. , Hobbs, J. R. , Hovy, E. H. , Using Part-of Relations to Discover Causality, Machine Reading Phase 3 Kickoff meeting, 2011, Seattle, WA     Mulkar-Mehta, R. , Welty, C. , Hobbs, J. R. , Hovy, E. H. , Granularity in Natural Language Discourse, Reasoning with Text, 2011 Workshop, at ICT, Los Angeles, CA     (Ph. D. Thesis) Mulkar-Mehta, R. (2011). Granular causality for learning by reading. University of Southern California.      (Journal Paper) Pan, F. , Mulkar-Mehta, R. , &amp; Hobbs, J. (2011). Annotating and Learning Event Durations in Text. Computational Linguistics, 37, 727-752.      (Conference Paper) Ovchinnikova, E. , Montazeri, N. , Alexandrov, T. , Hobbs, J. , McCord, M. , &amp; Mulkar-Mehta, R. (2011). Abductive Reasoning with a Large Knowledge Base for Discourse Processing. In Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011)     (Conference Paper) Mulkar-Mehta, R. , Gordon, A. , Hobbs, J. R. , &amp; Hovy, E. Causal markers across domains and genres of discourse. In Proceedings of the sixth international conference on Knowledge capture, pp. 183-184. 2011. [poster]     (Conference Paper) Mulkar-Mehta, R. , Hobbs, J. R. , &amp; Hovy, E. (2011). Granularity in natural language discourse. In Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011).     (Conference Paper) Mulkar-Mehta, R. , Hobbs, J. R. , &amp; Hovy, E. H. (2011, March). Applications of Discovery of Granularity Structures in Natural Language Discourse. In AAAI Spring Symposium: logical formalizations of commonsense reasoning.     (Conference Paper) Mulkar-Mehta, R. , Welty, C. , Hobbs, J. , &amp; Hovy, E. (2011, March). Using Part-Of Relations for Discovering Causality. In Twenty-Fourth International FLAIRS Conference. (Nominated for Best Student Paper Award [PPT]     2010       Mulkar-Mehta, R. , Jerry Hobbs, Causal Granularity: Answering  How  Events Happen, At the Abductive Inference ARO MURI 2010 Workshop     2009       Mulkar-Mehta, R. , Hobbs, J. , Liu, C. , &amp; Zhou, X. (2009). Discovering Causal and Temporal Relations in Biomedical Texts. AAAI Spring Symposium: Learning by Reading and Learning to Read.      2008         Hobbs, J. R. , &amp; Mulkar-Mehta, R. (2008). Using Abduction for Video-Text Coreference. In Proceedings of BOEMIE 2008 Workshop on Ontology Evolution and Multimedia Information Extraction.      2007           Barker, K. , Agashe, B. , Chaw, S. , Fan, J. , Friedland, N. , Glass, M. R. , Hobbs, J. , Hovy, E. , Israel, D. , Kim, D. S. , Mulkar-Mehta, R. , Patwardhan, S. , Porter, B. , Tecuci, D. , &amp; Yeh, P. Z. (2007). Learning by Reading: A Prototype System, Performance Baseline and Lessons Learned. AAAI.       Mulkar-Mehta, R. , Hobbs, J. , &amp; Hovy, E. (2007). Learning from Reading Syntactically Complex Biology Texts. AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning.       Pan, F. , Mulkar-Mehta, R. , &amp; Hobbs, J. (2007). Modeling and Learning Vague Event Durations for Temporal Reasoning. In Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI), Nectar Track, pp. 1659-1662, Vancouver, Canada.       Mulkar, R. , Hobbs, J. , Hovy, E. , Chalupsky, H. , &amp; Lin. C. Y. Learning by reading: Two experiments. In Proceedings of IJCAI 2007 workshop on Knowledge and Reasoning for Answering Questions, pp. 287-296. 2007.        2006       Pan, F. , Mulkar-Mehta, R. , &amp; Hobbs, J. R. (2006, July). Learning event durations from event descriptions. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (pp. 393-400). (Acceptance rate: 23%)     Pan, F. , Mulkar, R. , &amp; Hobbs, J. R. (2006, May). An Annotated Corpus of Typical Durations of Events. In LREC (pp. 77-82).        Pan, F. Mulkar, R. , Hobbs, J. R. Extending TimeML with Typical Durations of Events, In Proceedings of Annotating and Reasoning about Time and Events workshop at the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), 2006   "
    }, {
    "id": 8,
    "url": "http://localhost:4000/service",
    "title": "Service",
    "body": "Other  Seattle NLP meetup Organizer (Over 800 participants)Program Committee and Reviewer:   IJCAI-ECAI-18, Stockholm, Sweden, July 13-19, 2018  Association for the Advancement of Artificial Intelligence AAAI 2017  COLING 2014 Information Retrieval and Question Answering Track  The 5th International Symposium on Semantic Mining in Biomedicine (SMBM), 2012  International Workshop on Machine Reading for Social Media Content Analytics(IMRSMCA'12)   Information Retrieval and Knowledge Discovery in Biomedical Text symposium, at the AAAI 2012 Fall Symposia series.   International Journal on Semantic Web and Information Systems (IJSWIS), Special Edition on  Web Scale Knowledge Extraction , 2012, Ed. Aditya Kalyanpur, James Fan, Chris Welty  Natural Language Processing in the Web Era, Volume 6. 2, December 2012, Ed. Roberto Basili, Bernardo Magnini  Journal on BMC Medical Informatics and Decision Making 2012  Empirical Methods in Natural Language Processing (EMNLP-2011)  Webscale Knowledge Extraction co-located with KCAP 2011  Learning by Reading in the Real World (LERREW 2011) co-located with Conference of the Italian Association for Artificial Intelligence (AI*IA 2011)  Learning by Reading and its Applications in Intelligent Question-Answering (FAM-LbR/KRAQ 2011), co-located with IJCAI 2011  Conference on Intelligent Text Processing and Computational Linguistics CICLing 2011  Association of Computational Linguistics (ACL 2011)  Association for the Advancement of Artificial Intelligence (AAAI 2011)  Student Summit @ Machine Reading Phase 3 Kickoff meeting 2011  Artificial Intelligence Journal (AIJ 2010)  Workshop on Formalisms and Methodology for Learning by Reading (FAM-LbR 2010), co-located with NAACL 2010Program and Session Chair  Workshop on Learning by Reading and its Applications in Intelligent Question-Answering (FAM-LbR/KRAQ 2011), co-located with IJCAI 2011  Workshop on Formalisms and Methodology for Learning by Reading (FAM-LbR 2010), co-located with NAACL 2010"
    }, {
    "id": 9,
    "url": "http://localhost:4000/talks",
    "title": "Talks and Publications",
    "body": "  Publications:   2014       (Book Chapter) Ovchinnikova E. , Montazeri N. , Alexandrov T. , Hobbs J. R. , McCord M. C. , Mulkar-Mehta R. (2014) Abductive Reasoning with a Large Knowledge Base for Discourse Processing. In: Bunt H. , Bos J. , Pulman S. (eds) Computing Meaning. Text, Speech and Language Technology, vol 47. Springer, Dordrecht. https://doi. org/10. 1007/978-94-007-7284-7_7     2013       (Book Chapter) Hobbs, J. R. , &amp; Mulkar-Mehta, R. (2013). Toward a Formal Theory of Information Structure. In Evolution of Semantic Systems (pp. 101-126). Springer, Berlin, Heidelberg.      2012       (Book) Mulkar-Mehta, R. , Granular Causality: Discovering Causality in Natural Language, LAP LAMBERT Academic Publishing (2012), (ISBN 10:3847324470, ISBN 13:978-3847324478) Available from Amazon. com     (Conference Paper) Silgard, E. , Tharp, M. , &amp; Mulkar-Mehta, R. (2012). Temporal Relation Extraction from Medical Discharge Summaries. 2012 IEEE Second International Conference on Healthcare Informatics, Imaging and Systems Biology, 132-132, (HISB 2012), UCSD, La Jolla, CA     2011       Mulkar-Mehta, R. , Welty, C. , Hobbs, J. R. , Hovy, E. H. , Using Part-of Relations to Discover Causality, Machine Reading Phase 3 Kickoff meeting, 2011, Seattle, WA     Mulkar-Mehta, R. , Welty, C. , Hobbs, J. R. , Hovy, E. H. , Granularity in Natural Language Discourse, Reasoning with Text, 2011 Workshop, at ICT, Los Angeles, CA     (Ph. D. Thesis) Mulkar-Mehta, R. (2011). Granular causality for learning by reading. University of Southern California.      (Journal Paper) Pan, F. , Mulkar-Mehta, R. , &amp; Hobbs, J. (2011). Annotating and Learning Event Durations in Text. Computational Linguistics, 37, 727-752.      (Conference Paper) Ovchinnikova, E. , Montazeri, N. , Alexandrov, T. , Hobbs, J. , McCord, M. , &amp; Mulkar-Mehta, R. (2011). Abductive Reasoning with a Large Knowledge Base for Discourse Processing. In Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011)     (Conference Paper) Mulkar-Mehta, R. , Gordon, A. , Hobbs, J. R. , &amp; Hovy, E. Causal markers across domains and genres of discourse. In Proceedings of the sixth international conference on Knowledge capture, pp. 183-184. 2011. [poster]     (Conference Paper) Mulkar-Mehta, R. , Hobbs, J. R. , &amp; Hovy, E. (2011). Granularity in natural language discourse. In Proceedings of the Ninth International Conference on Computational Semantics (IWCS 2011).     (Conference Paper) Mulkar-Mehta, R. , Hobbs, J. R. , &amp; Hovy, E. H. (2011, March). Applications of Discovery of Granularity Structures in Natural Language Discourse. In AAAI Spring Symposium: logical formalizations of commonsense reasoning.     (Conference Paper) Mulkar-Mehta, R. , Welty, C. , Hobbs, J. , &amp; Hovy, E. (2011, March). Using Part-Of Relations for Discovering Causality. In Twenty-Fourth International FLAIRS Conference. (Nominated for Best Student Paper Award [PPT]     2010       Mulkar-Mehta, R. , Jerry Hobbs, Causal Granularity: Answering  How  Events Happen, At the Abductive Inference ARO MURI 2010 Workshop     2009       Mulkar-Mehta, R. , Hobbs, J. , Liu, C. , &amp; Zhou, X. (2009). Discovering Causal and Temporal Relations in Biomedical Texts. AAAI Spring Symposium: Learning by Reading and Learning to Read.      2008         Hobbs, J. R. , &amp; Mulkar-Mehta, R. (2008). Using Abduction for Video-Text Coreference. In Proceedings of BOEMIE 2008 Workshop on Ontology Evolution and Multimedia Information Extraction.      2007           Barker, K. , Agashe, B. , Chaw, S. , Fan, J. , Friedland, N. , Glass, M. R. , Hobbs, J. , Hovy, E. , Israel, D. , Kim, D. S. , Mulkar-Mehta, R. , Patwardhan, S. , Porter, B. , Tecuci, D. , &amp; Yeh, P. Z. (2007). Learning by Reading: A Prototype System, Performance Baseline and Lessons Learned. AAAI.       Mulkar-Mehta, R. , Hobbs, J. , &amp; Hovy, E. (2007). Learning from Reading Syntactically Complex Biology Texts. AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning.       Pan, F. , Mulkar-Mehta, R. , &amp; Hobbs, J. (2007). Modeling and Learning Vague Event Durations for Temporal Reasoning. In Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI), Nectar Track, pp. 1659-1662, Vancouver, Canada.       Mulkar, R. , Hobbs, J. , Hovy, E. , Chalupsky, H. , &amp; Lin. C. Y. Learning by reading: Two experiments. In Proceedings of IJCAI 2007 workshop on Knowledge and Reasoning for Answering Questions, pp. 287-296. 2007.        2006       Pan, F. , Mulkar-Mehta, R. , &amp; Hobbs, J. R. (2006, July). Learning event durations from event descriptions. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (pp. 393-400). (Acceptance rate: 23%)     Pan, F. , Mulkar, R. , &amp; Hobbs, J. R. (2006, May). An Annotated Corpus of Typical Durations of Events. In LREC (pp. 77-82).        Pan, F. Mulkar, R. , Hobbs, J. R. Extending TimeML with Typical Durations of Events, In Proceedings of Annotating and Reasoning about Time and Events workshop at the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), 2006     Chair and Organizer:     IJCAI-ECAI-18, Stockholm, Sweden, July 13-19, 2018  Association for the Advancement of Artificial Intelligence AAAI 2017  COLING 2014 Information Retrieval and Question Answering Track  The 5th International Symposium on Semantic Mining in Biomedicine (SMBM), 2012  International Workshop on Machine Reading for Social Media Content Analytics(IMRSMCA'12)   Information Retrieval and Knowledge Discovery in Biomedical Text symposium, at the AAAI 2012 Fall Symposia series.   International Journal on Semantic Web and Information Systems (IJSWIS), Special Edition on  Web Scale Knowledge Extraction , 2012, Ed. Aditya Kalyanpur, James Fan, Chris Welty  Natural Language Processing in the Web Era, Volume 6. 2, December 2012, Ed. Roberto Basili, Bernardo Magnini  Journal on BMC Medical Informatics and Decision Making 2012  Empirical Methods in Natural Language Processing (EMNLP-2011)  Webscale Knowledge Extraction co-located with KCAP 2011  Learning by Reading in the Real World (LERREW 2011) co-located with Conference of the Italian Association for Artificial Intelligence (AI*IA 2011)  Learning by Reading and its Applications in Intelligent Question-Answering (FAM-LbR/KRAQ 2011), co-located with IJCAI 2011  Conference on Intelligent Text Processing and Computational Linguistics CICLing 2011  Association of Computational Linguistics (ACL 2011)  Association for the Advancement of Artificial Intelligence (AAAI 2011)  Student Summit @ Machine Reading Phase 3 Kickoff meeting 2011  Artificial Intelligence Journal (AIJ 2010)  Workshop on Formalisms and Methodology for Learning by Reading (FAM-LbR 2010), co-located with NAACL 2010  Workshop on Learning by Reading and its Applications in Intelligent Question-Answering (FAM-LbR/KRAQ 2011), co-located with IJCAI 2011  Workshop on Formalisms and Methodology for Learning by Reading (FAM-LbR 2010), co-located with NAACL 2010    Talks            Redefine, Rebrand, Relaunch, ACT-W Advancing the Careers of Technical Women, May 2015, Seattle      A Tutorial for Using Python for Linguistic Data Analysis, pyData Seattle, July 2015      Using Python for Natural Language Processing, Meetup at Data Science Dojo, August 2015        Refereed Conference Papers            Emily Silgard, Melissa Tharp, Rutu Mulkar-Mehta,&nbsp;Temporal Relation Extraction from Medical Discharge Summaries, In the Second IEEE Conference on Healthcare Informatics, Imaging, and Systems Biology (HISB 2012), UCSD, La Jolla, CARutu Mulkar-Mehta, Jerry Hobbs, Causal Markers across Domains and Genres of Discourse, In The Sixth International Conference on Knowledge Capture, 2011&nbsp;[Poster]Rutu Mulkar-Mehta, Christopher Welty, Jerry R. Hobbs, Eduard Hovy, Using Part-Of Relations for Discovering Causality, In Proceedings of The 24th International Florida Artificial Intelligence Research Society Conference, 2011 (Nominated for Best Student Paper Award) &nbsp;[PPT]Rutu Mulkar-Mehta, Jerry R. Hobbs, Eduard Hovy, Applications of Discovering Granularity in Natural Language Discourse, In Proceedings of The Tenth International Symposium on Logical Formalizations of Commonsense Reasoning at the AAAI Spring Symposium 2011, Palo Alto [Poster]Rutu Mulkar-Mehta, Jerry R. Hobbs, Eduard Hovy, Granularity in Natural Language Discourse, In Proceedings of International Conference on Computational Semantics, 2011 [Poster]Ekaterina Ovchinnikova, Niloofar Montazeri, Theodore Alexandrov, Jerry Hobbs, Michael C. McCord, Rutu Mulkar-Mehta, Abductive Reasoning with a Large Knowledge Base for Discourse Processing, In Proceedings of International Conference on Computational Semantics, 2011 (Acceptance Rate: 40%)Rutu Mulkar-Mehta, Jerry Hobbs, Chun-Chi Liu, Xianghong Jasmine Zhou, Discovering Causal and Temporal Relations in Biomedical Texts, In Proceedings of Learning by Reading and Learning to Read at the AAAI Spring Symposium 2009, Palo Alto [PPT]Ken Barker, Bhalchandra Agashe, Shaw Yi Chaw, James Fan, Noah S. Friedland, Michael Glass, Jerry R. Hobbs, Eduard H. Hovy, David J. Israel, Doo Soon Kim, Rutu Mulkar-Mehta, Sourabh Patwardhan, Bruce W. Porter, Dan Tecuci, Peter Z. Yeh, Learning by Reading: A Prototype System, Performance Baseline, and Lessons Learned, In Proceedings of Twenty-Second National Conference on Artificial Intelligence (AAAI), 2007 (Acceptance Rate: 27%)Feng Pan, Rutu Mulkar, Jerry R. Hobbs, Modeling and Learning Vague Event Durations for Temporal Reasoning, In Proceedings of the Twenty Second Conference on Artificial Intelligence (AAAI), Nectar Track, pp. 1659-1662, Vancouver, Canada (Acceptance rate: 17%). Rutu Mulkar, Jerry R. Hobbs, Eduard Hovy, Learning from Reading Syntactically Complex Biology Texts, In Proceedings Proceedings of the 8th International Symposium on Logical Formalizations of Commonsense Reasoning at the AAAI Spring Symposium 2007, Palo AltoFeng Pan, Rutu Mulkar, Jerry R. Hobbs, Learning Event Durations from Event Descriptions, In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), pp. 393-400 Sydney, Australia (Acceptance rate: 23%)Feng Pan, Rutu Mulkar, Jerry R. Hobbs, An Annotated Corpus of Typical Durations of Events, In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC), pp. 77-82, Genoa, Italy  Refereed Workshop Papers:             Jerry R. Hobbs, Rutu Mulkar-Mehta, Using Abduction for Video-Text Coreference, In Proceedings of BOEMIE 2008 Workshop on Ontology Evolution and Multimedia Information Extraction [PPT]      Rutu Mulkar, Jerry R. Hobbs, Eduard Hovy, Hans Chalupsky, Chin-Yew Lin, Learning by reading: Two experiments, In Proceedings of the 3rd International Workshop on Knowledge and Reasoning for Question Answering (KRAQ&rsquo;07) colocated with IJCAI, Hyderabad, India, January 6, 2007. [PPT]        Other Talks, Papers and Posters                        Noah Friedland, David Israel, Peter Clark, Eduard Hovy, Jerry Hobbs,&nbsp;Rutu Mulkar, Bruce Porter, Ken Barker, Ralph Weischedel, Paul Martin, Project Mobius: a study on the feasibility of Learning by Reading,&nbsp;Technical Report, 2008      "
    }, {
    "id": 10,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 11,
    "url": "http://localhost:4000/page2/",
    "title": "Syntax and Semantics",
    "body": "  {{ page. title }}: I am maintaining a list of interesting foundational NLP concepts here.        {% for post in site. posts %}      {% if post. filter contains 'blog' %}        {% include postbox_nlp. html %}      {% endif %}    {% endfor %}        "
    }, {
    "id": 12,
    "url": "http://localhost:4000/page3/",
    "title": "Syntax and Semantics",
    "body": "  {{ page. title }}: I am maintaining a list of interesting foundational NLP concepts here.        {% for post in site. posts %}      {% if post. filter contains 'blog' %}        {% include postbox_nlp. html %}      {% endif %}    {% endfor %}        "
    }, {
    "id": 13,
    "url": "http://localhost:4000/blog/2023/ml-to-llm/",
    "title": "Machine Learning, Deep Learning and Large Language Models",
    "body": "2023/07/13 - Starting the early 2000s, the improvements in hardware to support deep learning networks has lead to a leap in modern deep learning approaches. Deep Learning (Hinton et al. 2006), (Bengio et al. 2007), which is an extension of neural networks, contain an input, an output, and a large number of hidden layers between the input and output. This type of an architecture is able to capture non-linear relationships in data, and are better at modeling data. Deep learning works much better than their Machine Learning predecessors as shown by their performance in several types of benchmark datasets such as SQuAD (The Stanford Question Answering Dataset), GLUE (General Language Understanding Evaluation), XTREME ((X) Cross-Lingual Transfer Evaluation of Multilingual Encoders) and others.  Deep learning is applicable to a large variety of applications ranging from Natural Language Processing, Speech Recognition, Computer Vision etc. For this doc, I will focus on Deep Learning as it pertains to Natural Language Processing, as it gives me the opportunity to delve deeper into LLMs - the newest kid on the block with Deep Learning. This doc is split into the following layout:  From ML to Deep Learning     Feature Representation         Tokenization and Subwords          Non-Linear Activation functions   Optimizers and Backpropagation   Deep Learning Architectures, Regularization and Attention         Feed Forward Neural Networks     Recurrent Neural Networks (RNN)     Long Short Term Memory Networks (LSTM)     Regularization to avoid Overfitting     Attention Mechanisms          Transformers for Transfer Learning and Contextual Embeddings         Self Attention and Transformer Architecture     Limitations of Transformers     Transformer Based Models           From Deep Learning and transformers to Large Language Models     Why Do LLMs work?         Scaling Laws          Emergent Abilities of LLMs   Pre-training of LLMs         Training - support for distributed training          Adaptation Tuning for LLMs         Instruction Tuning     Alignment Tuning          Model Quantization   Evaluation of LLMs    Final Thoughts References Appendix A: Word Vectors and Distribution Hypothesis     Embeddings and Vector Semantics         Distribution Hypothesis was studied a lot in lexical semantics     Words as vectors:          I describe the foundational improvements that have been made in every aspect of the deep learning architecture to bring us to where we are today. Although compute and data are 2 major areas which have enabled Deep Learning as it is today, my focus for this doc is on architecture and algorithmic improvements. From ML to Deep Learning: A simplified type of a neural network with a single hidden layer, require the following to train a model: Feature representation, loss function (such as cross entropy loss), classification function (such as sigmoid), optimizers (such as gradient descent). Grounding our understanding in terms of this, we can observe significant improvements in each of these areas, which has enabled deep learning to be as prevalent and effective as it is today. Deep Learning has enabled generalization of knowledge, and the possibility of pre-training on generic data and fine tuning to a specific domain (domain adaptation), without the need for any hand coded feature generation. In the rest of this section I will describe some foundational improvements that Deep Learning has provided based for every aspect of a neural network on the following high level areas, which makes deep learning work so well in practice compared to previous approaches. Feature Representation: In Machine Learning (ML), a rich feature representation is required as input for an ML model to learn from the data. Previously this was done using feature engineering, and hand-coding of features. However, with deep learning, the input is now either pretrained embeddings or the features/embeddings are learned from running text by the deep learning neural network. More details about the evolution of feature representation and vector semantics is provided in Appendix A. In 2003 Bengio at. al. [] introduced a revolutionary new intuition of using running text as supervised training data to predict whether a word “a” is more or less likely to show up near another word “b”. This avoids the needs for any hand-labeled data for supervised classification. The authors applied this approach for language modeling and illustrate how this approach improves perplexity (branching factor) of the model over existing n-gram approaches. In 2013, Word2Vec (Mikolov et al. 2013) was introduced, which used running text as input to a binary classification model to to classify whether a word exists in the neighborhood of another word. Word2vec uses 2 approaches to accomplish this: Skip Gram with Negative Sampling (SGNS), Cumulative bag of words (CBOW). With SGNS, for each target word, we treat the neighboring context words (within a window of words) as positive samples, and then randomly sample words from the rest of the lexicon and use them as negative samples. This is then provided as input to the classifier, which distinguishes between the positive and negative samples. The weights that are learned from the classifier are treated as the embeddings. Word2Vec has some shortcomings, as it can only provide static embeddings, and not different embeddings based on contextual information. This means that the words “bank” would be the same embedding irrespective of whether it is mentioned in the context of a “river bank” or a “financial institution bank”. Word2Vec also had trouble dealing with unknown words, as it tokenized based on words/phrases. Finally, Word2Vec could not handle word dependencies longer than the window of the surrounding text. In 2014 GloVe (Pennington et. al 2014) built on top of the limitations of Word2Vec, and not only used local context (like word2vec) but also global context to capture the the relationship between two words or phrases. GloVe is better than word2vec at handling rare words, because of global relationships between rare words and common words is captured by the model. Like Word2Vec, GloVe also worked on word or phrases as the smallest tokens, and had trouble working with unseen words. In 2017 Fasttext (Bojanowski et al. , 2017)](https://aclanthology. org/Q17-1010/) was introduced which works with subword instead of words or phrases. This means that it would handle rare words much better, as it tokenized the words into their subwords. E. g. “let’s” would be broken down into “let” and “‘s” and their embeddings would be learned independently. In 2018 (Peters et. al. 2018) proposed ELMo that is able to capture the concept of contextual embeddings (which addresses the “bank” problem above. ELMo as built on bi-directional RNN layers, capturing the embeddings of it from the forward and backward pass of the RNN. More recently there are newer representations of embeddings using the transformer architecture (such as BERT, TransformerXL, ChatGPT and others). Tokenization and Subwords: Creating embeddings at subword level is able to help us deal with unknown words much better than before. Now, we cam compose the definition of a new word based on it’s subcomponents. In 2015, (Sennrich et al. 2015) explored working with Byte-Pair Encoding (BPE) (Gage, 1994) that is originally a compression algorithm to encode running text. This helps capture text better than existing tokenization approaches, but as it works with unicode characters (144,697 unicode characters!), the unicode combinations are sparse. In 2016, Google introduced WordPiece (Wu et. al. 2016) which was the internal tokenizer used by BERT. In 2018 (Kudo and Richardson, 2018) introduced SentencePiece as a much more performant, and principled approach for subword encoding as compared to BPE alone. SentencePiece combines BPE with the Unigram model. SentencePiece was used to train T5 Language Model. In (Bostrom and Durett, 2020)(https://arxiv. org/pdf/2004. 03720. pdf)) the authors mention that BPE is not an effective way to train LLMs. Non-Linear Activation functions: Activation functions are used in the hidden layers of deep learning architecture, which each individual node takes in input from the previous layer, and performs an operation on them. The goal of activation functions is to be able to capture complex data from the input, without loss of information. Existing activation functions, such as Sigmoid cannot represent more complex data representations, and is heavily prone to gradient saturation for values close to 0 or 1. Explaining this a bit more, we know that: Sigmoid Activation is (y = 1 / (1+e^ (-z))), where z = sum (w_i * x_i) + b.       Fig 1: Sigmoid Activation Function: Image Credit - Wikipedia This z value is converted into a probability by using an activation function such as sigmoid. The problem with sigmoid is that it squashes outliers towards 0 or 1, making it challenging to capture outlier data, as it causes problems getting derivatives and propagating it back to the first layer using Backprop. This problem is also known as the Vanishing Gradients Problem. To address this, deep learning has explored with non-linear activation functions such as Rectified Linear Units or ReLU (Nair and Hinton, 2010). ReLU has linear behavior for positive values, and zero activation for negative values. Using ReLU transforms the input space (such as XOR) into a linear space in the hidden layers, which can then be classified using a linear approach (Goodfellow 2016, page 169) Relu Activation Function is: y = ReLU(z) = max(z,0), where z = sum (w_i * x_i) + b. Newer activations functions are now used for LLMs. An image with ReLU, GeLU and others along with more details are covered in the next section regarding LLMs. Optimizers and Backpropagation: Gradient Descent (GD) (Robbins and Monro 1951) is the algorithm to find the global minima in a gradient for all ML and Deep Learning algorithms. In 2010, Stoachastic Gradient Descent (SGD) (Bottou, L. 2010), was introduced, which is what is used in practice (however more recent new approaches have evolved that are effective for training LLMs). GD requires all of the data to be processed once, after which it will update it’s parameters. This works for small amounts of data, but it prohibitive when working with large amounts of data for deep learning. In comparison to SD, SDG only requires a single datapoint (or a batch of datapoints - called mini-batch) to be processed before updating the parameters. SGD typically converges faster and is more memory efficient. SDG and GD both work with a constant learning rate (or step size), irrespective of whether it has encounted the same datapoint more frequently or less frequently. As a result, the learning is often more time consuming, or is stuck at a local minima. To address this Adagrad (Adaptive Gradient Descent) (Duchi 2011) performs gradient descent with varying learning rates for different parameters, increasing the learning rate for rare datapoints to push for faster convergence. Adagrad is able to handle sparse data much better than GD or SGD. Adam optimizer (Kingma and Ba, 2015), builds on Adagrad, and combines adaptive learning rates with moment. Adam maintains different learning rates for different parameters, and combines them with the first moment (mean of the gradients, providing the overall direction of the gradient) and the second moment (variance of the gradient, providing the magnitude of the gradient). This helps with faster convergence and adaptation to varied types of gradients. Other honorary mentions for optimizers are RMSProp, AdaDelta, AdaMax etc. which have their own nuances which need to be evaluated before leveraging for a deep learning problem. Improving the optimizers have enabled much faster convergence of Deep Learning Networks, handling sparse data better, and improved exploration of the landscape to avoid local minima. Neural networks can contain a large number of stacked layers, where the output of the final layer needs to be propagated back to the first layer for learning. This is done using the error backpropagation or backprop (Rumelhart et al. , 1986), (LeCun et. al. 1998). Deep Learning Architectures, Regularization and Attention: Feed Forward Neural Networks: In 2003, (Bengio et al. 2003) first introduced the simple feed-forward language model primarily for language modeling. While previously, we were using n-gram language models, where ‘n’ usually ranged up to 3, neural language models, neural models are able to generalize over a larger context, and generalize better. The first feedforward neural network contained a single hidden layer, and was able to capture long distance dependencies much betters as compared to the n-gram approach and used running text as input to illustrate self-supervision. Recurrent Neural Networks (RNN): Recurrent Neural Networks are neural networks that have a cycle within the network, where the hidden layer computed in the previous iteration is leveraged as a form of context or memory for the next iteration. . This is very pertinent to language, which is largely dependent on the previous text/utterance. The first RNN language models were by (Mikolov et al. , 2010). RNNs are used for Language Modeling (for machine translation), sequence/text classification and several other downstream tasks. RNNs have been used for Machine Translation using en Encoder-Decoder architecture (also knows as seq-2-seq models). Here an encoder takes in an input sentence and converts it into an embedding of some form, which is taken as input by the decoder and converted into text in another language. Encoder Decoder architectures have been widely successful and also have been applied to tasks such as question answering, textual entailment, summarization etc. The intuition behind this is that the output text is a function of the input text (e. g. answer is related to the question being asked), even though the output and input both belong to the same language. Although RNNs capture the temporal nature of language and dependence on previous words, it has a limitation of not being able to parallelize the processing, as each token can only be processed after the previous token is processed and the weights from the hidden layers are passed to it. Another limitation of this was the problem of vanishing gradients, meaning that the gradients brought from the hidden layer were subjected to so many multiplications, that they eventually ended up becoming 0. This lead to the problem of long distance dependencies not accurately captured. Long Short Term Memory Networks (LSTM): LSTMs (Hochreiter and Schmidhuber, 1997) started being used to mitigate the issues introduced by RNNs, in particular regarding RNNs not being able to address or make use of long distant information. LSTMs introduced gates that selectively passed information from the input layer and also the hidden layer from the previous node. Regularization to avoid Overfitting: In order to avoid overfitting, various forms of regularization are used. One of the most important ones is called - dropout. Dropout is when we randomly drop some units and their connections from the network during training (Hinton et al. 2012), (Srivastava et al. 2014). Hyperparameter tuning is another important requirement to avoid overfitting or being stuck at a local minima. Attention Mechanisms: In 2014 (Bahdanau 2014) introduced the concept of Attention in Deep Neural Networks, which addressed the botteneck issue that was introduced by RNNs (where the data at n-1 needed to be processed before data at n so that data n could use the hidden layer as input from data n-1). Using the attention mechanism, data n could get hidden states from all of the previous data points, and not just data n-1. This was explained from the context of a machine translation task where the encoder created hidden layers for all of the items in the input, and all these hidden layers were provided to the decoder in the form of a context vector, which is a function of all the hidden layers. There are different types of attention mechanisms. Dot-product attention is one such approach, where all the hidden layers from the past few contexts are combined in the form of a dot product, and taken to the decoding layer of an RNN. Transformers for Transfer Learning and Contextual Embeddings: Self Attention and Transformer Architecture: In 2017, (Vaswani et al. , 2017), proposed the original transformer architecture which was based on two lines of prior research: self-attention (Bahdanau 2014) and memory networks (Sukhbaatar et al. , 2015). Transformer is based on the concept of attention, and it replaces RNNs and the bottlenecks introduced by it. In the original Transformer paper, it consists of an encoder and a decoder. In the encoder, Transformers use fully connected feed forward neural networks where each input token is connected with all the past tokens in a step called Self-Attention. Using this, and extending it to multi-head attention where each token is performing self-attention in parallel, the encoder is able to capture the dependency relationships between each token in the input. Transformers also add positional encoding (one hot encoding) of the position of the token in the sequence to capture word order, thereby replacing the need for RNN like architecture to encode the sequential dependency of a word/token on previous tokens. The encoder and decoder both contain similar stacked layers of self attention and fully-connected networks. Transformers allow parallel computation (which RNN’s could not). Transformers also introduce Layer Normalization (scaling the dot products after each layer), which is able to address the vanishing gradients problem that RNNs, LSTMs and other approaches could not address. Today, Transformers are the cornerstone all language models for autoregressive generation (gen AI). There are several improvements that have been made to vanilla transformers to train LLMs as they are today.       Fig 2: Vanilla Transformer Architecture Limitations of Transformers: Attention is quadratic in nature (because at token ‘n’, we are computing context for ‘n’ and all the previous ‘n-1’ tokens. As a result, Transformer architectures have not able to address very long documents. Some approaches that have been introduced to address this approaches like Longformer (Beltagy et. al 2020), where the attention mechanism scales linearly with sequence length. This enables processing much longer texts as compared to the vanilla transformer approach. More recently newer attention mechanisms have been introduced to address the quadratic nature of attention. Details are in the Pre-Training section of LLMs. Transformer Based Models: In 2019, BERT (Devlin et. al. 2019) was introduced which has two objective functions: Masked LM, and NSP (Next Sentence Prediction), so that it would learn bidirectional information from within a sentence, and learn about dependencies between 2 sentences.  It was one of the first initiatives that showed contextual embeddings. Also, honorable mention to ELMo (Peters et. al 2018) which was the first initiative for contextual embeddings, but it did not use the transformer architecture. Other transformer inspired early approaches are: RoBERTa (Liu et al. 2019), Distilbert (Sanh et. al. 2019), TransformerXL (Dai et. al. 2019), T5 (Raffel 2019). Deep Learning is able to generalize for unseen data much better than their Machine Learning counterparts. (Erhan et al. 2010) discuss the important of pretraining for deep learning tasks. From Deep Learning and transformers to Large Language Models: More recently researchers have observed that model scaling can lead to an improved model capacity (ability to represent complex patterns that is it trained on) and significant improvement in performance in downstream tasks. It is also discovered that this new era of Large Language Models (that have 10B parameters or more) exhibit some emergent capabilities (such as in context learning), that have not been present in small scale language models (such as BERT, DistilBERT etc - which have Millions of Parameters only). In 2018, GPT-1 (Radford et. al. 2018) which stands for Generative Pre-Training was developed using a generative decoder only Transformer Architecture. GPT-1 adopted an approach of pre-training followed by supervised fine-tuning. GPT-1 is a 117MM parameter model. Later in 2019, GPT-2 (Radford et. al. 2019), followed a similar approach as GPT-1, but increased the number of paramerters to 1. 5B. It aimed to perform tasks without explicit fine tuning using labeled data. To enable this, the authors introduced a probabilistic approach for multi-task solving p(output|input, task), where the output is conditioned not only on the input, but also the task. GPT-3 (Brown et. al. 2020) was released in 2020, and it scaled the number of model parameters to 175B. The authors introduce the concept of in-context learning, which uses LLMs in a few-shot or zero shot way. This means that the pre-training and prompting (with in-context information) will help the model converge to an answer within the context of the information provided. GPT-4 (OpenAI, 2023) was released in 2023 (March) and it extended text input to multimodal signals. GPT-4 shows strong capabilities in solving complex problems as compared to previous models. A recent study by (Bubeck et. al 2023) showed that GPT-4 can perform better at a variety of tasks of different domains (such as mathematics, coding, vision, medicine, law, psychology and more), and performs very similar to human results. The paper shares that this is the beginning of AGI (Artificial General Intelligence) Huggingface recently released the Bloom model (HuggingFace 2022) which has multilingual support for 46 natural languages and 13 programming languages, and Meta has released LLaMA (Touvron et. al. 2023) has 65B parameters . All of these are generative language models and they have moved us several leaps into NLP tasks such as : question answering, multi-task learning (Radford et. al. 2019), and others, and also have shown emergent abilities that can be observed using prompting. These generative models are also evaluated using prompting, which has lead to a whole new way of debugging language models, and learning about what these LMs know and how they can be leveraged in other areas. (Chen et al, 2023) provide a detailed survey of LLMs and what it has taken for us to get this far with them. Why Do LLMs work?: Scaling Laws: KM scaling law (Kaplan et. al. 2020) by open AI proposed a power law relationship of model performance with respect to three major factors - Model Size, Datset Size and amount of training compute. The Google DeepMind team (Hoffmann et. al. 2022) proposed another study (Chinchilla Scaling Law) which is an alternative form of scaling for training LLMs. Below is an image of how the number of parameter of a model have expanded over the years.       Fig 4: Image Credit: Stanford LLM Course Emergent Abilities of LLMs: LLMs introduced a new set of abilities that were not previously present in the smaller models (such as BERT), but were introduced when models were scaled to a much larger size. Some of these abilities include:  In-context learning: This learning ability was formally introduced by GPT-3 (Brown et. al. 2020) where given that a model is provided with some task demonstrations, it can generate the output text by completing the word sequence of the input text.  Instruction Following: After fine tuning on instruction datasets, LLMs are able to follow and execute tasks for new datasets.  Step-by-Step Reasoning: Using chain-of-thought (CoT)(Chung et. al. 2022) prompting approaches, LLMs are able to solve step by step reasoning steps. Pre-training of LLMs: LLMs rely on a massively large corpus of data for training. Most LLMs such as GPT-2 (Radford et. al. 2019) and PaLM (Chowdhery et. al 2022) are trained on generic datasets that are a collection of books, webpages and conversational text, as this generic data and introduce general purpose capabilities to the language model. CommonCrawl is one of the biggest sources of web data, and Reddit corpora is one of the biggest sources of conversational text. Several LLMs such as PaLM (Chowdhery et. al 2022) and Bloom (Huggingface 2022) also use specialized text data for training. This includes data like multilingual text, scientific text (research papers) and code (from stack exchange or Github). In order to train with this data, several preprocessing steps are performed to remove redundant, private data, irrelevant data or toxic data from the text. The text is encoded into subword tokenizers (some of which was discussed in the Subwords Section). Once the data requirements are established (all LLMs need to have large amounts of data and high quality data for optimal performance), the data is passed through one of the many architectures such as encoder-decoder architecture (used by BART, T5, causal decoder architecture (for next word prediction and used by GPTs, BLOOM), or the more recent Prefix Decoder (used by PaLM) Architecture. There has been a lot of research around the best location of layer normalization, and pre, post or sandwich layer norms are some different approaches used with the architecture. Activation functions used by LLMs are different from Deep Learning activations that I discussed in the previous section. These are largely GeLU (Gaussian Error Linear Unit) (Hendrycks 2016) or variants of GLU activation (Shazeer 2020) such as SwiGLU (Shazeer 2020) and GeGLU (Shazeer 2020). Below is an image of GELU activation function as compared to RELU and ELU (Clevert et. al. 2016).       Fig 5: Image Credit: Papers with Code Position embedding (as absolute positional encoding) as presented in the vanilla Transformer architecture has several new variations proposed, such as relative positional embedding, Rotary position embedding, and AliBi (Press et. al. 2022) Several different types of attention mechanisms can be used for LLMs today. For instance, sparse attention approaches are used to address the quadratic computational complexity by the vanilla transformer. GPT-3 uses factoid attention (Child et. al. 2019) where instead of full attention, each query can only be attended to by a subset of tokens based on positions. Another type of Attention is multiquery attention (Shazeer 2019) where the same linear transformation matrices are shared by different heads. Another approach is FlashAttention (Dao et. al, 2022) which proposes optimizations based on memory consumption of memory modules on GPU. There are 2 common objective functions used - language modeling (predict the next word given the previous word), or denoising autoencoding. During training, GPT-3 (Brown et. al. 2020) and PaLM (Chowdhery et. al 2022) have introduced a new strategy that dynamically increases the batch size during training. New LLMs also adopt a similar strategy with learning rates, where the learning rate is gradually increased to the maximum value, followed by a decay strategy. The optimizers used are ADAM (Kingma and Ba 2015), or variations of it such as ADAMw (Loshchilov and Hutter 2017). Training - support for distributed training: The training for LLMs is optimized using three commonly used parallel training techniques - data parallism where the training corpus is distributed across GPUs, pipeline parallelism (Huang et. al, 2019) where multiple layers of the transformers are distributed across GPUs and tensor parallelism (where the tensor is decomposed and parallelized). Adaptation Tuning for LLMs: Pretraining helps language models acquire general abilities for solving tasks. Most LLMs go through the next step of adaptation. There are 2 types of adaptation: instruction tuning (where abilities of LLMs are converted from language based to task based) and alignment tuning (where the output of the LMM is aligned with human preferences and values). Instruction Tuning: In a recent paper, (Chen et. al, 2023) describe several datasets that are available for instruction tuning, and how instruction tuning has helped generalize to unseen tasks. In 2022, (Chung et. al. 2022) first experimented with Chain-of-Thought (CoT) prompting to show commonsense reasoning ability and mathematical reasoning ability. Alignment Tuning: Regarding the second type of adaptation - alignment tuning - Open AI recently came out with their paper on InstructGPT (Ouyang et. al 2022) that helps further train models to align their output with human output. To align LLMs with human values (such as don’t use profanity, be polite, etc. ) , Reinforcement Learning from Human Feedback (RLHF) (Christiano et. al. 2017) was proposed which leverages algorithms like Proximal Policy Evaluation (Schulman et. al. 2017) to adapt LLMs to human feedback. An RLHF system has 3 components - the LLM to be tuned, a reward model, an RL approach for training. In order to efficiently fine tune using RLHF, several efficient parameter tuning approaches are proposed such as Adapter Tuning (Houlsby 2019), Prompt Tuning (Lester et al. , 2021), LoRA (Low-Rank Adaptation) etc. LoRA has been widely applied to open source LLMs (such as LLaMA and BLOOM) Model Quantization: LLMs are challenging to deploy in the real world because of their prohibitive memory footprint. Model Quantization is the approach that is used for reducing the memory footprint of LLMs using popular model compression approaches. There are two quantization approaches - Quantization-aware training (QAT) (this requires a full model retraining) and post-training quantization (PTQ) (this requires no model retraining). Some PTQ approaches include Mixed Precision decomposition (Dettmers et. al. 2022), Per-Tensor Quantization (Xiao et. al. 2023), etc. Final Thoughts: We have made tremendous progress in Deep Learning and NLP in the past few years, and although I have tried to cover a lot of the seminal work here, I wanted to emphasize that this is still a small drop in the massive amounts of work and research that has gone into this space. Some more concepts that are interesting and will be covered in future blog posts are - Knowledge Distillation, Quantization, Chain-of-Thought Prompting, in-context learning, Planning and Commonsense Reasoning using LLMs, Prompt Engineering, which hold promise for even bigger leaps into deep learning in the future. References: [1] (Bahdanau et. al 2015) Bahdanau, D. , K. H. Cho, and Y. Bengio. 2015. Neural machine translation by jointly learning to align and translate. ICLR 2015. [2] (Beltagy et. al 2020) Iz Beltagy, Matthew E. Peters, Arman Cohan, Longformer: The Long-Document Transformer, 2004, arXiv [3] (Bengio et. al 2003) Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. “A Neural Probabilistic Language Model. ” Journal of Machine Learning Research, vol. 3, 2003, pp. 1137-1155. [4] (Bengio et. al. 2007) Bengio, Y. , P. Lamblin, D. Popovici, and H. Larochelle. 2007. Greedy layer-wise training of deep networks. NeurIPS. [5] (Bottou 2010) Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT’2010 (pp. 177-186). Springer. [6] (Brown et. al. 2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. Language Models are Few-Shot Learners. NeurIPS 2020. [7] (Bubeck et. al 2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, “Sparks of Artificial General Intelligence: Early experiments with GPT-4”, 2023 [8] (Chen et al, 2023) Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen, “A Survey of Large Language Models”, 2023, arXiv [9] (Child et. al. 2019) Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever, Generating Long Sequences with Sparse Transformers, 2019, CoRR, abs/1904. 10509 [10] (Chowdhery et. al 2022) Chowdhery, A. , Narang, S. , Devlin, J. , Bosma, M. , Mishra, G. , Roberts, A. , Barham, P. , Chung, H. W. , Sutton, C. , Gehrmann, S. , Schuh, P. , Shi, K. , Tsvyashchenko, S. , Maynez, J. , Rao, A. , Barnes, P. , Tay, Y. , Shazeer, N. M. , Prabhakaran, V. , Reif, E. , Du, N. , Hutchinson, B. C. , Pope, R. , Bradbury, J. , Austin, J. , Isard, M. , Gur-Ari, G. , Yin, P. , Duke, T. , Levskaya, A. , Ghemawat, S. , Dev, S. , Michalewski, H. , García, X. , Misra, V. , Robinson, K. , Fedus, L. , Zhou, D. , Ippolito, D. , Luan, D. , Lim, H. , Zoph, B. , Spiridonov, A. , Sepassi, R. , Dohan, D. , Agrawal, S. , Omernick, M. , Dai, A. M. , Pillai, T. S. , Pellat, M. , Lewkowycz, A. , Moreira, E. , Child, R. , Polozov, O. , Lee, K. , Zhou, Z. , Wang, X. , Saeta, B. , Díaz, M. , Firat, O. , Catasta, M. , Wei, J. , Meier-Hellstern, K. S. , Eck, D. , Dean, J. , Petrov, S. , &amp; Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways. ArXiv, abs/2204. 02311. [11] (Chung et. al. 2022) Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei, Scaling Instruction-Finetuned Language Models, 2022, arXiv [12] (Christiano et. al. 2017) Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei (2017). Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems 30 (NIPS 2017) [13] (Clevert et. al. 2016) Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs), Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter, 2016 [14] (Dai et. al. 2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. “Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. ” Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2978-2988. [15] (Dao et. al, 2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré, FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness [16] (Dettmers et. al. 2022) Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer, LLM. int8(): 8-bit Matrix Multiplication for Transformers at Scale, 2022, 2208. 07339, arXiv [17] (Devlin et. al. 2019) Devlin, J. , M. -W. Chang, K. Lee, and K. Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL HLT. [18] (Duchi 2011) Duchi, J. , Hazan, E. , &amp; Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(7), 2121-2159. [19] (Erhan et. al. 2010) D. Erhan, A. Courville, Y. Bengio, P. Vincent, “Why Does Unsupervised Pre-training Help Deep Learning?”, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, PMLR 9:201-208, 2010. [20] (Gage 1994) Philip Gage. 1994. A New Algorithm for Data Compression. C Users J. , 12(2):23–38, February. [21] (Goodfellow et. al. 2016) Goodfellow, I. , Y. Bengio, and A. Courville. 2016. Deep Learning. MIT Press. [22] (Hendrycks 2016) Dan Hendrycks, Kevin Gimpel, Gaussian Error Linear Units (GELUs), 2016, arxiv [23] (Hinton et. al. 2006) Hinton, G. E. , S. Osindero, and Y. -W. Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554. [24] (Hinton et. al. 2012) G. E. Hinton , N. Srivastava, A. Krizhevsky, I. Sutskever and R. R. Salakhutdinov, Improving neural networks by preventing co-adaptation of feature detectors, CoRR, 2012 [25] (Hoffmann et. al. 2022) Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre, Training Compute-Optimal Large Language Models, 2022, 2203. 15556, arXiv [26] (Houlsby 2019) Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly, Parameter-Efficient Transfer Learning for NLP, 2019, CoRR, abs/1902. 00751 [27] (Hochreiter and Schmidhuber, 1997) Hochreiter, S. , &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780. [28] (Huang et. al, 2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen, GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism, 2019, arXiv:1811. 06965v5 [29] (Huggingface 2022) Huggingface, BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, 2022 https://arxiv. org/abs/2211. 05100 [30] (Kaplan et. al. 2020) Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess an Rewon Child an Scott Gray and Alec Radford an Jeffrey Wu an Dario Amodei, Scaling Laws for Neural Language Models, CoRR, abs/2001. 08361, 2020 [31] (Kingma and Ba 2015) Kingma, D. and J. Ba. 2015. Adam: A method for stochastic optimization. ICLR 2015 [32] (Kudo and Richardson, 2018) Kudo, T. , &amp; Richardson, J. (2018). SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Brussels, Belgium. [33] (LeCun et. al. 1998) LeCun, Y. , Bottou, L. , Orr, G. , &amp; Müller, K. (1998). Efficient backprop. In Neural Networks: Tricks of the Trade (pp. 9-48). Springer. [34] (Lester et al. , 2021) Lester, Brian and Al-Rfou, Rami and Constant, Noah, The Power of Scale for Parameter-Efficient Prompt Tuning, (https://aclanthology. org/2021. emnlp-main. 243) [35] (Levy and Goldberg 2014) Omer Levy, Yoav Goldberg, Neural word embedding as implicit matrix factorization, NIPS’14: Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2 December 2014 Pages 2177–2185 [36] (Liu et. al. 2019) Liu, Y. , M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. ArXiv preprint arXiv:1907. 11692. [37] (Loshchilov and Hutter 2017) Fixing Weight Decay Regularization in Adam. I. Loshchilov, F. Hutter. 2017. Introduces AdamW. [38] (Mikolov 2013) Mikolov, T. , Chen, K. , Corrado, G. , &amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1301. 3781. [39] (Mikolov et. al. 2010) Mikolov, T. , M. Karafiat, L. Burget, J.  Cernock, and S. Khudanpur. 2010. Recurrent neural network based language model. INTERSPEECH. [40] (Nair and Hinton 2010) V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltzmann machines,” in ICML, 2010, pp. 807–814. [41] (OpenAI, 2023) GPT-4 Technical Report, 2023, OpenAI [42] (Ouyang et. al 2022) Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe, Training language models to follow instructions with human feedback, 2022, arXiv, 2203. 02155 [43] (Pennington et. al, 2014) Pennington, J. , Socher, R. , &amp; Manning, C. D. (2014). GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532-1543). [44] (Peters et. al. 2018) Peters, M. E. , Neumann, M. , Iyyer, M. , Gardner, M. , Clark, C. , Lee, K. , &amp; Zettlemoyer, L. (2018). Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT) (pp. 2227-2237). [45] (Press et. al. 2022) Press, Ofir and Smith, Noah A and Lewis, Mike, Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, arXiv preprint arXiv:2108. 12409, 2022 [46] (Radford et. al. 2018) Radford, A. , Narasimhan, K. , Salimans, T. , &amp; Sutskever, I. (2018). Improving Language Understanding by Generative Pre-training. [47] (Radford et. al. 2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019. [48] (Raffel et. al. 2019) Raffel, C. , Shazeer, N. , Roberts, A. , Lee, K. , Narang, S. , Matena, M. , … &amp; Liu, P. J. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv preprint arXiv:1910. 10683. [49] (Robbins and Monro 1951) Robbins, H. , &amp; Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical Statistics, 22(3), 400-407. [50] (Rumelhart 1985) D. Rumelhart, G. Hinton, and R. Williams, “Learning internal representations by error propagation,” UCSD, Tech. Rep. , 1985. [51] (Sanh et. al. 2019) Sanh, V. , Debut, L. , Chaumond, J. , &amp; Wolf, T. (2019). DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910. 01108. [52] (Sennrich et. al 2016) Sennrich, R. , Haddow, B. , &amp; Birch, A. (2016). Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), Berlin, Germany. [53] (Shazeer 2019) Noam Shazeer, Fast Transformer Decoding: One Write-Head is All You Need, abs/1911. 02150, 2019 [54] (Shazeer 2020) Noam Shazeer, GLU Variants Improve Transformer, 2020, CoRR, abs/2002. 05202 [55] (Schulman et. al. 2017) John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov, Proximal Policy Optimization Algorithms, 2017, CoRR, abs/1707. 06347 [56] (Srivastava et. al. 2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov; ‘Dropout: A Simple Way to Prevent Neural Networks from Overfitting’ JMLR 15(56):1929−1958, 2014. [57] (Sukhbaatar et. al. 2015) Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, ‘End-To-End Memory Networks’, NeurIPS 2015 [58] (Touvron et. al. 2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample, LLaMA: Open and Efficient Foundation Language Models, 2023, arxiv [59] (Vaswani 2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in NIPS, 2017, pp. 6000–6010. [60] (Pennington et al 2014) Pennington, J. , R. Socher, and C. D. Manning. 2014. GloVe: Global vectors for word representation. EMNLP. [61] (Wu et. al. 2016) Yonghui Wu, M. Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, Yuan Cao, Qin Gao, Klaus Macherey, J. Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Y. Kato, Taku Kudo, H. Kazawa, K. Stevens, George Kurian, Nishant Patil, W. Wang, C. Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, G. Corrado, Macduff Hughes, J. Dean, Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. . 2016. Introduces WordPiece. Used by BERT. [62] (Xiao et. al. 2023) Guangxuan Xiao and Ji Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han, SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models, 2023, 2211. 10438, arXiv Appendix A: Word Vectors and Distribution Hypothesis: Embeddings and Vector Semantics: Distributional hypothesis first formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth (1957), “Words that occur in similar contexts tend to have similar meanings” Distribution Hypothesis was studied a lot in lexical semantics:  Word similarity: Two words that are similar to each other exist in similar contexts Word Relatedness: The meaning of two words can be related in ways other than relatedness similarity. One such class of connections is called word relatedness (Budanitsky association and Hirst, 2006) Topics and Semantic Field: One common kind of relatedness between words is if they belong to the same semantic field semantic field. A semantic field is a set of words which cover a particular semantic domain and bear structured relations with each other. For example, words might be related by being in the semantic field of hospitals (surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof, topic models kitchen, family, bed). Semantic fields are also related to topic models, like Latent Dirichlet Allocation, LDAWords as vectors:  Words as vectors: Osgood’s 1957 idea mentioned above to use a point in three-dimensional space to represent the connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954), and Firth (1957) to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments.  Term document matrix: The term-document matrix was first defined as part of the vector space model of information retrieval (Salton, 1971).  Term frequency: term frequency (Luhn, 1957): Inverse Document Frequency: (Sparck Jones, 1972) Pointwise Mutual Information: (Fano, 1961), (Church and Hanks 1989, Church and Hanks 1990)Measurement of similarity:  Cosine similarity - inner dot product between vectorswith sparse long vectors:  Representing words as 300-dimensional dense vectors requires our classifiers to learn far fewer weights than if we represented words as 50,000-dimensional vectors, and the smaller parameter space possibly helps with generalization and avoiding overfitting.  unable to handle unknown words independence assumption (which is untrue, and a very strong assumption to make) Dense vectors may also do a better job of capturing semantics, transfer learning, reasoning and other tasks that were previously hard to do. Dense embeddings like word2vec actually have an elegant mathematical relationship with sparse embeddings like PPMI, in which word2vec can be seen as implicitly optimizing a shifted version of a PPMI matrix (Levy and Goldberg, 2014c). "
    }, {
    "id": 14,
    "url": "http://localhost:4000/blog/2023/Important-books-for-AI/",
    "title": "Important books to read for AI",
    "body": "2023/06/09 - Here is a list of books that I highly recommend to bolster your foundational knowledge in ML and AI: Pattern Recognition and Machine Learning, by Christopher M. Bishop Speech and Language Processing, Dan Jurafsky and James H. Martin Deep learning, by Ian Goodfellow Information Theory,by David Mackay "
    }, {
    "id": 15,
    "url": "http://localhost:4000/blog/2021/probability-theory/",
    "title": "Probability Theory for Natural Language Processing",
    "body": "2021/05/08 - A lot of work in Natural Language Processing (NLP) such a creation of Language Models is based on probability theory. For the purpose of NLP, knowing about probabilities of words can help us predict the next word, understanding the rarity of words, analyzing and knowing when to ignore common words with respect to a given context - e. g. articles such as “the” and “a” have a very high probability of occurring in any document and add less information to the overall semantics, where as the probability of “supercalifragilisticexpialidocious” is incredibly low, but having it in a sentence provides much more semantics. Probability theory deals with predicting how likely something will happen. Below are some concepts to know and understand about probability. In this post I will discuss the following:  Experiment (Trial) Foundations     Sample space   Event Space   Disjoint Sets   Well Founded Probability Space    Probability Theory     Conditional Probability and Independence   Chain Rule   Independence of Events   Bayes Theorem   Random Variables   Probability Mass Function   Expectation   Variance   Standard Deviation   Joint Probability Mass Function   Marginal Distribution   Relative Frequency    Distributions     Continuous distribution    Maximum Likelihood Estimate     Bayesian Updating   Likelihood Ratio    Derivation Bayes Optimal DecisionExperiment (Trial)An experiment (also known as a trial) is a process by which an observation is made. Rolling a dice is a trial, observing the weather is a trial. The outcome of a trial is called an event. FoundationsSample space: The sample space is a collection of all the basic outcomes or events of our experiment.  Sample space can be discrete or continuous. In NLP, the sample space of a given dataset can be the exhausting combinations of words that can occur together as bigrams. In a trial with 2 dice, the sample space is all the combinations in which the dice can have an outcome. A sample space is represented as $\Omega$. $\phi$ represents an event that can never happen. Event Space: An event space is the set of all the possible subsets of the sample space. The size of an event space is $2^{\Omega}$. An event space is represented as $\mathcal{F}$. Disjoint Sets: Disjoint sets are sets that do not share any events with each other. E. g. in NLP if 2 datasets have no common vocabulary or vocabulary sequences, they are disjoint from one another. $A_j$ and $A_k$ are disjoint sets if they satisfy the following condition: [A_j \in \mathcal{F} (A_j \cap A_k = \phi, j \ne k)] [P(\cup_{j=1}^{\inf}A_j) = \sum_{j=1}^{\inf} P(A_j)] Well Founded Probability Space: A well founded probability space contains:  Sample space $\Omega$ a $\sigma$ field of events $\mathcal{F}$ A probability function (where all the individual probabilities sum to $1$)Probability TheoryConditional Probability and Independence: Conditional Probability is the heart of Naive Bayes algorithm. Conditional Probability measures the probability of an event given another event has occurred.       Fig 1: Conditional Probability       [P(A   B) = \frac{P(A \cap B)}{P(B)}]   A simpler way of understanding conditional probability is the following: If event $B$ has definitely happened, how likely is it for event $A$ to also happen? This is answered by the fraction of times $A$ happens when $B$ happens. Chain Rule: Chain rule of probabilities is used for Markov Models. According to chain rule, if we have 2 events $A$ and $B$, the probability of $A \cap B$ can be written as below:       [P(A \cap B) = P(A   B) P(B)]   When events A and B occur together they are written as $P(A \cap B)$ or $P(A B)$ For $n$ events, $A_1, A_2, \ldots, A_n$, the chain rule is:       [P(A_1 \cap A_2 \ldots \cap A_n) = P(A_1) P(A_2   A_1) \ldots P(A_n   \cap_{i=1}^{n-1}A_i)]   Another notation is:       [P(A_1 A_2 \ldots A_n) = P(A_1) P(A_2   A_1) \ldots P(A_n   \prod_{i=1}^{n-1}A_i)]   Applying this to NLP, we can compute the probability of the sentence “Pete is happy” by computing:       [P(Pete, is, happy) = P(Pete) P(is   Pete) P(happy   is, Pete)]   For very long sentences, such joint probabilities are very hard to compute, e. g. the probability of the sentence “I am happy because I ate salad for lunch on Wednesday” depends on the frequency of this exact sentence in the corpus. It is quite possible that this exact sentence does not exist in our training dataset, cut the component words do exist. In which case it is helpful to make an independence assumption. Independence of Events: Two events $A$ and $B$ are independent of each other if $P(A B) = P(A) P(B)$. That means that there is no overlap between the two events with respect to Figure 1. When applying the independence assumption, $P(A|B) = P(A)$ because the presence of $B$ does not affect the probability of $A$ at all. Two events $A$ and $B$ are conditionally independent given an event $C$ where $P(C) &gt; 0$ if:       [P(A \cap B   C) = P(A   C) P(B   C)]   Bayes Theorem: According to Bayes theorem:       [P(B   A) = \frac{P(A   B) P(B)}{P(A)}]   The denominator is a normalizing factor, and it helps produce a probability function. If we are simply interested in which event is most likely given A, we can ignore the normalizing constant because it is the same value for all events. Bayes theorem is central to the noisy channel model Random Variables: Random Variables map outcomes of random processes to numbers. Random variables are different from regular variables. Random variables can have many values, but regular variables can be solved for a small set of values. The probability $P(random|condition)$ helps with the mathematics notation of probability. The value held by a random variable can be discrete of continuous. Discrete is separate values e. g. 1, 2, 3, where as continuous is when the random variable can held any value within an interval e. g. between 0 and 1. Probability Mass Function: Probability Mass Function (or PMF) of a discrete random variable $X$ provides the probabilities $P(X=x)$ for all the possible values of $x$. Expectation: Expectation tells us what is the most likely outcome to expect. Expectation is the mean or average of the random variable: [E(X) = \sum_{x}xP(x)] Calculating Expectation is central to Information Theory. Variance: The variance of a random variable is a measure of whether the values of the random variable tend to be consistent over trials or to vary a lot. Variance is represented as $\sigma^2$ [\begin{eqnarray}var(X) &amp;=&amp; E((X-E(X))^2 \nonumber &amp;=&amp; E(X^2)-E^2(X))\end{eqnarray}] Standard Deviation: Standard Deviation is the square root of variance. It is represented as $\sigma$. Joint Probability Mass Function: The joint probability of two events $x$ and $y$ is represented as: [P(x,y) = P(X=y, Y=y)] Marginal Distribution: Marginal PMFs total up the probability masses for the values of each variable separately. [P_X(x) = \sum_y P(x,y)] [P_Y(y) = \sum_x P(x,y)] Relative Frequency: The proportion of times a certain outcome occurs is called the relative frequency of the outcome. $P$ - probability function $p$ - probability mass function We take a parametric approach to estimate the probability function in language. Non-parametric approaches are used in classification, or when the underlying distribution of the data is unknown. DistributionsThe types of functions for probability mass functions are called distributions  Discrete Distributions: Binomial discrete distributions are a series of trails with 2 outcomes only.      Multi-nominal Distribution: A special case of binomial distribution is Multi-nominal Distribution where each trial has more than 2 basic outcomes   Bernoulli distribution: A special case of discrete distributions where there is only one trial.     Continuous distributions Continuous distributions are also known as Normal distributionsContinuous distribution: A continuous distribution is also known as a Normal Distribution or a Bell Curve. We also refer to them as Gaussians (often used in clustering). A Gaussian is represented as: [n(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} e^{\frac{-(x-\mu)}{2\sigma^2}}] Where $\sigma$ is the standard deviation and $\mu$ is the mean. It is also the probability density of observing a single data point $x$, that is generated from a gaussian distribution. Maximum Likelihood EstimateMaximum Likelihood Estimate or MLE helps us identify which values of $\mu$ and $\sigma$ should we use that produce a curve that explains or covers all the data points. It is the way to determine the most likely outcome to a set of trials. Bayesian Updating: The process of using prior to get posterior, posterior becomes the new prior, as new data comes.       [P(\Theta   data) = P(data   \Theta) \text{ } P(\Theta)]   Here $\Theta$ is what we are interested in and what we are trying to estimate. It represented the set of parameters. If we are trying to estimate the parameters of a gaussian distribution then $\Theta$ represents both the mean $\mu$ and the standard deviation $\sigma$ and $\Theta = \{\mu, \sigma\}$ Here $P(\Theta|data)$ is the posterior and $P(\Theta)$ is the prior. Prior belief is computed as the following:       [P(s   \mu_m) = m^i (1-m)^j]   Here:  $i$ = outcome of 1 counts $j$ = outcome of 2 counts $\mu_m$ is the model that asserts the outcome $s$ is a sequence of observationsLikelihood Ratio: Ratio computed between 2 models to see which of them is more likely to occur. People often take log likelihood ratio and see it the result is $&gt;$ 1 or $&lt;$ 1 The ratio to determine which theory is most likely to occur given a sequence of events $s$. Let $v$ be the first theory and $\mu$ be the seond theory.       [\frac{P(\mu   s)}{P(v   s)} = \frac{P(s   \mu) P(\mu)}{P(s   v)P(v)}]   If the ratio is $&gt;1$ we prefer theory $\mu$ (the numerator). If the ratio is $&lt;1$ we prefer theory $v$ (the denominator). DerivationDerivation is the process of finding the maxima and minima of functions. Computing the partial derivative WRT $\mu$ and then setting the equation to $0$ gives us the MLE fo $\mu$ Bayes Optimal DecisionWhen we pick the best decision theory out of all the available theories that could explain the data, we make the Bayes Optimal Decision. "
    }, {
    "id": 16,
    "url": "http://localhost:4000/blog/2021/language-models/",
    "title": "The Foundations of Language Models",
    "body": "2021/04/24 - Language Models are models that are trained to predict the next word, given a set of words that are already uttered or written. e. g. Consider the sentence: “Don’t eat that because it looks…“ The next word following this will most likely be “disgusting”, or “bad”, but will probably not be “table” or “chair”. Language Models are models that assign probabilities to sequences of words to be able to predict the next word given a sequence of words. The probability of a word $w$ given some history $h$ is $p(w|h)$. In this post I am going to discuss the following concepts:  N-Grams     Working Example   Practical Issues    Problems with language Models Smoothing     Laplace Smoothing   Add-k smoothing   Backoff and Interpolation    Evaluating Language Models     Intrinsic Evaluation   Extrinsic Evaluation         Perplexity           N-Gram Efficiency considerations ReferencesN-Grams: The probability of a word $w$ given the history $h$ is defined as:       [p(w   h)]   as $h$ is several tokens long, we can rephrase it as the probability of the ${n+1}^{th}$ word $w_{n+1}$ depends on the words $w_1, w_2 \cdots w_n$.       [p(w_{n+1}   w_1, w_2, w_3 \cdots w_n)]   One way to answer this is using relative frequency counts. i. e. count the number of times we see $w_1, w_2, w_3 \cdots w_n$ and the number of times we see $w_1, w_2, w_3 \ldots w_n$ followed by $w_{n+1}$. Relative frequency is defined as the ratio of an observed sequence to the observed sequence followed by a suffix. Using the concept of relative frequency we can get:       [P(w_{n+1}   w_1 w_2 w_3 \ldots w_n) = \frac{C(w_1 w_2 w_3 \cdots w_n w_{n+1})}{C(w_1 w_2 w_3 \ldots w_n)}]   Where $C(x_1, x_2)$ is the count or the number of times we see a pattern of token $x_1$ followed by $x_2$. This approach of getting probabilities from counts works well in many cases, but if we wanted to know the joint probability of an entire sequence of words $p(w_1, w_2, w_3 \ldots w_n)$, we would have to compute - out of all the possible combinations of size $n$ how many are this exact sequence $w_1, w_2, w_3 \ldots w_n$. It fails when the size of the sequence is very long. Even the entire web isn’t big enough to compute such probabilities, because there are not enough examples for every word combination even on the world wide web). For this reason, we will have to introduce clever ways of computing probability. Let’s decompose this joint probability into a conditional probabilities using the chain rule of probability. Chain Rule of Probability helps us decompose a joint probability into a conditional probability of a word given previous words. Joint Probability of $n$ words is the probability of $n$ words occurring together. Using the chain rule, we can break down Equation the joint probability of a sequence of tokens into conditional probabilities. Here is how we do it: [\begin{eqnarray}P(w_1 w_2 w_3 \cdots w_n w_{n+1}) &amp;=&amp; P(w_1) P(w_2|w_1) P(w_3|w_1 w_2) \ldots P(w_n|w_1\ldots w_{n-1}) \nonumber \ &amp;=&amp; \prod_{k=1}^{n+1} P(w_k|w_1 \ldots w_k)\end{eqnarray}] The chain rule still has the constraint of needing the probability of a long sequence of previous words. One idea is to approximate this using the Markov Assumption. Markov Assumption says that the probability of a word only depends on the previous word and not the entire sequence of tokens preceding it. According to this assumption, we can predict a word without looking too much into the previous history of words. So, instead of working with the exact probabilities of a long sequence of preceding words, we can use a small window of preceding words. This is where we introduce a bigram model (that uses the preceding word only) a trigram model (that uses the preceding two words) to predict the probability of a sequence of tokens. Using the bigram model we can simplify the probability of a sequence of tokens to the following:       [P(w_1, w_2, w_3 \cdots w_n, w_{n+1}) = \prod_{k=1}^n P(w_k   w_{k-1})]   Similarly using the trigram model we can simplify the probability of a sequence of tokens to the following:       [P(w_1, w_2, w_3 \cdots w_n, w_{n+1}) = \prod_{k=1}^n P(w_k   w_{k-1} w_{k-2})]   Now that we have simplified the RHS, we need to compute the probabilities. Maximum Likelihood Estimation (MLE) is an intuitive way of measuring the parameters of an N-gram model, by computing the counts of words or tokens that exist together, normalized by the total counts so that the output is a probability that is between 0 and 1. Relative frequency is one way of measuring the MLE. Relative frequency can be computed using equation (7) below and Figure (1) explains the equation:       [P(w_n   w_{n-1}) = \frac{C(w_{n-1}w_n)}{\sum_w C(w_{n-1}w)}]         Fig 1: Relative Frequency Explained Working Example: Let’s build a small language model using bigrams, and use our model to predict the probability of a new sentence. For our toy example, consider a tiny corpus of the following sentences:  &lt;s&gt; I am Sam &lt;\s&gt;  &lt;s&gt; Sam I am &lt;\s&gt;  &lt;s&gt; I do not like that Sam I am &lt;\s&gt; Here &lt;s&gt; represents the start of a sentence and &lt;\s&gt; represents the end of the sentence. This corpus has the following lexicon or unique words:       token   frequency       I   4       am   3       Sam   3       do   1       not   1       like   1       that   1   Computing the conditional probabilities       [P(I   &lt;s&gt;) = \frac{2}{4}]         [P(Sam   &lt;s&gt;) = \frac{1}{3}]         [P(am   I) = \frac{3}{4}]   We can continue computing probabilities for all different possibilities of bigrams, then for any new sentence such as the following:  I like Sam We can compute the join probability of the tokens of this sentence from the conditional probability of the bigrams $P(like|I)$ and $P(Sam|like)$. Practical Issues:  Always use Log probabilities. Multiplication in linear space is addition in log space, and we will avoid numerical underflow It is typical to use trigrams instead of bigrams (although we illustrated bigrams in the example above) There are often unknown words in the sentence we want to predict the probability of, and we need to handle that.  We often won’t find instances of joint probability in our corpus, and we need to account for that. E. g. in the above example we do not have an instance of ‘sam like’, and so the conditional probability of $P(Sam|like)$ will be $0$. Problems with language Models: Language Models face the issue of sparsity - which means that the training corpus is limited and some perfectly acceptable English word sequences are bound to be missing from it. This means that it is possible to have several n-grams with a probability of $0$, but should actually have a non-zero probability. Another issue with language models is that the vocabulary the the language model is trained on might not have seen words from the test dataset - introducing the issue of unknown words or out of vocabulary words(OOV). One way to deal with OOV words is to replace all words with a frequency below a certain threshold by ‘UNK’. Other ways to deal with this is to use smoothing and discounting techniques. Smoothing: There will always be unknown words in the test dataset that the language model will have to work with that sparsity. To keep the language model from assigning zero probabilities to unseen events, we will shave off some probability mass from some more frequent events and give it to the events that we have never seen. This process is called smoothing or discounting. A few types of smoothing are:  add-1 smoothing (or Laplace smoothing) add-k smoothing backoff and interpolation stupid backoff Kneser ney smoothingLaplace Smoothing: Laplace smoothing involves adding $1$ to all of the bigram (or n-gram) counts before we normalize them into probabilities. [P(w_i) = \frac{c_i}{N}] If we add $1$ to each probability, and there are $V$ words in the vocabulary, we will need to add $V$ to the denominator as we add $1$ to each numerator. [P_{laplace}(w_i) = \frac{c_i}{N+V} \label{laplace}] In equation \ref{laplace}, $w_i$ is the $i^{th}$ word, $N$ is a normalizer (total number of words) and $V$ is the vocabulary size. But instead of adding to the numerator and denominator, a better way to is change the numerator only, and show how it affects smoothing by describing an adjusted count $c^*$. [c_i^* = (c_i + 1) \frac{N}{N+V}] Now we can convert each adjusted count into a probability by dividing it by $N$ (the total number of tokens) Although Laplace smoothing isn’t the best type of smoothing as it gives away a lot of probability mass to infrequent terms, it is still used and is practical for classification. ** Question: How can you use Language Models for classification? **       [P(w_n   w_{n-1}) = \frac{C(w_{n-1} w_n)}{C(w_{n-1})}]   Using Laplace Smoothing, this becomes:       [p^*_{Laplace}(w_n   w_{n-1}) = \frac{C(w_{n-1} w_n) + 1}{\sum_w C(w_{n-1}w) + V}]   Add-k smoothing: Add-K smoothing is an alternative to add $1$ smoothing, where we move a bit less of the probability mass from the seen events to the unseen events.       [p^*_{add-k}(w_n   w_{n-1}) = \frac{C(w_{n-1} w_n) + k}{\sum_w C(w_{n-1}w) + kV}]   Add-K requires us to have a method for choosing our k (0. 5? 0. 1? 0. 05?) e. g. one can optimize over a dev set or some other data source. Backoff and Interpolation: Backoff is an approach for smoothing using which we only backoff to a lower order n-gram when we have zero evidence for a higher level n-gram. So, we use a trigram if the evidence is sufficient, but if such a trigram does not exist, we backoff to a bigram, and if the bigram does not exist, we backoff to a unigram. Interpolation is an approach is using a mixture of probability estimates from all the n-gram estimators. For instance, if we are looking at trigrams, we would compute its probability by combining the trigram, bigram and unigram counts. Interpolation for a trigram can be defined by the following formula:       [\hat{p}(w_n   w_{n-2}w_{n-1}) = \lambda_1 p(w_n   w_{n-2}w_{n-1}) + \lambda_2 p(w_n   w_{n-1}) + \lambda_3 p(w_n)]   where $\sum_i \lambda_i = 1$ The values of $\lambda$ can be computed by optimizing over a heldout dataset. EM Algorithm (Expectation Maximization Algorithm) is an iterative learning algorithm that converges on locally optimal $\lambda$s Evaluating Language Models: There are 2 ways to evaluate language models:  intrinsic evaluation: Evaluation of the model as a measure of how much it improves the application that it is used in.  extrinsic evaluation: Measure of the quality of the model independent of any applicationIntrinsic Evaluation: For intrinsic evaluation of a language model we need to have:  training dataset test dataset held out datasetWe use the language model to compute scores on the test dataset, and use the heldout and training dataset to optimize out language model. Extrinsic Evaluation: To compute extrinsic evaluation of a language model, we compute the effect of a new language model on the final end to end product that it is integrated with. Good scores during intrinsic evaluation does not always mean better scores during extrinsic evaluation, which is why both the types of evaluation are important. Perplexity: Perplexity is the measure of computation of the probabilities learned from the training dataset and applied on the test dataset. Perplexity is represented as $PP$ and is measured as the inverse probability of the test set, normalized by the number of words. [\begin{eqnarray}PP(w) &amp;=&amp; P(w_1 w_2 w_3 \ldots w_n)^{-\frac{1}{n}} \nonumber &amp;=&amp; \sqrt[n]{\frac{1}{P(w_1 w_2 w_3 \ldots w_n)}} \nonumber &amp;=&amp; \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i|w_1 w_2 \ldots w_{i-1})}}\end{eqnarray}] To compute perplexity of a bigram, we can simplify Equation (11) to the following:       [PP(w) = \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i   w_{i-1})}}]   Similarly, to compute perplexity of a bigram, we can simplify Equation (11) to the following:       [PP(w) = \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i   w_{i-1} w_{i-2})}}]    Minimizing perplexity is equivalent to maximizing the test set probability. If the perplexity is low, it means that the training data captures the probability of the test set really well. Another way to think about perplexity is to think of it as the weighted average branching factor of the language. The branching factor of a language is the number of possible next words that can follow any word. Intrinsic improvement in perplexity does not guarantee an extrinsic improvement in the performance of the language processing task. N-Gram Efficiency considerations: When a language model uses large sets of n-grams, it is important to store the efficiently. Below are some ways to store LMs efficiently:  Words: storing words in 64 bit hash representations, and the actual words are stored on disc as string Probabilities: 4-8 bits instead of 8 byte float n-grams: Stored in reverse tries Approximate language models can be created using techniques such as bloom filters n-grams can be shrunk by pruning i. e. only storing n-grams with counts greater than some threshold.  Efficient Language Models such as KenLM     Use sorted Arrays   Efficiently combined probabilities and backoffs into a single value   Use merge sorts to efficiently build probability tables in a minimal number of passes through a large corpus   References:  Chapter 3, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition by Daniel Jurafsky, James H. Martin Language Models, Wikipedia Lecture 2, Language Models, CS224n Stanford NLP NLP Lunch Tutorial: Smoothing, Bill MacCartney, Stanford NLP Relative Frequency, mathisfun. com"
    }, {
    "id": 17,
    "url": "http://localhost:4000/blog/2021/logistic-regression/",
    "title": "The Comprehensive Guide to Logistic Regression",
    "body": "2021/04/23 - In Natural Language Processing (NLP) Logistic Regression is the baseline supervised ML algorithm for classification. It also has a very close relationship with neural networks (If you are new to neural networks, start with Logistic Regression to understand the basics. ) In this post I will discuss the following:  Introduction Components of a Classification System     Logistic Regression Phases    Feature Representation Classification using the Sigmoid Function     Characteristics of Sigmoid Function    Learning Process in Logistic Regression     Cost Function: Cross Entropy Loss         Cross Entropy     Convex Optimization Problem     Decision Boundary          Gradient Descent         Learning Rate     Stochastic Gradient Descent     Batch Training     Mini-Batch Training          Regularization         L2 Regularization     L1 Regularization           Multinomial Logistic Regression     Loss Function in Multinomial Logistic Regression    Working Example of Logistic Regression     Gradient Descent Step 1   Gradient Descent Step 2    Summary of Logistic Regression Further ReadingIntroductionLogistic Regression is a discriminative classifier.  Discriminative models try to learn to distinguish what different classes of data look like. Some examples of discriminative classifiers are Logistic Regression, Neural Networks, Conditional Random Fields, and Support Vector Machines. Naive Bayes, is a generative classifier.  Generative models have the goal to understand what different classes of data look like. Some examples of generative classifiers include Naive Bayes, Bayesian Networks, Markov Random Fields, and Hidden Markov Models. LDA (Latent Dirichlet Allocation is a generative statistical model for topic modeling. Ng and Jordan, 2001 provide a great analysis of generative vs discriminative models. Components of a Classification SystemA Machine Learning system for classification has 4 components:  Feature Representation: For each input observation $x^{(i)}$, this will be a vector of features $[x_1, x_2, x_3, … , x_n]$ A Classification Function: This gets us the probability of the output, given an input. This is denoted by $P(y|x)$. Sigmoid and Softmax are tools for classification for Logistic Regression.  Objective Function for Learning: This is the function that we want to optimize, usually involving minimizing error on training examples. Cross Entropy Loss is the objective function for Logistic Regression.  Algorithm for Optimizing the Objective Function: We use the Stochastic Gradient Descent Algorithm for optimizing over our Objective Function. Logistic Regression Phases: Logistic Regression has two phases:  Training Phase: We train the system (specifically the weights $w$ and bias $b$) using Stochastic Gradient Descent and Cross Entropy Loss Test Phase: Given a text example $x$, we compute $p(y|x)$ and return the higher probability label. Feature RepresentationA single input observation $x$ can be represented by a vector of features $[x_1, x_2, x_3, \ldots , x_n]$. For Logistic Regression this is usually done by feature engineering which is the process of manually identifying which features are relevant to solve the problem, and convert the text features into real numbers. Feature Interactions: Feature Interaction is the combination of different features to form a more complex feature. Feature Templates: Feature Templates is when templates are used to automatically create features using abstract specification of features. Representation Learning: Representation Learning is the process of learning features automatically in an unsupervised way from the input. In order to avoid the excessive human effort of feature design, recent NLP efforts are focused on representation learning Classification using the Sigmoid FunctionThe result of Logistic Regression is: [z = (\sum_{i=1}^n w_i x_i) + b] Here, $w_i \cdot x_i$ is the dot product of vectors $x_i$ and $w_i$ and $z$ is a real number vector ranging from $-\infty$ to $+\infty$  Dot Product: The dot product of 2 vectors $a$ and $b$ is written as $a \cdot b$ and is the sum of the products of the corresponding elements of each vector. $z$ is not a legal probability. To make it a probability, $z$ will pass through the, written as $\sigma(z)$ [y = \sigma(z) = \frac{1}{(1+e^{-z})}]       Fig 1: Sigmoid Function (Image Credit: Wikipedia) Characteristics of Sigmoid Function:  Sigmoid takes a real valued number and maps it to the range [0, 1] (which is perfect to get a probability) Sigmoid tends to squash outlier values towards 0 or 1 Sigmoid is differentiable - which makes it handy for learning (A function is not differentiable if it has a undefined slope or a vertical slope)For classification into two classes: [\begin{eqnarray}p(y=1) &amp;=&amp; \sigma(w \cdot x) + b \nonumber   &amp;=&amp; \frac{1}{1+e^{w. x + b}}\end{eqnarray}] [\begin{eqnarray}p(y=0) &amp;=&amp; 1 - \sigma(w \cdot x) + b \nonumber &amp;=&amp; 1 - \frac{1}{1+e^{w. x + b}} \nonumber &amp;=&amp;\frac{e^{w. x + b}}{1+e^{w. x + b}}\end{eqnarray}] Learning Process in Logistic RegressionCost Function: Cross Entropy Loss: Cross Entropy Loss is a function that determines for an observation $x$, how close the output of the classifier $\hat{y}$ is to the correct output $y$. This $Loss$ is expressed as: $ L(\hat{y}, y) $ $ L(\hat{y}, y)$ is computed via a loss function that prefers the correct class labels of the training examples to be more likely. This is called conditional maximum likelihood estimation. We choose parameters $w$ and $b$ that maximize the probability of the true $y$ labels in the training data given the observations $x$. Given a Bernoulli Distribution (a distribution that can only have 2 outcomes):       [p(y   x) = \hat{y}^y (1-\hat{y})^{1-y}]   Taking log on both sides:       [\log{p(y   x)} = y\log{\hat{y}} + (1-y)\log{(1-\hat{y})}]   Equation (6) is what we are trying to maximize. In order to turn this into a loss function (something that we need to minimize), we just flip the sign on Equation $(6)$. The result is Cross Entropy Loss $L_{CE}$. [\begin{eqnarray}L_{CE}(\hat{y}, y) &amp;=&amp; -\log{ p(y|x)} \nonumber \ 1        &amp;amp;=&amp;amp; -[y \log{\sigma(w \cdot x + b) + (1-y)\log(1-\sigma(w \cdot x + b))}] \end{eqnarray}\]Equation (7) is known as the cross entropy loss. It is also the formula for the cross entropy between the true probability distribution $y$ and the estimated distribution $\hat{y}$. Cross Entropy: Cross Entropy is the measure of the difference between two probability distributions for a given random variable. Convex Optimization Problem: For logistic regression the loss function is convex, i. e. it has just one minimum. There is no local minima to get stuck in, so gradient descent will always find the minimum. The loss for multi-layer neural networks in non-convex, so it is possible to get stuck in local minima using neural networks. Decision Boundary: Decision Boundary is the threshold above which $\hat{y} = 1$, and below which $\hat{y} = 0$. This means that the decision boundary decides which class a given instance belongs to, based on the final probability of the item and the value of the decision boundary. [\hat{y} = \left{  \begin{array}{ll}    1 &amp; \mbox{if $p(y=1|x)$ $\gt$ 0. 5}    0 &amp; \mbox{otherwise}  \end{array}\right. ] Here 0. 5 is the decision boundary, and it is the threshold that decides which class an item belongs to. Gradient Descent: Gradient Descent finds the gradient of the loss function at the current point (by taking a differential of it), and then moves in the opposite direction of gradient. Learning Rate: The magnitude of the amount to move in gradient descent is the slope $ \frac{d}{dw} \; f(x,w)$ weighted by the learning rate $\eta$. [w^{t+1} = w^{t} - \eta \; \frac{d}{dw} \; f(x; w)] Learning Rate: A parameter that needs to be adjusted:  If the learning rate is too high, each step towards learning becomes too large and it overshoots the minimum If the learning rate is too low, each step is too small and it takes a long time to get to the minimum Start $\eta$ at a higher value and slowly decrease it, so that it is a function of the iteration $k$ in trainingStochastic Gradient Descent: It is called Stochastic Gradient Descent because it chooses a single random example at a time. It moves weights so that it can improve the performance of that single instance. Batch Training: When we use Batch Training, we compute the gradient over the entire dataset. It looks at all the examples for each iteration to decide the next step. Mini-Batch Training: We train a group of $m$ examples (where $m$ is 512, or 1024 or similar) that is less than the size of the entire dataset. When $m = 1$ it becomes Stochastic Gradient Descent again. Mini-Batch Training is more efficient than Batch Training and more accurate than Stochastic Gradient Descent. Mini-Batch Gradient Descent is the average of the individual gradients. Regularization: Regularization is used to avoid overfitting and to generalize well for the test data. If the model fits the training data too well, it will not be able to handle new cases presented in the test data. A regularization term $R(\theta)$ is added to the objective function. i. e. the function that computes $\hat{\theta}$, where $\hat{\theta}$ is the next set of $w$ and $b$ parameters. Once we add regularization, we can find $\hat{\theta}$ as: [\hat{\theta} = argmax_{\theta} \sum_{i=0}^m log(p(y^{(i)}|x^{(i)})) - \alpha R(\theta)] Here we are maximizing the log probability instead of minimizing the loss (Equation (7)). $R(\theta)$ is used to penalize large weights. The intuition behind this is that if the model matches the training data perfectly, but uses large weights, then it will be penalized more than the case where the model matches the training data a little less, but does so using small weights. L2 Regularization: L2 Regularization is the euclidean distance of the vector squared from the origin. $ R(\theta) = || \theta ||_2^2$ Is the notation of L2 Norm $ R(\theta) = \sum_{j=1}^n \theta_j^2$ Is how we can compute L2 Norm L2 regularization is easy to optimize because of it’s simple derivative. It prefers weight vectors in smaller weights. L1 Regularization: L1 Regularization is the linear function named after L1 norm or Manhattan Distance, which is the sum of the absolute values of the weights. $R(\theta) = || \theta ||_1$ is the notation of L1 norm L1 Regularization is hard to differentiate as the derivative of $| \theta |$ is non continuous at 0. It prefers sparse solutions with larger weights. Multinomial Logistic Regression[Multinomial Logistic Regression]() is also known as Softmax Regression or Maxent Classifier. In Multinomial Logistic Regression the target variable $y$ ranges over more than two classes. In order to support more than 2 classes, Multinomial Logistic Regression uses the softmax function instead of the sigmoid function. For a vector $Z$ of dimensionality $k$ the softmax function is defined as: [Softmax(Z_i) = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}] In the above equation the denominator helps normalize the values into probabilities.  Like Sigmoid, Softmax has the property of squashing values towards 0 or 1 If one of the inputs is larger than the others, it will tend to push its probability towards 1, and suppress the probabilities of the smaller inputs. Loss Function in Multinomial Logistic Regression: To compute the loss function in Multinomial Regression, we need to account for $k$ classes that a given item can belong to (instead of just 2 classes)\(\begin{eqnarray}L_{CE}(\hat{y}, y) &amp;=&amp; - \sum_{k=1}^k 1 \{y=k\} log p(y=k|x) \nonumber \\&amp;=&amp; - \sum_{k=1}^k 1 \{y=k\} log \frac{e^{w_k \cdot x + b}}{\sum_{j=1}^k e^{w_j \cdot x + b_j}}\end{eqnarray}\) In equation (11) $ 1{ }$ evaluates to 1 if the condition in the brackets is true, and $0$ otherwise. Working Example of Logistic RegressionConsider a simple scenario where we are doing sentiment analysis of a dataset, and we have 2 classes: positive and negative (there is no neutral class in our hypothetical scenario). In this example we will be using the sigmoid function (because we have only 2 classes, and for simplicity we will not be using any regularization. The following are the matrices representing the actual outcome $y$ and the features corresponding to it $x$. $y = \begin{pmatrix}y\\1 \\ 0\end{pmatrix} \;\; x = \begin{pmatrix}x1 &amp; x2 \\ 3 &amp; 2\\ 1 &amp; 3\end{pmatrix}$ In this example we have only 2 features $x_1$ and $x_2$. $x1$ is the number of positive words found in the sentence and $x2$ is the number of negative words found in the sentence. Also, $y=0$ represents negative (or bad sentiment), and $y=1$ represents positive (or good sentiment). This stage is feature extraction and representation of our data. The next state is to initialize out initial weights. For this example, we are setting $w_1 \; = \; w_2 \;= \; b \;= \; 0$. ($w_1$ is the weight of feature $x_1$ and $w_2$ is the weight of feature $x_2$), $\beta$ is our bias term. $\eta \;= \; 0. 1$ Each step in learning for logistic regression is represented by the following formula: [\theta^{t+1} = \theta^t - \eta \; \delta_\theta \; L(f(x^{(i)}, \theta), y^{(i)})] Here is a breakdown of each of the components of the formula.       Fig 2: Each step for Learning in Logistic Regression Gradient Descent Step 1: Considering the first row, where $y=1$ finding the gradient for this example: $ \delta_{wb} = \begin{pmatrix} \frac{\partial L_{CE}(w, b)}{\partial_{w_1}} \\ \frac{\partial L_{CE}(w, b)}{\partial_{w_1}} \\ \frac{\partial L_{CE}(w, b)}{\partial_{b}} \end{pmatrix}$ $ \delta_{wb} = \begin{pmatrix} (\sigma (w \cdot x + b) - y)) \; x_1 \\ (\sigma (w \cdot x + b) - y)) \; x_2 \\ \sigma (w \cdot x + b) - y \end{pmatrix}$ We know that initial $w, b = 0$. Substituting these values: $ \delta_{wb} = \begin{pmatrix} (\sigma (0) - 1)) \; x_1 \\ (\sigma (0) - y)) \; x_2 \\ \sigma (0) - y \end{pmatrix}$ Given that $\sigma(0) = 0. 5$ (See the sigmoid image above) and $x_1 = 3$ and $x_2 = 2$ for the first example in matrix $x$ (first row in matrix) $ \delta_{wb} = \begin{pmatrix} (0. 5 - 1) * 3 \\ (0. 5 - 1) * 2 \\ 0. 5 - 1 \end{pmatrix}$ $ \delta_{wb} = \begin{pmatrix} -1. 5 \\ -1. 0 \\ -0. 5 \end{pmatrix}$ Now that we have the gradient, we can compute $\theta_1$ by moving in the opposite direction as the gradient. $ \theta_{1} = \begin{pmatrix} w_1 \\ w_2 \\ b \end{pmatrix} - \eta \begin{pmatrix} -1. 5 \\ -1. 0 \\ -0. 5 \end{pmatrix}$ Given that $w_1 = w_2 = b = 0$ and $\eta = 0. 1$: $ \theta_{1} = \begin{pmatrix} 0. 15 \\ 0. 1 \\ 0. 05 \end{pmatrix}$ The weights after one step of gradient descent: $w_1 = 0. 15$, $w_2 = 0. 1$, $b = 0. 05$. Gradient Descent Step 2: This time we will consider row 2 of our matrix. The weights learned so far are : $w_1 = 0. 15$, $w_2 = 0. 1$, $b = 0. 05$The values from our matrix are $x_1 = 1$, $x_2=3$, $y=0$. From Equation (9) computing the gradient: $ \delta_{wb} = \begin{pmatrix} (\sigma (w \cdot x + b) - 0)) \; x_1 \\ (\sigma (w \cdot x + b) - 0)) \; x_2 \\ \sigma (w \cdot x + b) - y \end{pmatrix}$ $ \delta_{wb} = \begin{pmatrix} (\sigma (0. 15 * 1 + 0. 1 * 3 + 0. 05) - 0) \; 1 \\ (\sigma (0. 15 * 1 + 0. 1 * 3 + 0. 05) - 0) \; 3 \\ \sigma (0. 15 * 1 + 0. 1 * 3 + 0. 05) - 0 \end{pmatrix}$ $ \delta_{wb} = \begin{pmatrix} (\sigma (0. 15 + 0. 3 + 0. 05) - 0) \; 1 \\ (\sigma (0. 15 + 0. 3 + 0. 05) - 0) \; 3 \\ \sigma (0. 15 + 0. 3 + 0. 05) - 0 \end{pmatrix}$ Sigmoid of 0. 5 is 0. 622. (Compute your own sigmoid) $ \delta_{wb} = \begin{pmatrix} (0. 622 - 0) \; 1 \\ (0. 622 - 0) \; 3 \\ 0. 622 - 0 \end{pmatrix}$ $ \delta_{wb} = \begin{pmatrix} 0. 378 \\ 1. 134 \\ 0. 378 \end{pmatrix}$ Now that we have the gradient, we can compute $\theta_2 $ by moving in the opposite direction as the gradient. $ \theta_{2} = \begin{pmatrix} w_1 \\ w_2 \\ b \end{pmatrix} - \eta \begin{pmatrix} 0. 378 \\ 1. 134 \\ 0. 378 \end{pmatrix}$ $ \theta_{2} = \begin{pmatrix} 0. 15 \\ 0. 1 \\ 0. 05 \end{pmatrix} - 0. 1 \begin{pmatrix} 0. 378 \\ 1. 134 \\ 0. 378 \end{pmatrix}$ $ \theta_{2} = \begin{pmatrix} 0. 15 - 0. 0378 \\ 0. 1 - 0. 1134 \\ 0. 05 - 0. 0378 \end{pmatrix} $ $ \theta_{2} = \begin{pmatrix} 0. 1122 \\ -0. 0134 \\ 0. 4622 \end{pmatrix} $ At the end of step 2, $w_1 = 0. 1122$, $w_2 = -0. 0134$ and $b = 0. 04622$. We can continue this process for $k$ number of steps and iterate through the examples again and again till we find the global minimum. Summary of Logistic Regression Each input is composed of a vector $x_1, x_2 \ldots x_n$ We compute $\hat{y} = \sigma(w \cdot x + b) Compute loss = $ \hat{y} - y$. We use cross entropy loss to compute this value Compute the gradient of the loss = $\frac{d}{dc}L_{CE}$ Sigmoid is replaced by Softmax when we do multinomial Logistic Regression Regularization is used to avoid overfitting and make the model more generalizedFurther Reading On discriminative vs. generative classifiers: a comparison of logistic regression and naive Bayes NIPS’01: Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, January 2001 Pages 841–848 Chapter 5, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition by Daniel Jurafsky, James H. Martin Introduction to Latent Dirichlet Allocation, by Edwin Chen"
    }, {
    "id": 18,
    "url": "http://localhost:4000/blog/2021/byte-pair-encoding/",
    "title": "What is Byte-Pair Encoding for Tokenization?",
    "body": "2021/01/28 - Tokenization is the concept of dividing text into tokens - words (unigrams), or groups of words (n-grams) or even characters. Morphology traditionally defines morphemes as the smallest semantic unit. e. g. The word Unfortunately can be broken down as un - fortun - ate - ly [[un [[fortun(e) ]\(_{ROOT}\) ate]\(_{STEM}\)]\(_{STEM}\) ly]\(_{WORD}\) Morphology is little studied with deep learning, but Byte Pair Encoding is a way to infer morphology from text. Byte-pair encoding allows us to define tokens automatically from data, instead of precpecifying character or word boundaries. This is especially useful in dealing with unkown words. Modern Tokenizers: Modern tokenizers often automatically induce tokens that include tokens smaller than words - called Subwords. E. g. the subwords “-ly”, “-ing” give us an ideal about the type of the word - which is what subword tokenization aims to do. Most tokenizers have two parts:  A token learner: takes a raw training corpus and indices a vocabulary - a set of tokens.  A token segmenter: takes a raw test sentence and segments it into the tokens in the vocabulary. Three algorithms are widely used :  Byte Pair Encoding (Sennrick et. al 2016) Unigram Language Modeling (Kudo 2018) Wordpiece (Schuster and Nakajima 2012) and Sentencepiece (Kudo and Richardson, 2018)Byte Pair Encoding (BPE) is the simplest of the three. Byte Pair Encoding (BPE) Algorithm: BPE runs within word boundaries. BPE Token Learning begins with a vocabulary that is just the set of individual characters (tokens). It then runs over a training corpus ‘k’ times and each time, it merges 2 tokens that occur the most frequently in text. e. g. ‘e’ and ‘r’ are merged into a single token ‘er’ when they occur together in the same order. At the end of ‘k’ iterations, the algorithm produces a list of most frequent ‘k’ tokens along with the original set of characters. 12345678def byte_pair_encoding(string_list: List[str], k: int) -&gt; vocab: List[str]:	 vocab = &lt;list of unique characters in string_list&gt;	 for i in range(0, k+1):	 	c_left, c_right = most frequent pair of adjacent tokens in string_list	 	c_new = c_left + c_right # create a new bigram	 	vocab = vocab + c_new # add the bigram to teh vocabulary	 	replace each occurence of c_left, c_right with c_new # update the corpus	 return(vocab)Once the token learner learns the vocabulary, the token parser is used to tokenize a test sentence from teh learned tokens that were leraned from teh training data. In real applications of BPE algorithms BPE is run with many thousands of merges such that most words are represented as tokens and only the rare words are represented by their parts. Byte-Pair Encoding was originally a compression algorithm where we replace the most frequent byte pair with a new byte - thereby compressing the data. For further reading check out this NLP class slides from Stanford or this chapter on Text Normalization from the Jurafsky and Martin Textbook References:  T. Kudo. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates. 2018 T. Kudo and J. Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. 2018 M. Schuster and K. Nakajima. Japanese and Korea Voice Search. 2012 R. Sennrich, B. Haddow and A. Birch. Neural Machine Translation of Rare Words with Subword Units. ACL 2016"
    }, {
    "id": 19,
    "url": "http://localhost:4000/blog/2020/willpower/",
    "title": "Willpower",
    "body": "2020/01/12 - In this book Roy Baumeister and John Tierney talk about Willpower, why it is important, how successful people have implemented the power of willpower in their own lives, and how to build and practice our own muscle of “willpower”. Success has attributed to 2 qualities: intelligence and self control. Researchers have not been able to permanently increase intelligence, but they have discovered how to improve self-control. Most major problems center in self control - e. g. compulsive spending, impulsive violence, unhealthy diet etc. People spend about a quarter of their waking hours resisting desires - at least 4 hours per day. And that doesn’t include the instances in which willpower is exercised. Every successful person has the quality of self-control, but it is rarely ever mentioned as a reason for success (It is almost always mentioned as the reason for failure). In this book Baumeister and Tierney talk about how people who have mastered self control are more successful, in all aspects of their life as compared to their counterparts. Ultimately self-control lets you relax because it removes stress and enables you to conserve willpower for the important challenges. Is willpower more than a metaphor?: “Ego Depletion” - Baumeister’s term for describing people’s diminished capacity to regulate their thoughts, feelings and actions. When the brain was wired up with an EEG to monitor the anterior cingulate cortex (or the conflict monitoring system or error detection system), it was discovered that people who act against what they actually feel, face ego depletion and are not able to do subsequent tasks correctly. E. g. 50% of the people were showed a sad movie but were asked to suppress their emotions. These people went on to not being able to do unrelated tasks properly after the movie, compared to their counterparts who did not go through ego-depletion. Studying thousands of people inside and outside a laboratory, experiments consistently demonstrated:  you have a finite amount of willpower that becomes depleted as you use it you use the same stock of willpower for all manner of tasksWe can divide the uses of willpower into 4 broad categories:  control of thoughts: earworms such as a song stuck in your head, to ways in which we deceive ourselves (e. g. Tiger woods convinced himself that monogamy didn’t apply to him, bankers convinced themselves that there was no problem issuing loans to people having no income and no assetts) control of emotions: controlling feeings - e. g. trying to escape bad moods impulse control: the ability to resist temptations like alcohol, tobacco, Cinnabons and cocktail waitresses performance control: Focusing your energy on the task at handIf you are trying to make a big change in your life that requires willpower - make sure to focus only on that change. The limited willpower that one has is used for all aspects of their life - no matter how unrelated. e. g. quitting alcohol, saying good things about bullys - both share the same willpower resources. Willpower isn’t just a metaphor. There is power driving this virtue. Where does the power in Willpower come from?: There is a very strong correlation between glucose in the body and willpower. The more glucose that is converted from the bloodstream and pumped into the brain, the higher is the person’s willpower. No glucose, no willpower. When people have more demands for self-control in their daily life, their hunger for sweets increases. Not hunger for food in general, but the hunger for sweets. Don’t try to skimp on calories when working on serious problems (more serious than being overweight). Sugar can be used effectively to boost your self control right before a brief challenge - like a math test. To get your glucose, go for the “slow burn” - i. e. food that takes a while to digest (low glycemic index) and can sustain you got a longer time. Sleep also affect how effective the body is to create glucose. Which is why having a good nights sleep is beneficial for willpower. "
    }, {
    "id": 20,
    "url": "http://localhost:4000/blog/2020/the-hard-thing-about-hard-things/",
    "title": "The Hard Thing about Hard Things",
    "body": "2020/01/11 - This book is one of the first books to read as an entrepreneur. Ben Horowitz candidly talks about the struggles, and hard things about starting, running and growing companies. Here is a Techcrunch article reviewing this book. The beginning: A bully told Ben to ask another kid for his wagon, and if the other kid refuses, to spit in his face and call him profanity. Ben reluctantly went to the other kid and politely asked to play with his wagon, and the other kid obliged. This is how Ben met his best friend - by doing something uncomfortable. It taught him not to judge things by their surfaces, and that there are no shortcuts to anything. Some tidbits of learning:  The most important rule of raising money privately: Look for a market of one. You only need one investor to say yes, so it is better to ignore the other thirty who say “no” The best things about startups is that you only ever experience two emotions - euphoria and terror, and the lack of sleep enhances them both.  You need 2 kinds of friends in your life - the one you can call when things are great and who can be happy for you, and one you can call when things are going bad and who is your lifeline in that bad time.  Have mentors who have gone though some horrible and devastating circumstances so that they can advice you how to avoid them Startup CEOs should not play the odds. WHen you are building a company, you must believe that there is an answer and you cannot pay attention to your odds of finding it. When things fall apart: The secret to being a successful CEO is the ability to focus and make the best move when there are no good moves. Struggle in your company is not failure - but it often causes failure - if you are not strong enough to endure it. How to deal with struggle:  Divide and conquer - seek help, don’t put it all on your own shoulders Don’t underestimate the complexity - this is chess not checkers.  Play long enough and you might get lucky Do not take it personallyCEOs should tell it like it is  ignoring the negatives does not pain a realistic picture and makes you appear less trustworthy If there is a problem, it is better for the team to find a solution together, rather that the CEO to shelter the team from itIf you ever need to fire your employees, executives, or demote a friend - do it the right way. When you are in a trouble, nobody really cares - not employees, customers, media or investors. Your time is best spent on figuring out a solution, thank worrying about what anyone thinks of you. Take care of the people, the products and the profits - in that order:  when hiring executives, hire for strength rather than lack of weakness.  If you are looking for a genius who might not be the best cultural fit, know what you are getting into, and hire them for the genius that they bring to the team Being a good company doesn’t matter when things go well, but it can be the difference between life and death when things go wrong. Things always go wrong! It is important to train your employees     It improves productivity   Sets expectations   Improves quality and throughput   Increases employee retention as they will strive hard to reach specific targets   Best place to start is management and new employees         Good Product Managers   Bad Product Managers         Know the marker, product, product line and competition extremely well   Work with excuses - not enough funding, too much competition etc.        Crisply define the target and the “what”   Feel good about themselves when they figure out the “how”       Create collateral such as FAQs presentations   Complain about spending their time answering questions       Take written positions on important issues   Use verbal communication       Focus the team on revenue and customers   Focus the team on how many features the competitors are building       Decompose problems into segments   Combine all problems into one       Think about the story they want to be written in the press   Think about covering every feature and being technically accurate with the press       Articulate simply and clearly   Never explain the obvious       Disciplined execution of status reports   Don’t value discipline   Should you hire your friends employees?  probably not, if you don’t want to do the same to youIssues with hiring big company executives:  Rhythm mismatch - exec is used to waiting for incoming communication rather than taking action Skill-set mismatch - Large company execs are good at managing complex organizations, process improvement etc, which isn’t quite where a smaller organization skill requirement is. These problems can be allayed by screening for mismatches early on and asking questions like:  what will you do in your first month on the job how will your new job differ from your current job why do you want to join a small companyOnce a candidate is on board, aggressively integrate them in your company by doing the following:  force them to create and produce immediately make sure they “get it” make sure they connect with and initiate connections with everyone in the organizationWhile hiring executives:  Know what you want Run a process the figures out the right match Sometimes make the lonely decision that a CEO has to makeLike technical debt, be careful of management debt. Here are a few ways in which management debt can arise:  Putting two people together to manage Overcompensation a key employee because they get another job offer No performance management of employee feedback processConcerning the growing concern: Sometimes an organization doesn’t need a solution, it just needs clarity. E. g. Ben’s employees were uncomfortable with the amount of profanity used at the company. He offered an explanation and suggestion to use it in the right context of work and urgency, and not to intimidate or sexually harass anymore. He compared it to the word “cupcake”. It is okay to say - “Those cupcakes you baked were delicious”, but it is not okay to say “Hey cupcakes, you look fine in those jeans. “ No one likes corporate politics, however it often unintentionally develops in an organization by sometimes incentivizing political behavior (unwittingly). E. g. if a senior executive requests a pay raise, with a competitive offer at hand, a CEO might offer them additional compensation to have them stay. Although it might seem innocent, this is strong incentive for political behavior as the CEO is rewarding behavior that has nothing to do with advancing the business. This is bad because:  others in the staff would like to ask for raises as well and this doesn’t seem performance driven but arbitrary less aggressive members might be denied off cycle raises people start optimizing over total compensation rather than growth of the productHow to minimize politics:  Hire people with the right kind of ambition and not necessarily political.  Build strict process for potentially political issues and do not deviateSmart people are sometimes bad employees:  when they are disempowereed when they are fundamentally a rebel when they are immature/naiveHiring senior people (older people - who have been there) to your startup is like taking performance enhancing drugs. If all goes well, you will achieve incredible new heights. If all goes wrong, you will start degenerating from the inside out. It is challenging to hire senior employees:  they come with their own culture they know how to work the system you do not know the job as well as they doIt is important to have effective one on ones with employees. Here are a few questions:  if we could improve in any way, how could we do it? what is the #1 problem with our organization? why? what is not fun about working here? who is really kicking ass in the company? who do you admire? if you were me, what changes would you make? what don’t you like about the product? what is the biggest opportunity that we are missing out on? what are we not doing that we should be doing? are you happy working here? When you are building a company and a product, you should have the following covered:    You should be at least 10 times better than the competition (one 2 or 3 times better)  You should take the market before anyone else does If you are in the right direction with your product and market, company culture is the next important thing because it:  distinguishes you from the competitors ensures and defines operating values helps identify employees who fit with your missionE. g. Amazon has frugality as their culture as they made employees use cheap doors from Home Depot as desks. E. g. Facebook has the motto of “Move fast and break things”, which make people stop and realize that they need to move fast and innovate, which might lead to breaking things - which is accepted. How to lead even when you don’t know where you are going: As an entrepreneur one needs to stop focusing on the things you did wrong, or might do wrong. The most difficult skill to learn as a CEO is the ability to manage their own psychology. Everything else is relatively straightforward as compared to keeping the mind in check. As CEO - nothing prepares you to run a company (not being a manager, etc). You learn how to run a company, by running a company. If CEOs were graded on a curve, the mean on the test would be 22 out of 100. This is because:  there is always broken stuff, and CEOs either take it too personally, or not personally enough It is a lonely job being a CEOGreat CEOs face the pain, and the torture. Great CEos don’t quit.  There is no difference in how you feel when you are a hero or a coward. The fear is the same. The only difference is what you do with that fear that makes you either a hero, or a coward. Great CEOs need to trust themselves and might have to go against the wishes of their team - for hiring, etc. for the benefit of the company. There are 2 types of CEOs in the world:  ONES - managers who are happier setting the direction of the company     most founding CEOs tend to be ones   enjoy making decisions   strategic minds and enjoy the 8 dimensional chess with their competitors    TWOS - those who enjoy making the company perform at the highest level     enjoy the process of making the company run well   often have difficulty with the strategic process itself   A good CEO needs to have the qualities of ONES and TWOS. Most organizations are run by ONES and have a team of TWOS. What makes people follow a leader?  The ability to articulate the vision The right kind of ambition The ability to achieve the visionThere are 2 modes of being a CEO:  when facing an imminent threat - Wartime CEO when it is business as usual - peacetime CEOIt is an art to be able to be a wartime and peacetime CEO. It takes a long time to develop CEO skillset, and it is hard to tell whether someone can develop it or not. How to give feedback as a CEO:  be authentic some from the right place (you want them to succeed) don’t get personal don’t embarrass anyone in front of their peers feedback is not one size-fits all be direct, but not mean feedback is a dialog, not a monologueFirst rule of entrepreneurship - there are no rules: In technology business you rarely know everything upfront, and letting people be creative can be the difference between magical and mediocre. "
    }, {
    "id": 21,
    "url": "http://localhost:4000/blog/2020/how-to-win-friends-influence-people/",
    "title": "How to Win Friends and Influence People",
    "body": "2020/01/06 - This book is a timeless classic by Dale Carnegie. In this book Carnegie goes into great depth about how to win hearts of everyone around you - employees, employers, students, mentors, friends, spouses, children etc. Carnegie provides countless examples and specific action to be taken to practice a lifestyle that involves winning friends and influencing people. Fundamental Techniques in Handling People: 1. Don’t criticize, condemn, or complain: Criticism is futile and it makes the opposite person on the defensive and make them strive to justify themselves. Any fool can criticize, condemn and complain- and most fools do. But it takes character and self-control to be understanding and forgiving. 2. Give honest and sincere appreciation: There is only one way to get anyone to do anything - by making them want to do it. There is no other way. Dr. John Dewey said that the deepest urge in human nature is “the desire to be important”. This desire to be important makes poverty-stricken grocery clerk to study law books (Lincoln), or for Dickens to be inspired to write his immortal novels. Charles Schwab was the first person in America to be paid a million dollars a year - because of his ability to deal with people. Schwab considered his greatest asset to be his ability to arouse enthusiasm among his people. The way he did it was with appreciation and encouragement. All humans need a nourishing dose of self esteem. And providing others this nourishment of and nurturing in the form of kind words of appreciation is greatly accepted and appreciated. Honest appreciation is not to be confused with flattery. Flattery is shallow, selfish and insincere. True appreciation is honest, and is sincere, without expecting anything in return but a smile from the recipient and to make someones day better. People crave the feeling of importance. Make someone feel important and they will think well of you. Diminish someone’s importance and they will resent you. 3. Arouse in the other person an eager want Appeal to the other person’s interests. Virtually all people care more about what they want than what you want. You wouldn’t go fishing with cheesecake as a lure, since fish don’t like cheesecake. Go fishing with worms. Keep asking yourself - “what is it that this person wants?”Everyone has something they can teach you, and you benefit by figuring out what that is. This belief leads to a genuine interest and appreciation for other people. Angry people are often angry because they feel unheard. Once you sympathize with them, they will soften their anger substantially. If there is one secret to success, it lies in the ability to get the other persons point of view and see things from that person’s angle as well as your own. The world is so full of people who are grabbing and self-seeking, that an unselfish individual who is willing to help others just to be helpful has a huge advantage and no competition. Six Ways to Make People Like You: 1. Become genuinely interested in other people It is interesting to see that the dog is the only animal that does not “work” - unlike hen (lays eggs), cow (gives milk), canary (sings). Dogs make a living nothing but love. You can make more friends in 2 months by being interested in other people that you can in 2 years by trying to get other people interested in you. Everyone loves to be listened to and be appreciated. When we start with the recipient and their needs and wants, there is a much bigger and better chance for them to be amiable and warm and kind to you. 2. Smile Smiling makes a good first impression. If you don’t feel like smiling, force yourself to smile (the action of smiling brings out the feeling of happiness and makes one want to smile more. ) Everyone in the world is looking for happiness, and the sure way to find it is by looking inwards and finding that in your heart. 3. A person loves their name. A person’s name is the most important word in any language to them. Use it often and respect it. Remember a name and call it easily, and you have paid an effective compliment to the recipient (that their name is worth remembering), forget it or misspell it and you have placed yourself at a sharp disadvantage. Andrew Carnegie remembered and honored many of his workers name, because of which there was never a strike or an issue under his leadership. All the employees felt loved and heard. 4. Be a good listener. Encourage others to talk about themselves When people are heard, it helps dissolve anger and helps them feel heard. Many a times just listening to a disgruntled customer helps deescalate the situation. Being a good listener is also a good way of becoming a good conversationalist. The more you listen to someone, the more they feel heard and important, and the more they enjoy your company. 5. Talk in terms of the other person’s interests If your recipient is interested in books, starting a conversation with that interest is a great ice-breaker, and establishes a wonderful common ground to setup a communication pattern/style. 6. Make the other person feel important - and do it sincerely Doing this will make people like you instantaneously. Everyone has admirable qualities, and finding those and highlighting them - without having an agenda to sell to them - but only focusing on making them happy - will bring you a long way in building a personal connection with them. This approach could be used with someone you have just met, or others who you have known for years. Emerson said - “Every man that I meet is mu superior in some way. In that, I learn of him. ” Finding that superior quality and highlighting it with honestly is a wonderful quality that makes people like you instantaneously. How to Win People to Your Way of Thinking: 1. The only way to get the best of an argument is to avoid it. If you argue and rankle and contradict, you may achieve a victory sometimes, but it will be an empty victory because you will never get your opponent’s good will. Buddha said - Hatred is never ended by hatred but by love. And a misunderstanding is never ended by argument but by tact, diplomacy, conciliation and a sympathetic desire to see the other person’s point of view. 2. Show respect to the other person’s opinions. Never say, “You’re wrong” Instead, approach with an open-minded view: “I may be wrong. I often am. And if I’m wrong, I want to change and be right. Let’s discuss the facts. ”Praise the other person for a trait that will help resolve the argument - like their patience, open-mindedness, fairness, and receptivity to new facts. Understand that the other person has a valid view of the situation. If you were born as them with their brain and undergoing their experiences, you would by definition feel the same way they do. Your job is to understand what led them to believe what they believe. Express sympathy for their situation. “You have the absolute right to be upset. If I were in your shoes, I would be too. ” To influence people to do things, praise and appreciation are more effective than orders. Don’t start by criticizing or complaining. This makes them defensive and rationalize their actions. Instead, praising them lowers their defenses, and they’ll be more receptive to your feedback. It is important to be diplomatic and look at all points of view. 3. If you are wrong, admit it quickly and emphatically Humility and acceptance of being wrong is a huge virtue. It readily disarms the recipient and moves the conversation from “who is right”, to “how do we fix this”? This strategy readily brings people together, and often brings the opposite person to your rescue by dismissing your mistakes as - “not that big”, and “easy to fix”. 4. Begin in a friendly way Starting any conversation with anger or antagonism will only make the recipient feel a similar level of anger. Beginning a tough conversation in a friendly way prompts the recipient to match your disposition, making it much more conducive to reach an agreement quickly. 5. Get the other person saying “yes” immediately When talking to people, begin with topics and things that you know you agree on. Keep emphasizing that you both have the same goals, and the same destination, and just a slight difference in method to get to the goal. Getting listeners to a “yes” quickly gets their psychological process moving in the affirmative direction. It takes a lot of energy and effort to convert a “no” into a “yes” (even if it is about different topics). But starting with “yes” and agreeing on topics makes it easier to agree on other topics. 6. Let the other person do a great deal of the talking Listen first. Give your opponents a chance to talk through. Do NOT interrupt as they’re speaking. Ask people where they feel the problems are. Ask for their opinions on how best to proceed. Ask lots of questions instead of stating commands. When people are talking, they can often talk themselves out of the negatives, and focus on the positives. Letting others talk always leads to a more favorable outcome as compared to forcing them to listen to you and follow your direction. 7. Let the other person feel that the idea is his or hers A manager of an automobile company asked his sales people what qualities they expected out of him, and wrote them out. He then asked them what qualities he ought to expect out of them - to which they readily listed qualities they should have. The manager made the sales people come up with their own goals for qualities they should possess - which made them want to achieve it because it was their own idea. Similarly, an artist used to create sketches for buyers and was going through a rough spell with acceptance of his work. So he took some half finished sketches to the buyer and asked the buyer how they expected the sketches to be completed. With the buyers feedback the artist was able to make all the sketches to the buyers liking and get back into the grove of selling all of his sketches. To get results, one needs to lead the recipient into thinking in your way, and then get them to reach your thought process/idea on their own accord. It is very effective to execute an idea when the stakeholders believe that they have come to the ideas on their own accord. 8. Try honestly to see things from the other person’s perspective A woman was 6 weeks behind in her car payments, and received a call from the man who was handling her account. Accepting the worst, she thought of the situation from his perspective rather than her own, and admitted to being late on her payment - and possible a very challenging customer for the man - and thereby apologizing. Her empathy to his situation softened him a little and he worked with her to come up with a better way to deal with the car payments. Looking at things from another persons perspective sheds new light on the situation - which is always a good position to be in when problem solving. 9. Be sympathetic with the other person’s ideas and desires There is a magic phrase to stop arguments, eliminate ill feeling, create good will and make the other person listen attentively: “I don’t blame you one bit for feeling the way you do. If I were you I would probably be feeling the same way. ” Using a phrase like this can soften the most antagonistic, defensive or cantankerous people. Three-fourths of the people that one meets are hungry for sympathy, and if you give it to them - they will love you. Responding to anger or insult with kindness is another way of accepting the opposite persons viewpoint and agreeing with it - a pretty effective way to shut down a bully. 10. Appeal to the nobler motives J. Pierpont Morgan observed that a person usually has 2 reasons for doing a thing: One that sounds good, and a real one. In order to change people - appeal to the nobler motive. E. g. is someone is trying to break a lease (against the lease agreement), appeal to their nobler virtue of being a person of their word, about morals and ethics, as opposed to creating a scene and being angry about the possibility of legal action. When you don’t know anything about someone, assume that they are sincere, honest and truthful and proceed conversations from that perspective. 11. Dramatize your ideas This is the day of dramatization. Merely stating a truth isn’t enough. The truth has to be made vivid, interesting, dramatic. You have to use showmanship. The movies do it, TV does it, and you have to do it if you want attention. “You are losing pennies to the dollar” he said as he dramatically dropped several pennies to the grown. The dramatization had a much bigger impact as compared to simply mentioning the statement. 12. Throw down a challenge Charles Schwab said - “the way to get things done, is to simulate competition. Not in a sordid money-getting way, but in the desire to excel. “ Simulating the desire to excel enables create healthy competition to be better - than ourselves, than our competition. Frederic Herzberg (behavioral scientist) discovered through his work that the most important motivator for people was the actual work - not benefits, not working conditions, and not fringe benefits - but the actual work. Be a Leader: How to Change People Without Giving Offense or Arousing Resentment: A leaders job often includes changing your people’s attitudes and behavior. Some suggestions to accomplish this: 1. Begin with praise and honest appreciation It is always listen to unpleasant things after we have heard some praise about our good points. So if any feedback needs to be given, start with good points of the recipient, and how great they are, and then follow up with criticisms. People are more likely to make improvements when information is presented to them in a kind manner. 2. Call attention to people’s mistakes indirectly This is an effective way to criticize, and yet not be hated for it. Charles Schwab was passing through one of his steel mills when he noticed some of his employees smoking right below the “no smoking” sign. Instead of calling them out on this, he used an alternate technique. He gave each one of his employees a cigar and then said - “I’ll appreciate it, boys, if you smoke this on the outside. ” He was able to get his message through and simultaneously made his employees feel important. It is hard not to love a person like that. Using the word “but” after giving praise totally negates the praises, and makes them feel like empty statements. E. g. “We are proud of you for your grades this semester, but you could have gotten better grades if you worked harder in Algebra” vs “We are proud of you for your grades this semester, and you can get even better grades if you continue working this way. ” The second approach is way more effective than the first one. 3. Talk about your own mistakes before criticizing the other person It isn’t nearly as difficult to listen to your own faults, when the person telling you your faults is admitting that he too is far from impeccable. E. g. Dale Carnegie needed to criticize his secretary for something, but before he said anything, he took a step back to consider the situation from her perspective and point of you and age, and realized that when he was that age, he made many more mistakes than her. He started off his criticisms by mentioning that fact, and then gently suggesting the request he had of her - it was readily accepted. 4. Ask questions instead of giving direct orders Asking questions makes the order more palatable, and often simulates the creativity of the persons whom you ask. People are more likely to accept an order if they have had a part in the decision that caused the order to be issued. E. g. “what do you think of this?”, “Do you think this would work?” It might be more time consuming in the short term, but much more effective in the long term. 5. Let the other person save face We bulldoze over the feelings of others, getting our own way, finding faults, issuing threats, criticizing a child or an employee in front of others, without even considering the hurt to the other persons pride. A Fresh aviation pioneer once said = “I have no right to do or say anything that diminishes a man in his own eyes. What matters is not what I think of him, but what he thinks of himself. Hurting a man in his dignity is a crime. “ 6. Praise the slightest improvement and praise every improvement. Be “hearty in your approbation and lavish in your praise” When you praise someone, instead of finding faults, people start capitalizing on their praise and improving what they are already good at. People appreciate it when they are called out on their positive qualities. 7. Give the other person a fine reputation to live up to If you want to improve a person in a certain respect, act as though that particular trait were already one of this or her outstanding characteristics. E. g. You are a phenomenal mechanic, and you have done great work in the past few years. Off late things are not the same way, and maybe we can come up with a solution for it? Making the recipient realize their own qualities helps them build their own self esteem and set up a target for them to reach - all by their own initiative. 8. Use encouragement. Make the fault seem easy to correct It is way easier to climb a molehill instead of a mountain. If the fault is describe as the magnitude of a molehill, instead of a mountain, a person is more likely to readily work on it to fix it. 9. Make the other person happy about doing the things you suggest E. g. if you cannot do a speaking engagement, suggest alternatives who are more likely to accept. It gives the recipient the chance to focus on next steps rather than being disappointed about your refusal. Think about what the other person wants, and try to help the best you can to mitigate their own problems and satisfy their wants - while you make your suggestions. "
    }, {
    "id": 22,
    "url": "http://localhost:4000/blog/2020/start-with-why/",
    "title": "Start with Why",
    "body": "2020/01/02 - This book is by Simon Sinek. In this book Sinek talks about finding the elusive - “why” - which is the primordial reason for developing brand loyalty, making friends, selling, and all other aspects of our life. Great leaders are able to inspire other people to act and make a difference. Leaders give their following a sense of purpose and belonging - making people do the work for themselves - without an external benefit to be gained. Apple, Disney, Harley Davidson, and Southwest airlines are prime examples of companies whose leadership inspires the employees, customers and even the stock holders to align with their WHY. A world that doesn’t start with why: It is important to start with the basics - like a Japanese car manufacturer that has focused on each part being perfect, unlike an American car manufacturer that employs a person at the end of an assembly line who uses a mallet to make the assembly perfect. Working inside out - bottoms up - helps with coming with a a clean message, a focused outlook and a more refined product. If the WHY (or purpose) isn’t very well defined, people resort to alternatives of manipulation to get customers to take action. Some of these are:  Price: Reducing price to attract customers Promotions: Running promotions - such as rebates to gain an edge over competition Fear: Fear of the alternative if item is not purchased. (fear motivates to move away from something horrible) Aspiration: Showing a shiny object to move towards (Aspiration motivates to move towards something desirable) Peer Pressure: Friends, family, competition or celebrity using a product Novelty: New and interesting features in a given productManipulation techniques do not inspire repeat business or loyalty. It is a short term transaction oriented approach to sales. Having loyal employee and customer base not only reduces costs but also provides massive peace of mind. Manipulations work, but they are not sustainable and do not drive loyalty. There is an alternative. An alternative perspective: As an alternative to manipulation Sinek suggests inspiring rather than manipulating. In order to inspire one needs to know their purpose. Golden Circle Sinek introduces the concept of Golden Circle WHAT: Every company knows what they do and can describe the service they provideHOW: Some companies know HOW they do WHAT they provide. This is often known as the differentiatorWHY: Very few companies know WHY they do WHAT they are doing. This defines the purpose, cause or belief that people can associate with. This is the belief that resonates with customers, employees and even investors. Sinek illustrates how starting marketing or advertising with the WHY helps the consumer connect with the brand and provides the reason to buy, and the actual product is just proof of the WHY. People do not buy WHAT you do, they buy WHY you do it. More qualified and products often fail, when up against people and products that have a much better defined WHY. E. g. Wright Brothers were the first people to build an airplane, even though Samuel Langley was much more qualified and funded to build an airplane. Other examples include Creative Technology Ltd. which was much more better positioned to sell a music player as compared to Apple - which was largely a computer brand at that point. Companies that have a clear sense of WHY never worry about competition. They don’t think of themselves as being like everyone else and they don’t have to “convince” anyone of their value. Having good quality products (or WHAT you sell) is ofcourse very important, but when the WHY is clearly defined, the WHAT sells much more easily. If a customer feels inspired to buy a product rather than manipulated, they will be able to verbalize the reasons why they think what they bought is better and inspires loyalty in the product and brand. Knowing your WHY is not the only way to be successful, but it is the only way to maintain a lasting success. It is all biologicalIt is biological for humans to feel belonged. Our desire to feel like we belong is so powerful that we will go to great length and do irrational things. Such gut decisions don’t happen in your stomach. The limbic brain is responsible for making these gut decisions and feelings of trust and loyalty. However the limbic brain has no capacity for language - which is why it cannot quite describe the actual reasons for making the decision. Figuring out the WHY is hard work. e. g. laundry detergent manufactures took a long time to discover that people resonated with the WHY of smelling fresh laundry rather than the “whiteness” of their clean laundry. Clarity, discipline and consistencyTo understand the WHY - clarity is most important. In order to get to HOW to achieve your WHY one needs discipline. Make HOW actionable - Nouns are not actionable, verb are actionable. E. g. instead of “integrity” say “always do the right thing”. Consistency between WHAT and WHY - When the why is well defined, and the message is authentic, there is clear consistency between the product and the purpose of the product. Leaders need a following: Leads can inspire a following if there is trust. With trust comes value - real value, not just value equated with money. To inspire trust, we need to find employees who connect with your WHY, so that they cab believe in what you believe. When you fill an organization with good fits, people who believe what you believe, success just happens - everyone is moving towards the same goal. Everyone in the planet is passionate about something, but it is not the same things that we are all passionate about - so it is important to find the right people who are passionate about the same things are you are. Great companies don’t hire skilled people are motivate them, they hire already motivated people and inspire them. Average companies give their people something to work on. In contrast, the most innovative organizations give their people something to work towards - that way people are achieving their own personal WHY while in alignment with the company’s WHY. Trust comes from knowing how good someone is at what they do. It also comes from influence of others and their recommendations. Law of Diffusion According to law of diffusion, the entire population is made up of 5 segments:  2. 5% are innovators 13. 5% are early adopters 34% are early majority 34% are late majority 16% are laggardsWhen you have a new product, it is always good strategy to first market to the early adopters, rather than the majority. The early adopters offer loyalty and a following, which can be used to target the early and late majority of the market. Tivo is a company that was perfectly positioned, but made the grave mistake of targeting the majority, before building their loyal customer base and eventually did not succeed. According to the Law of Diffusion, mass-market success can only be achieved after you penetrate between 15% to 18% of the market. Building a loyal customer base means the the people are willing to suffer some inconvenience or pay a premium in to do business with you. They perceive value in what you do, and have trust and belief in what you do. They are the ones who, on their own volition, will tell others about you. These are the people who share your beliefs and what to incorporate your ideas. How to rally those who believe: Energy of leaders excites, but charisma of leaders inspires. The goal is to amplify the source of inspiration. Figure out a way to amplify the WHY of the CEO to the HOW that is implemented by the next level of people. Those who know WHY need those who know HOW. Just having an idea and purpose doesn’t help, one needs to know how to bring it to reality. HOW-types doesn’t need WHY-types to do well. But WHY-types for all their vision and imagination, often get the short end of the stick. Without HOW-type people to complement them WHY-types might end up as starving visionaries. Being authentic is the core of communicating the WHY. Say it only if you believe it. The ability of some companies not to just succeed but to repeat their success is due to the loyal following they command, the throngs of people who root for their success. Greatfulness is repeatable - for instance look at Ron Bruder. When a company is small, it revolves around the personality of the founder. There is no debate that the founder’s personality is the personality of the company. As the company grows, the CEO’s job is to personify the WHY. To ooze it, to talk it, to preach it. E. g. There is no difference between Sir Richard Branson’s personality and that of Virgin’s personality. Similarly there is no difference between Steve Jobs’ personality and Apple’s personality. The struggle that so many companies have to differentiate and communicate their true value to the outside world is not a business problem, it is a biology problem. Like people struggle with putting words to emotions, companies also struggle with that aspect of communication. We rely on metaphors, imagery and analogies to communicate how we feel. The goal for any business - Communicate clearly and you will be understood. Celery Test Celery test is a simple test to evaluate exactly the WHAT and HOW is aligned to your WHY. E. g. people might suggest you to buy Oreo cookies, cake, coconut milk and celery to help your business. However, if your WHY is to eat healthy - Oreo cookies and cake will not pass the celery test. Using the celery test to evaluate your choices will help in making the right choices for the business. The more celery you use, the more trust you can build. The biggest challenge is success: The single greatest challenge any organization will face is success. All companies start out small, and as they grow the founders use their gut to make the decisions for their company. As the company grows, it becomes virtually impossible for the single person to make all the right decisions for the company. School Bus TestIf the founder or a leader were to be hit by a school bus, would the organization continue to thrive at the same pace without them? To pass the school bus test the founders WHY must be extracted and integrated into the culture of the company. A strong succession plan should aim to find a leader inspired by the founding cause and ready to lead it into the next generation. Figuring out what to measure - the right KPI’s - is what defines goals and what gets done. E. g. Christina Halbridge changed the business of debt collection from being aggressive to building a connection with the recipient. She measured the effectiveness based on the number of thank you cards her employees sent. They were incredibly successful not only because of the WHY, but also because she found the right way to measure the WHY. Discover why: Discovering WHY is hard work. One can discover WHY by looking back and analyzing the reason behind why some projects wok and others don’t. If you follow your WHY, others will follow you. When you compete against others, no one wants to help you. But when you compete against yourself, everyone wants to help you. Leadership requires two things: a vision of the world that does not yet exist, and the ability to communicate it. Leaders inspire action. "
    }, {
    "id": 23,
    "url": "http://localhost:4000/blog/2019/how-to-fail-at-everything-still-win-big/",
    "title": "How to Fail at Almost Everything and Still Win Big",
    "body": "2019/12/31 - This book is by Scott Adams - the Author of Dilbert comics. Adams does a phenomenal job of break down the psychology behind failing and winning, and simple techniques to implement them in your own life. Overview of the concepts of the book:  Goals are for losers - if you are setting goals for yourselves you are losing Your mind is a moist computer that you can program.  The most important thing that you can track is your personal energy Every skill that you acquire doubles your odds of success Happiness is good health and freedom of time Luck is managed and manufactured - you can create it Conquer shyness by faking it Fitness is a lever that can move the world Simplicity transforms ordinary into amazing!Goals vs. Systems: When you have a system you constantly follow that system without worrying about reaching a “destination”. Each day you follow your system is a successful day. On the other hand, if you have a goal, each day that you are not at your goal you feel that yearning to be at your goal. Each day is another day that you have not achieved that goal. Each day is a failure to reach the goal. For example, in the world of dieting, eating right is a system, but losing 10 pounds is a goal. If you do something everyday, it is a system. If you’re waiting to achieve it some day in the future, it is a goal. Most people who are goal oriented stay in a permanent cycle of pre-success failure. Most people who succeed follow systems not goals. Another disadvantage of a goal is that once you achieve it, you set a next goal and continue to feel like you have failed till you achieve the goal - thereby living in a continued cycle of presuccess failure. Deciding vs. Wanting: Everything has a price. If you want success, figure out the price and then pay it. Once you decide on what you want, you are acknowledging the price associated with it and are willing to pay it. Successful people dont wish for success. They decide to pursue it. Be Selfish: It is important to be selfish and help yourself first. Only when you are fulfilled can you do something great for others. If you are not selfish - your other choices are to be stupid, or a burden to society - neither of which are quite helpful to society. Important things to be selfish about:  personal fitness eating right pursuing your career spending time with family and friendsIt is stupid to neglect your health or your career - both of which guarantee long term happiness. It is especially important to be a little selfish when times are hard, so that when they are better one can give back to society in terms of jobs to others etc. Maximize personal energy:  Have something in like that makes you excited to wake up in the morning. It is important to have good health, sleep, avoiding stress etc. But one of the major things that can help with maximizing energy is to have something in life that makes one excited to wake up in the morning. When a sad person enters a room, the energy of the entire room drops, but when a happy person enters a room one can feel the energy of the entire room rising. Try to be the latter.  Manage mental state to task: Do the task that is suited for a given mental state. E. g. when relaxed do the task suited for it (like creative tasks).  Simplifiers vs Optimizers: Simplifiers prefer the easy way of doing a task. An optimizer looks for the best solution. Simplification of tasks frees up energy. Optimizing is goal oriented, and often leads to spending more energy to maximize achieving of goals. Be a simplifier unless necessary.  Sitting position: The mind takes cues from the body based on how we sit stand etc. E. g. if you sit in a relaxed position - like slouching, the brain with start the relaxation subroutine. If you sit up straight and ready for work, the brain will start the engaging subroutine.  Tidiness: Every time you see a messy room and think about cleaning it, it is a distraction from your more important thoughts.  Knowledge (or no knowledge): Everyone has the fear of not knowing things that are required by their career. But - our problem isn’t unique. There is always someone who has a similar problem and other who have provided solutions for it. Adams recommends “flash research” - which means spending just 1 minute on search engine to learn some basic ideas/solutions to the problem.  Be a nice person: Don’t be a jerk! Priorities: Set clear priorities in your life. Adams’ priorities are - him, job, family, city, country …. Managing your Attitude: It is an art (or science?) to control your attitude directly, as opposed to letting the environment dictate how you feel. In a way it is like a superpower!  You can control your attitude by manipulating your thoughts, your body, and your environment.  Exercise, food and sleep - these are the minimum requirements to have a good attitude Daydreaming positive things - make you feel positive and happy. Dream without worrying about whether those dreams will come true.  Work on projects that have chance to change the world Work on projects that make you excited to get out of bed in the morning Smile! Smiling make you feel better, even if your smile is fake Success in easy for you areas - if you are successful in one area, it spills over into other areas of your life.  Pick a delusion - This idea is similar to wearing “lucky socks” for a test/game or doing something irrational just because you feel that it will be helpful to you somehow. If you believe that you are writing something that millions will read, it motivates you to write. Pick your delusional belief if it help you build a better attitude towards your work.  Fake it till it becomes real. When it becomes real, one can quickly adopt the mannerisms and skills associated with the new status and position. Recognizing your Talents and Knowing When to Quit: Malcolm Gladwell wrote that it takes 1000 hours of practice to become an expert in just about anything. Things that someday work out well, start out well. Things that never work start out bad and stay that way. One rarely sees a stillborn failure that transforms into a stellar success. Believe more in what people say, than what people do. They say things to spare your feelings, and be nice. They are far more honest with what they do. If they really like something that you write, or create they will share it, talk about it and email and tweet about it. It is generally true that if people are not excited about what you create in the beginning, they will not be excited about it later on and they never will be. Practice!: The more you practice, the better you will get. The hard part is figuring out what to practice and what skill to acquire. If you code more - you become a better code. If you manage more - you become a better manager.  Practice involves putting your consciousness in suspended animation. Practice is mundane, and it isn’t living. But when you used the skills that you have acquired via practice, and merge it with life experiences, you feel alive! Managing the odds for success:  Every skill that you acquire doubles the odds for success. Two skills that you are good at is better than one skill that you are excellent at.  The more you know, the more you can know. Once you get into the habit of reading, you want to read more and more, and can learn more and more. The Math of Success: The world is made of math, and not magic. We should pick a game where the odds of winning are better. According to Adams, the odds of success increase significantly if one masters the following:  Public Speaking: It gives tremendous power to talk about ideas and share knowledge.  Psychology: It is important to understand psychology to sell, to convert ideas, to communicate. If there was just one skill to acquire, psychology would be it! A good place to start is by going down and learning about Wikipedia’s list of cognitive bias.  Business Writing: Business writing is all about getting to the point and leave out all the noise. It is also the foundation of humor writing/ Accounting: If you don’t know accounting, you have a giant knowledge gap in life or business. Learn how to manage money, and do your accounting.  Design (the basics): Some basic understanding of how to make something look pleasing is important - be it PowerPoint slides, or website.  Conversation: To be a skilled conversationalist, all you have to do is introduce yourself and ask questions until you find a point of mutual interest. It is laughably simple! The point of the conversation is to make the other person feel good.  Overcoming Shyness: Shyness is analogous to swimming - we are not born with it, but we can learn it and practice overcoming shyness, as we can swim.  Second Language: Often qualify you for a large range of jobs as compared to peers Golf: Business gets done on a golf course. Women would especially benefit from learning golf.  Proper Grammar: Talking in bad grammar is a put off, and people can easily dismiss you and your skills if you do not present them with proper grammar.  Persuasion: No matter what you do in life, you will spend a lot of time persuading people to do things. e. g. closing a sale, getting a kid to eat their greens, managing employees etc.  Technology: Good to know basics so that you can learn whatever you need, and can talk intelligently about it.  Proper voice technique: It is helpful to have different vocal strategies for different scenarios. E. g. high pitched fun voice should be different from low pitched persuasive voice for the respective scenarios. Breathing techniques and posture also help with the right voice techniques. Besides the above, a couple other skills to develop are:  Lack of fear or embarrassment Get the right kind of education - no rick of unemployment ExerciseAffirmations: Affirmations are simply the practice of repeating to yourself what you want to achieve while imagining the outcome that you want. Adams mentions that working with affirmations has always yielded a better outcome for him then not working with affirmations. It is a very simple and positive practice, so why not do it? Timing is luck: If the timing is not right, no amount of hard work will make things move forward. But you can allow luck to follow you by getting the timing right. Association Programming: Associating with successful people makes you think in a different way, and be successful by association. Studies show that if you hand out with overweight people, you could gain weight. Similarly living around optimistic winners can make you one too. Qualities of the people you hang out with can run off on you. Happiness: The only reasonable goal in life is to maximize your total lifetime experience of happiness. It is important to understand what happiness is, and how it works. The simplest way to enable more happiness is by managing your body chemistry with exercise, diet, sleep and daydream happy things for yourself. These strongly influence mood. Another things that helps with happiness is to have the option of a flexible schedule by being in control of your time. Finally have a routine helps with removing the paradox of choice and simplifying life. Thereby routines help happiness. Diet: It is possible to change your food preferences by thinking of your body as a programmable robot, rather than a fleshy bag of magic. Experiment with different types of food and see how they make you feel? Do carbs put you to sleep - even if you had them at breakfast? So fresh greens and fruit make you more awake and make you want to workout later? Discover correlations between foods and your body, and pick the types of food that enable you to live the life that you want to live - that helps you achieve the goals that you want to achieve. A simple way to disengage from sugar and simple carbs is to allow yourself to each as much as you want of anything healthy - try lots of other types of food to your hearts content - cheese, peanuts, fruits. Eventually the appeal of simple carbs goes away. Make healthy food taste good. Learn new recipes and make food such that even the foods that you didnt like before become enjoyable. Avoid foods that feel like punishment - that isnt sustainable! Hanging out with fit people make you wnat to be fit too. Adams also recommends having 2-4 cups of coffee per day - a great way to manage your energy levels. Fitness:  Be active every day If you can be active every day - without having to think about it and just doing it daily - like brushing your teeth - you wont have to use your limited supply of willpower for it. Once exercise becomes habitual, you wont need willpower to sustain it. Find an easy way to incorporate exercise into your routine such that it works with your mind and body. Schedule your exercise at the same time every day, to get into a routine and a “no-brainer”. Don’t take any rest days. Just work out at your pace each day, and every day. When your body gets stronger, you will automatically start taking on harder exercises. Reward yourself after exercising - small snack, or anything else that you feel is reasonable. "
    }, {
    "id": 24,
    "url": "http://localhost:4000/blog/2019/so-good-they-cant-ignore-you/",
    "title": "So Good They Can't Ignore You",
    "body": "2019/07/27 - Author’s profile and book description: So Good that they can’t ignore you, by Cal Newport There was once a person who wanted to become a monk. He believed that once he becomes a monk and discovers his truth, he will feel peace and bliss. He discovers it - the feeling of being one with the world and the universe, but he still feels empty. He knows what he wanted to know, but he still is the same person, and it doesn’t change who he is or what he does. Fulfilling his dream to become a full time Zen practitioner did not magically make his life wonderful.  In reality, career passions are rare. Most passions are related to art, sports and hobbies. It is wise to stay away from passions as they can be misleading.  Passion takes time to develop. The more time you spend on something the better you will get at it and more passionate you will feel about it. ABout 10,000 hours is what is takes to master something.  Passion is a side effect of mastery. There are 2 types of mindset that one can have:  Craftsman mindset: Focus on what you are producing for your work and your career Passion mindset - Focus on what your job or work offers youTo do anything successful you need to be so good that they cannot ignore you. This is valid for anything in life including  getting a book published getting a music gig getting a new job building a scaling a companyIf you are good at the core of it - where you are doing what you do really well - it is hard for other people to ignore you.  If you have the craftsman mindset - you are constantly focusing on what you are producing, how good is the quality of it, how good you are, and if you can become better at what you do. Craftsman mindset is crucial for building a career that you love. Career Capital - this is the process of building up rare and valuable skills that you can build as your career progresses. Some traits that define great work:  creativity impact controlIt is the quest of most people to do great work by striving for one of more of these qualities from their work. How to have a craftsman mindset?:  To become amazing at what you do, you need to focus on stretching your ability by receiving feedback from others. This will push you to get better and become more open to criticism.  Deep Study - it takes 10,000 hours of work to become great at something. And it takes a lot of time to study something at its core. If you just show up and work hard, you will soon his a performance plateau beyond which you will fail to get any better. To successfully adopt the craftsman mindset we have to approach our work with a dedication to deliberate practice - like Gary Kasparov does with chess.  To adopt a craftsman mindset you need to constantly solicit feedback from colleagues and professionals. You need to start enjoying criticism. In order to master something and do deep work, you need to be mindful of where you are spending your time through the day. Create a spreadsheet recording the time you spend doing deep work and high priority work. Work towards getting your spreadsheet looking like how you want it to look. 5 habits of a craftsman:  Knowing what capital market you are in: There are 2 types of markets - winner-takes-all market where there is a single monopoly, and auction market - where there are several players in the market - each owning a part of the pie. e. g. blogging is a a winer takes all market, clean teach is an auction market. Knowing which market you are in helps you develop a strategy on how to work towards a solution, Identify your capital type: Which skills are important to acquire in the market that you are in? Seek “open gates” which are opportunities to build you skills or market capital.  Define good - what is good enough for you? How good do you need to be? Stretch and destroy - doing things we know how to do well is enjoyable, and that is exactly the opposite of what deliberate practice demands. Deliberate practice is above all an effort of focus and concentration. That is what makes it “deliberate” as distinct from the mindless playing of scales or hitting of tennis balls that most people engage in.  Deliberate practice is often the opposite of enjoyable. If you are not uncomfortable then you are probably stuck at an acceptable level  Be patient - acquiring capital takes time. Control trap - It is dangerous to pursue control in your working life before you have career capital to offer in exchange. Control over your work gives you happiness and fulfillment, but it is important to acquire career capital (patiently) before you use that to increase your control over your work. The law of financial viability - “Do what people are willing to pay for” When deciding whether to follow an appealing pursuit that will introduce more control in your work like, seek evidence of whether people are willing to pay for it. If you find this evidence - continue - if not - move on. Mission - A unifying mission to your working life can be a source of great satisfaction Missions require little bets, these can be transformed into successes by using small and achievable projects. Missions require marketing. Missions can be transformed into great successes as a result of projects that satisfy the law of “remarkability”. This means an idea inspires people to remark about it and it launches at a venture where such remarking is made easy. Pay attention to where you launch an idea. E. g. if you launch a social media idea at a farm it isn’t remarkable, but if you launch an agrotech idea at a farm it might be remarkable. Working right trumps finding the right work. "
    }, {
    "id": 25,
    "url": "http://localhost:4000/blog/2019/manageml/",
    "title": "Managing Machine Learning Experiments",
    "body": "2019/07/24 - I run Machine Learning experiments for a living and I run an average of 50 experiments per stage of a project. For each experiment I write code for training models, identifying the right test cases and metrics, finding the right preprocessors - the list goes on. So how to manage these experiments? Here are a few criteria that I have:  Compatible with Git: I manage all my code with Git and I want to make sure that experiment manager can keep track of how my code changes with time.  Version control for data: I want to be able to work with multiple versions of my test and training datasets. I need to know if any 2 datasets are duplicates of each other, so that I am running my tests on the same datasets.  Model Management: When I run experiments and store models, I want to store models that are associated with an experiment, rather than a particular run. I need to have meta-data associated with the model that tells me information about how this model was created, data it was trained on etc. (This is also the experiment meta-data) Metrics: I want to be able to store the output metrics for each experiment, and create new metrics by running the same model over different test datasets.  (Optional) Running experiments: I want to be able to run experiments with a single command - this can be bath experiments, or a single experiment. I don’t want to have to worry about containers and dockers and the logistics of it.  (Optional) Experiment optimization: I have so many variations and tests to try out. If a system to automatically try out these different variations for me that go beyond optimizing hyper-parameters of an algorithm, I would love to try such a system out. I went on a quest to find a solution to my problems. And while I was on my quest, I discovered some more criteria that I had previously not considered while evaluating tools. Here are some products that I have been looking at:    Product  Pricing      Comet  Paid      neptune. ml  Paid      Tensordash   Paid      Weights and Biases   Free for individuals and academics, Paid      Valohai  Paid      FloydHub  Paid      Verta. ai  Not Launched Yet    SirioML  Not Launched Yet Below is an impressive list of opensource tools in this space for running, managing and analyzing experiments.    MLFlow  Free  Yes    DVC  Free  iterative. ai    Guild. ml  Free  Yes    MLModelScope   Free       Machine Learning Lab  Free  Yes      ModelChimp  Free      Trains  Free  Yes    ModelDB  Free  Yes    Omniboard  Free  Yes    Datmo  Free  Yes    Randopt  Free  Yes    StudioML  Free  Yes    KubeFlow  Free  Yes    Lore  Free  Yes     Featureforge  Free  Yes    pachyderm  Free  Yes    PolyAxon  Free  Yes    Runway  Free  paper: http://www. jsntsay. com/publications/tsay-sysml2018. pdf    Sacred  Free  Yes    Sumatra  Free  Yes "
    }, {
    "id": 26,
    "url": "http://localhost:4000/blog/2019/a-universe-from-nothing/",
    "title": "A Universe from Nothing",
    "body": "2019/06/28 - Lawrence Krauss answers questions about the universe - where it came from, what was there before, what will the future bring, and if it all really came from nothing. New York Times article about the book: A Universe from Nothing: Why There Is Something Rather than Nothing - by Lawrence M. Krauss NPR interview with Lawrence Krauss: A Universe from Nothering The Beginning: When Einstein completed his new theory of gravity (which is known as the general theory of relativity) and tried to apply it to describe the universe, it became clear that this theory did not describe the universe in which he lived. At that time, the universe as considered static and eternal - consisting of a single galaxy. It was not until he applied the theory of relativity on the orbit of Mercury - and it explained the slight deviation of Mercury’s orbit, did we discover that the universe is in fact - not static. Henrietta Swan Leavitt discovered that the brightness of stars goes down inversely with the square of the distance to the star. So if we are able to compute the brightness of the star, we can infer the distance to it! This helped discover that the universe isn’t composed of just the Milky Way but billions of other galaxies. “Absorption lines” (dark bands in the light spectrum) are observed when a ray of light is split into individual components using a prism. This means that something in the atmosphere is absorbing a certain spectrum of light. The absorption lines were wavelengths that were absorbed by known materials on Earth - including hydrogen, oxygen, iron, sodium etc. Each new absorption line discovered helped us discover a new element. e. g. Helium. Along similar lines was the term “Doppler effect” - waves coming at you from a moving source will be stretched when the source of moving away, and compressed when moving towards you. This same phenomenon is observed in light waves also. Using this phenomenon it was observed that absorption lines from spiral nebulae were almost all shifted towards longer wavelengths - which led to the conclusion that those objects are moving away from us at considerable velocities.  This led to the conclusion that our universe is expanding. The Big Bang happened - during which nuclear reactions readily take place between protons and neutrons as they bind together and breaks apart. Following this the universe cools, and it can be predicted how frequently these nuclear constituents with bind to form nuclei of atoms. and it explains the presence of light elements. Hydrogen, Helium and Lithium were created during the Big Bang. The other heavier components - Carbon, Nitrogen, Oxygen, Iron were made in the fiery cores of stars and reach us when the star explodes. If everything is moving apart right now at a tremendous rate, it probably was closer together in earlier times. The age of the universe is inferred to be 13 billion years. Weighing the Universe: Vera Rubin observed that the stars and hot gas that are farther away from the center of the galaxy are moving much faster that they should have been if the gravitational force driving their movement was due to the mass of the observed objects within the galaxy. So we have computed the mass of the universe incorrectly basing it only on the matter that we can see. There is more mass that we cannot see - dark matter. Einstein proposed a theory that space would curve in the presence of matter or energy. This lead us to believe that our universe can have three different geometries - open (continue to expand forever at a finite rate), closed (one day will re-collapse - reverse big bang) or flat (slowing down but never quite stopping). Determining the amount of dark matter will help us identify how the universe will end. Einstein demonstrated that space acts like a lens - bending and magnifying light - just like lenses in reading glasses. This meant that if a bright object is located behind an intervening distribution of mass, light rays going out on various directions could bend around the intervening distribution and converge again - magnifying the original object and producing several copies of it - called lensing. Fritz Zwicky took this theory and said it could be used for :  Testing general relativity Use galaxies as a telescope to magnify more distant objects Resolving the mystery of why clusters appear to weigh more than accounted for in visible matterAll experiment point to the universe being flat. Dark matter must be made of something entirely new - something that we have not found on earth yet. Curvature of Universe: Weighing the universe leads to inconsistencies in measurement, because we cannot measure what we cannot see, and we don’t know what we cannot see in the universe. It would be better to measure the geometry of the whole universe. How to observe curvature? The sum of the angles of a 3D surface is greater than 180. Cosmic microwave background radiation - has been under our noses for decades, but was discovered by 2 scientists in New Jersey. As we look out at distant objects, we are looking back in time - because light takes longer for us to get from these objects. In theory, if we keep looking farther enough, we can look all the way to the big bang. However, in practice, there is a wall between us - not a physical wall, but more of an interference. It is theorized that when the universe was 300,000 years old it was made of dense plasma of charged particles. However, plasma can be opaque to radiation. This means that the charged particles within the plasma absorb photons and re-emit them so that radiation cannot easily pass through such a material uninterrupted. As a result, one cannot look back in time to see beyond this time when the universe was comprised of such plasma. This time then - is the last scattering surface. So - if we can take a picture of this last scattering surface, we can actually get a picture of the universe that is merely 300,000 years into its existence. We used a hot air balloon and later a space probe to compute the hot and cold spots of microwave background radiation from the universe. Plotting them shows that the universe is flat. However, based on the computation of the weight of the universe, the universe still seems to way 3x less than what it should to be a flat universe. Inside atoms: There is energy inside atoms. Each matter has its accompanying antimatter. Electron has positrons (-ve has positive counterparts) Dark Energy: There is something in “nothingness” of the universe. The empty space - blankness has energy - dark energyUniverse is accelerating in its expansion. Universe is NOTHING: When a balloon is blown larger and larger, the curvature at its surface get smaller and smaller. Something similar happens for the universe whose size is expanding exponentially. In a flat universe, and only in a flat universe, the total average Newtonian gravitational energy of each object moving with the expansion is precisely zero. In such a universe, the positive energy of motion is exactly canceled by the negative energy of gravitational attraction. Next Chapter: Galaxies do not suddenly disappear. Rather, as their recession speed approaches the speed of light, the light from these objects gets even more red-shifted. Eventually all their visible light moves to infrared, microwave, radio wave, and so on, until the wavelength of light they emit ends up becoming larger than the size of the visible universe, at which point they officially become invisible. This will happen in about 150 billion years. "
    }, {
    "id": 27,
    "url": "http://localhost:4000/blog/2019/rising-strong/",
    "title": "Rising Strong",
    "body": "2019/06/25 - In Rising Strong, Brené Brown talks about how we often fall when we try, and goes into great detail about how we can change the narratives in our mind, and the stories that we tell ourselves to be able to rise up again. Author Page: Rising Strong: How the Ability to Reset Transforms the Way We Live, Love, Parent, and Lead - by Brené Brown The Physics of Vulnerability: Vulnerability is not winning or losing. It is having the courage to show up without knowing or having any control of the outcome. A few rules of engagement for rising strong:  If we are brave enough - often enough - we will fall. This is the physics of vulnerability.  Once we are in the service of being brave - it isn’t possible to go back This journey belongs to you alone - no one can do it for you.  We are wired for story - anything happens we make a story for it explaining it.  Creativity takes knowledge from our heads to our hearts through our hands.  Rising Strong can be applicable in professional or personal aspects of life Comparing your suffering is caused due to fear and scarcity. Pain is pain - it is stupid to compare different peoples pain - just makes you deny your own pain or makes you feel bad for feeling pain when others are going through more.  Vulnerability and courage are unique and custom for each person. It isn’t possible to engineer a one size fits all solution.  Courage is contagious. When you observe courage and vulnerability, it transforms you.  Rising Strong is a spiritual practice. You can’t skip day 2: Day 2 is the middle part of your process - when you are in the dark and have no clue what is happening, where you are going. The door behind you has closed, and there is no way you can turn back. The only way is forward, but the light is still far away. It is the point of no return. Day 2 is the non-negotiable part of the process - when things are raw, and real, emotions are high. This is when the struggle seems to real and too large. Think of it as story telling (book/movie): Act 1: Protagonist is called on an adventure. Act 2: Protagonist looks for every comfortable way to solve the problem - but is not able to. This act includes the “lowest of the low” Act 3: The protagonist needs to prove that they learned a lesson, and are enlightened character with the knowledge and resolve to solve the problem. Our “Day 2” is the same as “Act 2” of a movie! The crappy, gut-wrenching, never-ending feeling of everything going wrong. But it is a prerequisite for step 3. You cannot solve the problem - you cannot find your enlightenment till you FEEL the need - REALLY FEEL THE NEED - to fix it - to solve it. The premise of rising strong - the three acts - are broken down into the following three processes:  The Reckon The Rumble The RevolutionReckon: “Reckoning” - is the process where we assess where we are at and what we are feeling. It isn’t a simple process. Recognizing emotion means being aware of how we are thinking - the storing we are making up - the people we are finding to blame. No real transformation can happen unless we are able to assess, acknowledge and observe our current situation for what it is - without judgment of the situation or ourselves. We don’t like to feel difficult emotions, and we don’t know what to do with discomfort and vulnerability. Our instinct tells us to run from pain. But despite our fears, a part of us wants to engage in emotions - because they are also the juice of life. When we suppress and diminish our emotions, we feel deprived. But then we watch reality shows like - fear factor - seeking emotional intensity - which is only a simulation and does not teach us anything about ourselves. Curiosity is what leads to reckoning - but it is the core of life. Curiosity makes us question and learn. It is an act of vulnerability and courage - vulnerability in not knowing, and courage in being brave enough to want to find out. Rumble: “Reckoning” is when we walk into our story, but “Rumble” is when we OWN it. To rumble is when we are getting honest about the stories that we are telling ourselves and making up about our struggles. Reckoning leads us to a place where we are face down on an arena floor. Rumble is when we are trying to figure out what the heck is happening. Our brains reward us with dopamine when we recognize and complete patterns - stories are those patterns. Even though the stories we make up are incomplete and incorrect - we still run with them for the dopamine reward. E. g. when a husband is rude to his wife, her first thought is - “he is a bad man who tricked me into marrying him for 20 years”, or “I am the problem. I am not enough and I need to do more - which is why he is rude to me. “ There is growing evidence that healthy, ordinary people are strikingly prone to “confabulate” in every day situations. These are stories - lies - that are told honestly. E. g. when shoppers were asked to pick socks from 10 different pairs and answer why - they gave elaborate and unique reasons for picking what they did, even though the socks were all identical. (The reasons are the stories that we make up for ourselves) To capture these “first stories” are learn from them - we need to use our creativity. The most effective way is to write your SFD (shitty first draft) - of your thoughts and emotions. This is equivalent to a child’s draft - no one is going to see it and you can shape it later. It allows this childlike part of you to channel whatever voices and visions some though and onto the page. Delta - is the difference between what we make up about our experiences and the actual truth that we discover through our rumbling. Deltas are also where rivers meet the sea - they are marshy, full of sediment, rich, fertile areas of growth, and forever changing. Is everyone doing the best they can? No one knows - but it feels better when we believe that everyone is doing the best they can in the circumstances that they are in. It helps us let go of our judgment, and blame of others, and accept them for who they are. Do you attach your self worth to being a helper? We are social people and everyone needs help at one time or another. Accepting readily and helping others when they need help is a comforting process. Judging yourself or others for needing help isn’t a productive process and holds us back. Change your narrative from “I am a failure” to “I failed at X”. While the first talks about the “state” the second is an activity. Moving from the first to the second narrative gives you the room to change or fix what went wrong in the incident “X” and move on from it. Revolution: A revolution starts with a new vision of what is possible. Our vision is that we can rise from our experiences of hurt and struggle in a way that allows us to live more wholehearted lives. Our willingness to rumble, learn from it and change ourselves - together - is what leads to a revolution. The 5 Rs: Respect: yourself and othersRumble: on ideas and strategies, misunderstandingsRally: together to own our decisions, learningsRecover: with family friendsReach out: To each other with love and empathy "
    }, {
    "id": 28,
    "url": "http://localhost:4000/blog/2019/braving-the-wilderness/",
    "title": "Braving the Wilderness",
    "body": "2019/06/19 - This book is about our struggle to try to “belong” to something, what the belonging means. This book inspires the reader to belong to themselves, and giving yourselves that place in the universe that you seek acceptance from. Author Page: Braving the Wilderness, A quest for True Belonging and the Courage to Stand Alone - by Brené Brown Everywhere and Nowhere: In this chapter Dr. Brown talks about the famous quote by Dr. Maya Angelou  You are only free when you realize you belong no place - you belong every place - no place at all. The price is high. The reward is great. As humans we find great comfort in belonging to something - a team, a group, a place that gives comfort of community. Brené Brown talks about her childhood - growing up in New Orleans, Texas, Washington DC and the impact that it had in her life to belong somewhere. She talks about how she didn’t get into her school drill team - something she desperately wanted, to enable her to belong somewhere and have an enviable social life. She talks about the desire to belong in our families, and with our spouses. The biggest desire that is often unmet is our need to belong to ourselves - giving ourselves the permission to be us - and following our path and our truth - unadulterated by judgment from ourselves and from others. We belong everywhere - and yet we don’t belong completely at any one place. But the realization that we belong to ourselves is huge - it is a steep uphill realization, but the rewards are incredible. The Quest for True Belonging: We want to belong - truly and completely in a way that is authentic and unconditional. Dr. Brown studied that we (as humans) want to belong to a larger community and experience being part of something - but without giving up our own self and our own authenticity. It is hard for us to chose between being loyal to ourselves and being loyal to a group - which creates some pressure to “conform” and to “fit in”. The definition of trusting can be captured by the acronym BRAVING  B - Boundaries - Respect boundaries, and if unclear ask! R - Reliability - Say what you do and do what you say A - Accountability - Own your mistakes and apologize and make amends V - Vault - Don’t share information that is not yours to share I - Integrity - Chose the right path, instead of the easy path N - Non-judgment - Ask for what you need, and don’t judge others for what they need G - Generosity - Be generous with your intentions, words and actions of othersApplying the concept of BRAVING to yourself - do you trust yourself?  B - do you respect your own boundaries? R - are you reliable with yourself? A - are you accountable with yourself? V - do you respect your vault/your information? I - do you act with integrity? N - do you ask for what you need? G - are you generous towards yourself? True belonging is the spiritual practice of believing in and belonging to yourself so deeply that you can share your most authentic self with the world and find sacredness in both - being a part of something and standing alone in the wilderness. True belonging doesn’t require you to CHANGE who you are; it requires you to BE who you are. High Lonesome: A Spiritual Crisis: “High Lonesome” - that holler that is in between a spirited yippee and a painful wail - something that is thick with misery and redemption. We have geographically, politically and spiritually sorted ourselves into like-minded groups in which we grow more extreme in our thinking, and consume only the facts that support our beliefs - making it even more easier to ignore evidence that our positions are wrong. So we live in a giant feedback loop - hearing our own thoughts about what is right and wrong being bounced back to us in the TV shows that we watch, books we read etc. We sort ourselves and also others in an unintentional and reflexive manner - but it creates distances and divisiveness. We are all ideologically diverse. However selecting like-minded friends and neighbors and separating ourselves from people whom we think of as different has not delivered that deep sense of belonging that we are hardwired to crave. Loneliness is - “perceived social isolation”.  We experience it when we feel disconnected. “Loneliness” and “being alone” are very different things. Being alone is finding therapy in solace, being lonely is feeling disconnected from your surroundings. To combat loneliness we must first learn to identify it, and then treat it like a warning sign - signaling a need for socialization - like the warning sign we get from our body when we are hungry (and need food), or thirsty (and need water). High lonesome can be a beautiful and powerful place if we can own our pain and share it, instead of inflicting pain on others - and if we can find a way to FEEL the hurt (and do something about it) rather than SPREAD the hurt. People are Hard to Hate Close Up: MOVE IN!: We might hate or dislike large groups of people of a certain category, but we can love individuals belonging to those categories. e. g. “Democrats are such losers” - yes - except your coworker who helped watch your kids when you were taking care of your sick family member. e. g. “Republicans are selfish assholes” - yes - except your son-in-law who is so kind and warm. We don’t like certain ideologies - which bring us pain - but to deny our own pain, or denying others pain - neither of those options really work. We need to be able to find a way to get on the same page. Anger needs to be transformed into something life-giving - like courage, love, change, compassion and justice. Courage is forged in pain - but not in all pain. Pain that is denied or ignored becomes fear or hate. Anger that is never transformed becomes resentment and bitterness. Some thoughts on conflict resolution:  Q: Sometimes we decide to “agree to disagree”. Does that work?   A: “Agree to disagree” without exploring the full nature of the disagreement leads to avoidance and making assumptions. The goal should be increased mutual understanding.   Q: If we decide to continue working on a disagreement, how do we push through the vulnerability and stay civil?   A: Find out what the conversation is “really about”, what is the “common goal”. Understand each others motives and interests more closely   Q: “Putting people on the stand” - you said “X” last week but you are saying “Y” now, which ones is it?   A: It never helps to go down that route. Focus on “where are we now”? “What are we trying to accomplish for the future”? “What do we want our relationship to be going forward”? Shift the focus on getting on the same page.   Q: “Conflict transformation” instead of “conflict resolution”. Why?   A: The latter dissipates the tension to go back to previous states of affairs, the former transforms the conflict to a higher level of understanding and creating something new from the situation.   Q: I am preparing arguments in my head when other people are talking. But I don’t like it when others do it to me. What is the alternative? A: One of the more courageous things to do is to say - “tell me more”. Just when you want to end the conversation, turn away etc - use it as an opportunity to fully understand the other persons perspective even better. Listen to them - really listen. We have to listen the same way we want to be understood.  We have to listen the same way we want to be understood. Speak Truth to Bullshit. Be Civil. : Liar, truth speaker - are different sides to the same game. Liar is aware of the authority of the truth, but ignores it. A bullshitter is playing another game - ignoring the rules of the game altogether - he pays no attention to truth or lying and just says what he wants to say. It is easier to stay civil when we are combating lying, as opposed to when we are speaking truth to bullshit. When we are bullshitting, we are not interested in the truth as a shared starting point. This makes arguing about truth not important and the focus shifts to - what I think matters and not the truth.  Civility is about disagreeing without disrespect and seeking common ground as the starting point. A man chided his father for using the term “oriental” for the fathers next door neighbor who the father was very close with. The father clearly didn’t know that “orient” was a derogatory term and was not used anymore. The conversation could be civil by simply stating it without judgment to the father instead of making the father feel ashamed for using the wrong word. It is a paradox to be able to be civil when someone is bullshitting and trying to illicit an emotional and irrational response to you. But when you belong to you, you are true to yourself, and it is easier to be civil when you are honoring yourself and your place in the world rather than responding to another persons emotion. Hold Hands with Strangers: Collective joy and collective pain are sacred experiences. The joy shared by a stadium full of people watching football, or the joy shared by humankind for successful space mission - brings us together in an inexplicable sense of being. The pain that we share when something goes wrong in our world - even imaginary world like when Professor Dumbledore died in Harry Potter is shared by our collective beings and the sorrow brings us together. The more we are willing to seek out the moments of collective joy (in person not online), the more it becomes difficult to deny our human connection. The bond that we often share with some people - connected by our collective hate of other people or other things, is a counterfeit connection as the opposite of true belonging. The emotion we experience is intense and immediately gratifying, and an easy way to discharge outrage and pain, but is NOT fuel for real connection. Social media is one such example - not a real connection. Experiencing collective joy or pain needs courage. And the foundation of courage is vulnerability. We need to be vulnerable and open to be able to experience joy - true joy - not an instant gratification like feeling that one might get by dehumanizing someone else. We are afraid to be vulnerable enough to feel joy - which is why when we experience it - we sucker punch it by thinking the most horrific thing we can. E. g. Vacation - hurricane, kids going to prom - car crash. The only way to combat it is by “gratitude”. It is only possible to fully lean in to joy if we can practice gratitude.  No vulnerability, no courage Strong back. Soft Front. Wild Heart. : Having a strong back - metaphorically speaking - and develop a spine that is flexible yet sturdy, we are confident in our shell, and have a strong core of belonging to ourselves. With a strong back, we can afford to have a soft front - vulnerable and open - letting the world see us for who we are and what we are. A soft and open front is not being weak - it is being brave and being the wilderness. A wild heart is being awake to the pain of the world, but does not diminish its own pain. A wild heart can beat with gratitude, lean it to pure joy. "
    }, {
    "id": 29,
    "url": "http://localhost:4000/blog/2019/persuasion/",
    "title": "Influence - The Psychology of Persuasion",
    "body": "2019/06/17 - This book is about how our brain loves shortcuts, and other people can make us follow them by pushing our emotional buttons. These manipulators are everywhere in the industry, they’re compliance professionals, and their entire job is to influence you into doing what they want. Author Page: Influence: The Psychology of Persuasion The world is a complex place, where you can’t process everything, and our brains evolved to use these shortcuts to make decisions. Sadly others can hack our brains. For example, people can skip a line by simply mentioning a reason At a printer line: “May I skip the line, because I’m in a rush” -94% complied If no reason was given, only 60% complied Fascinatingly, even BS reasons seemed to work“Can I skip the line, because I want to make copies” Our shortcut deems any reason sufficient There are 6 principles for exploitation, widely used by marketing and business people, be wary of them:  Reciprocation Scarcity Consistency Social proof Likability AuthorityI will expand on each one of them. 1. Reciprocation: Humans have an overpowering need to return favors. Ever received free samples? That is a trick, since people feel obliged to return favors, even much larger favors. Don’t fall for it. When colleagues were given a 10 cent gift as a nice gesture, people felt compelled to return 5 times more in return, value of 5 cokes. We are socially conditioned and evolved to return favors, because otherwise we upset the group, and we are seen as ingrates or moochers, and this fear of being labeled and perceived is what manipulators use to get us to do what they want. Manipulation tactic: Rejection and retreatStarting with an outrageous price, and retreating from there can win concessions This is because we feel bad and obliged to refuse the person yet again, even if the first offer was ludicrous We feel bad to say No. Don’t fall for it There is also a contrast principle, a bug in our brains:When two items are presented to us one after the other, we magnify the difference of the second from the first item presented. Defense against Reciprocation tactics:  Make a habit to think if people are genuine Check people’s reputation with others If you’re feeling obliged, don’t be. Say No.  Don’t negotiate with people that are playing dirty, and starting with unrealistic offers. 2. Scarcity exploitation - When something is hard to obtain, it makes us inclined to buy it our of fear of missing out (FOMO): Contributors to scarcity exploitation:  Recency bias (When we perceive the availability of something to be recently decreased, we want it more) Competitive nature (We hate losing to a rival) Loss aversion (We like winning, and we hate losing out)Examples of exploitation through scarcity: 80% off this weekLimited offerLimited stockLast chance, sale ends in two days Scarcity especially applies to time pressures. It causes an eagerness effect, hence people buy out of FOMO Irrational wish to posses junk A fun observation on sale exploitation: In experiments, when people perceived a sale to end, they were inclined to buy it. But they were inclined to buy it even MORE if they were told they were the only ones that knew of the deal. Manipulation compounds. Don’t be a sucker. Scarcity sales effect: Banning something makes it more attractive People want what they can’t have. Grass is always greener where you can’t step. Fixation on the forbidden Called Romeo &amp; Juliet effect. It’s human nature. It’s all desire. Humans hate losing opportunities How to defend against manipulation through scarcity:  Don’t be impulsive Don’t rush Have patience Ignore price decreasesThere is always time to make buys, rather than lose out of being impulsive in the long run. 3. Consistency - Staying true to your word: (even when that is not in your interest) We have a strong desire for consistency, especially internal, not just social. Done out of laziness, easier for the brain to copy past actions and habits than be creative. For example, when people observed a beach towel theft, only 20% reacted, but when explicitly asked “please watch this towel”, 95% people reacted, even chasing the guy What dictates consistency?Commitment. Socially we become accountable, and our reputation and perception matters to us. Especially if made explicit, publicly or easily verifiable commitments. Our self image is malleable, and others that wish to manipulate us can mold it through small unconscious commitments. See “Dumping strategies”When a new company has absurd low prices just to get consumers hooked, and change their beliefs. “I am an X brand buyer now” The harder something is to get, the more we value it. See signaling effects:  Luxury brands Tribalism Gang tattoos making you want a product, then mentioning the price after, or even changing it4. Social Proof - We often decide the best course of action by copying other people. : Ever wondered why laugh tracks are added in sitcoms. Because our brains short-circuit through imitation, and find stuff funny, which is not. See on Youtube “laugh track removed” videos Other examples of copying behavior:  “salting” donation boxes with some initial money best-seller luxury brands everyone facing same way in elevators bystander effects evolution of fashion how suicides increase after celebrity does it fake endorsementsThe more similar we are to others, the more they influence us. Ingroup vs. outgroup. Hence why salesmen try to always mention “I do that too” to get you to feel like you are part of the same tribe. Don’t fall for it. How to defend against social proof manipulation:  Be alert for fake signals Be wary if people are crafting perceptions on shaky foundations and external validation Ask reality, and reason from first-principles Never assume, always check5. Likability - We comply with people we like and it’s easy for others to trick us into liking them: Best person to sell you something is a friend. That’s why multi-level marketing schemes are so popular and perverse. A friend can never wish you harm, right?Unless he doesn’t know what he’s doing, or he has hidden motives. We’re suckers for compliments and familiarity. Hence why sales people frequently compliment us and claim similarities to us. We are also more drawn to attractive people, or popular people, even though they are not competent at all. (Halo effect) Examples:  good cop/bad cop interrogation “we on the same team” “we fighting the same fight”Anti-thesis: Shoot the messenger effect How to defend against likability manipulation:  Be skeptical It’s too good to be true If you liked someone recently, they’re probably sucking up6. Authority effect - We obey authorities because they appear competent. : We were raised to obey as kids (fathers, teachers) But adults are full of shit. Examples:  Electric shock experiment Nurse that received note “Add meds in R ear” from doctor, and ended up adding the meds in the poor guy’s ass (“Rear”) instead of right earNever assume, always check Authority negates independent thinking. We trust blindly, assume they know better. We try to infer symbols of authenticity (uniforms, titles) Just the brain being lazy. Don’t trust blindly. Even highly competent and authority figures. They’re human too. And even if they are competent, maybe their expertise is not applicable with the problem we are facing now. To avoid authority bias, ask these Q’s:  Is he an authority, or a fake? Is his authority relevant to the situation? How honest can an authority be in this situation? Do they have my best interests at heart. That was it. Hope it makes you understand your mental shortcuts better, before others manipulate you first. And remember:“Do your best not to fool yourself, and know that you’re the easiest person to fool” "
    }, {
    "id": 30,
    "url": "http://localhost:4000/blog/2019/Quiet/",
    "title": "Quiet - The Power of Introverts in a World That Can't Stop Talking",
    "body": "2019/06/10 - Quiet - The Power of Introverts in a World That Can’t Stop Talking, by Susan Cain: The book is a study of quiet intensity - and a study of introvertism and extrovertism and how each of these play a part in moving mankind forward. The book starts by giving the example of Rosa Parks - the quiet person who spoke up against segregation - and who became the reason for Martin Luther King to speak out against segregation. People consider introvertism as a second class trait, but without introvertism we would be devoid of “the theory of relativity”, “the theory of gravity”, “Google”, “Harry Potter” a lot of other creations, because the creators are all introverts. Guardian Article about the book: Quiet: The Power of Introverts in a World That Can’t Stop Talking by Susan Cain There isn’t an all-purpose definition of an introvert, but a few points that psychologists agree upon are: Introverts like less stimulation as compared to extroverts. Introverts work slower and are often more deliberate with their work than extroverts. Introverts prefer listening than talking. They enjoy social interaction, but also derive comfort in solitude. An introvert is not a synonym for a hermit, misanthrope, being shy or highly sensitive. How it all startedThe first section of the book talks about how Americans started moving towards wanting to be extroverted because of advertisements, because of the need to find a good partner in life, and other reasons. Americans have slowly moved away from internal strength to wanting to learn to project an external facade of self-assuredness. We are urged to develop an extroverted personality as a way of outshining the crowd in a competitive society. A Tony Robbins seminary, and people at Harvard Business School all represent this trait - of being outgoing, projecting confidence and projecting the sense of being sure of themselves. Introverts make good leaders, because of their ability to listen. Extroverts often want to win at all costs, and if the ideas are not presented to them in a strong and confident manner, might squash the very idea that can help the organization thrive. Creativity more often than not requires independence and solitude. Steve Wozniak created the first personal computer by himself. In his book Woz says that “most inventors and artists are like him - they live in their heads. Artists work best alone where they can control the design of the invention. ”  “If you are in the backyard sitting under a tree while everyone else is clinking glasses on the patio, you are most likely to have an apple fall on your head. ” Solitude is an important key to creativity - and we should all develop a taste for it - teach our children and employees how to work independently. However we are increasingly doing the opposite. It has been shown that to become to the best at anything you do (play chess, become an expert at violin) practicing in solitude was the key differentiator. It is only when you are alone that you can engage in deliberate practice which is the key to exceptional achievement. Deliberate practice is done in isolation because it takes intense concentration, and having other people around is distracting. It involves deep - self generated - motivation, and involves working on the task that is most challenging to YOU personally. Personal space is vital to creativity and so is freedom from peer pressure. It has also been shown scientifically that group brainstorming processes are much less effective than individual brainstorming processes. 40 years of research has reached the same startling conclusion. Why? - some people tend to sit passively, - some people are dominating speaking who block others, - some people are afraid of looking stupid in front of their peers. You - yourself - your biologyIn an experiment done on babies about their reaction to new experiences - it was discovered that babies who reacted actively to new toys grow up to be quiet introverted adults, and babies who were not bothered by new toys or experiences grew up to be extroverted adults. This was because the nervous system of the former reacted more strongly to unfamiliar objects as compared to the latter. Quiet infants were silent not because they were future introverts - just the opposite - because they had nervous systems that were unmoved by novelty. Most low reactive kids court danger from the time they are toddlers. They become desensitized to all sorts of experiences and keep growing they sensitivity to risk as they grow up. When low reactive children grow up in attentive families, they become achievers, but if they have negligent caregivers, they could become bullies. We all have free will, and can use it to shape our personalities. We are elastic and can stretch ourselves, but only so much. Our brains contains “amygdala” which learns emotions and fears. The frontal cortex helps to extinguish fears when new behaviors are learned. However, those fears come roaring back when the frontal cortex has other things to do in times of stress. Introvertism and extrovertism can be viewed as preferences for a certain level of stimulation. One tries to constantly and consciously situate themselves in environments that offer the right amount of stimulation. Highly sensitive people are keen observers who look before they leap. They are highly empathic, as it they have thinner boundaries between themselves and the world. They avoid violent movies and TV and are acutely aware of themselves. Introverts and extroverts react differently to the prospect of rewards. Our old brain (limbic system) constantly tells us - eat more, drink more, which is triggered by the reward seeking part of the brain. The new brain - neocortex - is more evolved and is responsible for thinking and planning. Both these parts of the brain work together in making decisions and choices. Extroverts dopamine pathways are more evolved than introverts, and they tend to experience more pleasure and excitement than introverts. Introverts have a smaller response to rewards. Extroverts experience “buzz” - which has a delightful champagne-bubble like quality. It gives you a “high” - but it also has considerable downsides - ignoring warning sides that we should be heeding and clouding judgment. Different cultures and extroverted idealAsian-American families have a culture of introvertism - where introvertism is celebrated. There is a strong sense of filial obligation and its connection to prioritizing work and study over social life. Gandhi was an introvert and shy. Over time he learned to manage his shyness but he never really overcame it. He couldn’t speak extemporaneously, and avoided making speeches whenever he could. But with his shyness came a unique brand of strength. Asians bring a culture of “soft power” - which isn’t limited to Gandhi or the excellence Asians bring to Math and Science. Quiet persistence requires sustained attention - in effect restraining ones reactions to external stimuli. How to love and how to workWhen should you act more extroverted than you really are? “Free trait theory”: We are born and culturally endowed with certain personality traits - introversion for example - but we can act out of character for core personal project. This theory explains how introverts are capable of acting like extroverts for the sale of work they consider important. When introverts were asked to act like extroverts - they came very close to mirroring a true extrovert. Psychologists call this “self-monitoring”. Self monitors are highly skilled at modifying their behavior to the social demands of the situation. Introverts like people they meet in friendly contexts, extroverts like people they compete with. "
    }, {
    "id": 31,
    "url": "http://localhost:4000/blog/2019/the-subtle-art-of-not-giving-a-f-ck/",
    "title": "The Subtle Art of Not Giving a F*ck",
    "body": "2019/06/04 - This book is about how we need to be careful about what we give importance to in our lives. How do we pick and chose the right things to give our attention to, and let go of other things. Author’s summary of the book: The Subtle Art of Not Giving a F*ck: A Counterintuitive Approach to Living a Good Life, by Mark Manson Don’t Try: Stop trying to be happy, stop trying to fight your present and yearn for something that you believe you should “have” in order to be happy. It is counter intuitive, but the more you yearn, the more you believe that you do not have something, and the loop continues. Instead, let that yearning go. Accept your current circumstances - stop trying to make lemonade from the lemons you were given, and instead learn to “stomach those lemons better”. Don’t be indifferent - that is ridiculous and actually “pathological”. Not caring about anything is as bad as caring about everything. Be accepting of who you are, and be careful about what you care about. When you stop giving a f*ck about what you don’t have, and become more comfortable being different. When you care less about something, you get better at it. You are not tied to the end goal, and you just accept the process for what it is, and it is easier to bear with the journey when you are not fixated on the goal. Everything worthwhile in life is won through surmounting the associated negative experience. “All the negatives in life lead to the positives. ” Happiness is a problem: A rich and entitled prince feels that he will be happier when we renounces his riches, a child feels that they will be happier when they build a complicated Lego set. . but this premise in itself is a problem. Life is not a math equation where if we solve for X then we will be happy. Human life is pain, and even though we hate the pain, it is useful. It helps us understand good from bad. Humans cannot differentiate between physical and psychological pain. Problems never stop in life - they merely get exchanged or upgraded. If we solve one problem, it creates another problem. E. g. if you get a gym membership to solve the “fitness” problem, it creates other problems of driving to the gym, showering after, logistics etc. The secret sauce is to start enjoying solving problems, rather than resenting having problems in the first place. To be happy we need to solve something - such is human nature. People mess up problem solving in 2 ways:  Denial - believing that the problem does not exist Victim Mentality - believing that nothing can be done to solve the problem and someone is to blame for this misfortuneWe like the idea that ultimate happiness can be attained - but it cannot! Everything comes with a sacrifice - whatever makes us feel good, will also make us feel bad. E. g. wanting to earning too much money will lead to new problems like needing to work long hours and spending time away from family. Choose your struggle - if you really really want a boatload of money, be OK with spending less time with your family. If you really want that fit body, be OK with sweating it out at the gym. No one is special: Adversity and failure are necessary for developing strong minded and successful adults. “Feeling good about yourself doesn’t mean anything unless you have a good reason to feel good about yourself”. You need adversities to overcome to believe in yourself. The true meaning of self worth is not about how positively you feel about yourself, but how positively you feel about the negative aspects of your life. Trying to feel good about yourself without earning it will make one feel entitled, and then one feels the need to feel good about themselves at the expense of others. It is very hard to break out of the self aggrandizing patterns. Entitlement is a failed strategy - just provides temporary highs - like alcohol or drugs. Entitlement plays out in 2 ways:  I am awesome and everyone else sucks, so I deserve special treatment I suck and everyone else is awesome, so I deserve special treatmentNo one is extraordinary - everyone is mediocre and great in different ways. A lot of people are afraid to accept mediocrity. The rare people who do accept mediocrity become exceptional - why? Because when you believe that you are mediocre, you strive to do better and become obsessed with improvement. There is value in suffering: Suffering is inevitable in our life. Instead of asking “How do I stop suffering? “, as “Why am I suffering? - what purpose? “. One needs to pick the right purpose to suffer through. It is important to have a good measure of your life. A rock-star who compares himself to a more successful band will always feel bad about his life. A rock-star who measures himself by his family and love that surrounded him will always be happy - even though both the rock-stars were kicked out of their band in their 20s. Some bad measurements:  Pleasure - it is great, but a horrible value to prioritize around.  Material success - Research shows that once one is able to provide for basic physical needs, the relationship between happiness and worldly success quickly approaches zero.  Always being right - Our brains are inefficient machines - we make poor choices - emotional decisions - misjudgments etc. People who base their self worth on being right about everything prevent themselves from learning from their mistakes.  Staying positive - Denying negative emotions leads to deeper and more prolonged negative emotions and to emotional dysfunction. The next few sections are dedicated to 5 counterintuitive values to adopt We are always choosing: There is only one difference between a problem being painful or being powerful - we choose it. E. g. a marathon that is practiced for is a glorious and important milestone, but the same marathon if forced upon you is that of pain and misery. Whether we consciously recognize it or not, we are always responsible for our experiences. It is impossible not to be! So the question is - we must give a f*ck about SOMETHING, what do we choose? The more responsibility you take of your life, the more powerful you will feel. A lot of people hesitate to take responsibility for their problems because they believe that to be responsible for your problems is to also be at fault for your problems. But this isn’t true. e. g. if someone puts a baby on your doorstep, the baby isn’t your fault, but your responsibility. e. g. a judge is responsible to issue a verdict, even though he is not at fault for the crime. Many people may be to blame for your unhappiness, but no one is ever responsible for your unhappiness but you! In life we all get dealt with cards. Some of us get better cards than others. It is easy to get hung up on the cards, but the real game is in the choices we make with those cards and not the actual cards themselves. Being wrong and accepting uncertainty: Growth is an endlessly iterative process. When we learn something new, we don’t go from wrong to right, we go from wrong to slightly less wrong. We shouldn’t seek to find the ultimate “right” answer, but should seek how to be slightly less wrong tomorrow. We don’t actually know what a positive or a negative experience is. Some of the most difficult and stressful moments of our lives also end up being the most formative and motivating. All we know for certain is what hurts in the moment and what doesn’t. And that is not worth much. The human brain is imperfect. We mistake things we see and hear, forget or misinterpret things quite easily. “The more you embrace being uncertain and not knowing, the more comfortable you will feel in knowing what you don’t know. “ Uncertainty removes all judgments of others and oneself. It is the root of progress and growth. We cannot learn anything without first not knowing anything. Before we observe or change our values into better healthier ones, we must first become uncertain of our current values. How to become less certain of yourself?  Ask yourself - what if I am wrong? Ask yourself - what would it mean if I were wrong? Ask yourself - would being wrong create a better or a worse problem than my current problem?Fail and you will go forward: Improvement is based on thousands of tiny failures, and the magnitude of success is based on how many times you have failed at something. If someone is better than you at something, it means that they have failed at it more than you have. Most people have fear of failure. A lot of fear of failure comes from measuring yourself incorrectly. E. g. If one measures themselves by making everyone they meet to like them - they are not in control and are at the mercy of other peoples judgmentsInstead, if one measures themselves by “improving their social life”, they are in control regardless of how people respond. If your metric for success is - buy a house and nice car - once it is achieved in 20 years, you will reach your midlife crisis, because the problem that you were chasing most of your adult life has gone away. Pain makes us stronger, more resilient, more grounded. Our most radical moments happen at the tail end of our worst moments (aka hitting rock bottom, or existential crisis, or weathering the shitstorm) Pain is part of the process, FEEL IT! To overcome failure, we need motivation - which is often driven by inspiration. When we don’t have the inspiration we feel that we will never get motivated to make a change. That is not true! Action creates inspiration. Which is why when you are not motivated - do something - anything - which will cause inspiration to trigger and your motivation to come back. Saying No: We must give a f*ck about “something” in order to value that “something”. And to value that “something”, we must reject what is “NOT that something”. We are defined by what we chose to reject. For any relationship to be healthy, both parties should be able to say no and hear no. Saying no is very liberating - it creates other opportunities and opens doors that actually aligns better with our values and our chosen metrics. Life is short. In the end we all die. There is no point giving a f*ck about things that do not serve us, that do not align with our values. "
    }, {
    "id": 32,
    "url": "http://localhost:4000/blog/2017/what-is-nlp/",
    "title": "What is Natural Language Processing (NLP)?",
    "body": "2017/12/12 - Last year I wrote a highly popular blog post about Natural Language Processing, Machine Learning, and Deep Learning. In this post, we will break down NLP further and talk about Rule-Based and Statistical NLP. I will discuss why everyone needs to know about NLP and AI (Artificial Intelligence), how Machine Learning (ML) fits into the NLP space (it is indispensable actually) and how we are using it in our daily life even without knowing about it. Introduction to NLP: Natural Language Processing or NLP is a phrase that is formed from 3 components - natural - as exists in nature, language - that we use to communicate with each other, processing - something that is done automatically. Putting these words together, we get Natural Language Processing or NLP - which stands for the approaches to “process” natural language or human language. This is a very generic term. What does this “processing” even mean? As a human, I understand English when someone talks to me, is that NLP? Yes! When done automatically, it is called Natural Language Understanding (NLU).              Human Understanding. Image Credit: http://www. stuartduncan. name   I translated some Hindi to English for my friend, is that NLP? Yes, it is called Machine Translation (MT) when done automatically.              Machine Translation. Image Credit: Google Cloud   As humans, we perform Natural Language Processing pretty well but we are not perfect; misunderstandings are pretty common among humans, and we often interpret the same language differently. So, language processing isn’t deterministic (which means that the same language doesn’t have the same interpretation, unlike math where 1 + 1 is deterministic and always equals 2) and something that might be funny to me, might not be funny to you. This inherent non-deterministic nature of the field of Natural Language Processing makes it an interesting and an NP-hard problem. In this sense, understanding NLP is like creating a new form of intelligence in an artificial manner that can understand how humans understand language; which is why NLP is a subfield of Artificial Intelligence. NLP experts say that if humans don’t agree 100% on NLP tasks (like language understanding, or language translation), it isn’t possible to model a machine to perform these tasks without some degree of error. Side note - if an NLP consultant ever tells you that they can create a model that is more precise than a human, be very wary of them. More about that in my post about 7 questions to ask before you hire your Data Science consultant. Rule-Based NLP vs Statistical NLP: NLP separated into two different sets of ideologies and approaches. One set of scientists believe that it is impossible to completely do NLP without some inherent background knowledge that we take for granted in our daily lives such as freezing temperatures cause hypothermia, hot coffee will burn my skin and so on. This set of knowledge is collectively known as commonsense knowledge and has brought about the field of Commonsense Reasoning and very many conferences and companies (a special mention to Cyc). Encoding commonsense knowledge is a very time intensive and manual effort driven process as is considered to be in the space of Rules-Based NLP. It is hard because commonsense knowledge isn’t found in the written text (discourse), and we don’t know how many rules we need to create, before the work for encoding knowledge is complete. Here is an example: as humans, we inherently understand the concepts of death and you will rarely find documents that describe it by explaining the existence and nonexistence of hydrocarbons. Similarly are the concepts of moving and dancing which usually do not require any explanation to a human, but a computer model requires the breakdown of moving into the origin, destination, and the concept of not being at the origin after the move has happened. Dancing, on the other hand, is also a type of moving, but it is obviously very different from a traditional move, and requires more explanation because you can move a lot and still end up in your original location, so what is the point of a dance move? Another set of scientists have taken a different (now deceptively mainstream) approach to NLP. Instead of creating commonsense data that is missing in textual discourse, their idea is to leverage large amounts of already existing data for NLP tasks. This approach is statistical and inductive in nature and the idea is that if we can find enough number of examples of a given problem, we could potentially solve it using the power of induction. Statistical NLP makes heavy use of Machine Learning for developing models and deriving insights from a labeled text. Rule-Based NLP and Statistical NLP use different approaches for solving the same problems. Here are a couple of examples: Parsing: Rules-Based uses Linguistic rules and patterns. E. g English has the structure of SVO (Subject Verb Object), Hindi has SOV (Subject Object Verb).  Statistical NLP induces linguistic rules from the text (so our models are only as good as our text), along with lots of labeling of the text to predict the most likely parse tree of a new data source.              Natural Language Parsing. Image Credit: cs. cornell. edu   Synonym extraction Rules-Based approaches use thesaurus and lists. Data sources such as Wordnet are very useful for deterministic rule-based approaches Statistical NLP approaches use statistics to induce thesaurus based on how similar words have similar contexts             Synonyms using Word2Vec. Image Credit: oscii-lab   Sentiment Analysis Rules-based approaches look for linguistic terms such as “love”, and “hate”, “like” and “dislike” etc. and deterministically classify text as positive and negative Statistical NLP approaches use Machine Learning and do some feature engineering to provide weights to linguistic terms to determine the positive and negative nature of texts. Which approach is better? Both the approaches have their advantages. Rules-based approaches mimic the human mind and present highly precise results, however, they are limited by what we provide as rules. Statistical approaches are less precise, but they have a much higher coverage than rules-based systems as they are able to account for cases that are not explicitly specified in the rules. Most institutions prefer to use a hybrid approach to NLP, using Rule-Based along with Statistical Systems. Solving NLP Problems: We use NLP every day when we do a google search. Search engines use Information Retrieval in their backend, which is one of the subfields of NLP. NLP had earned its popularity because of several mainstream types of language-based problems - such as text summarization, sentiment analysis, keyword extraction, question answering, conversational interfaces and chatbots, machine translation, to name a few. Conclusion: Today NLP is largely statistical because of the availability of massive amounts of data. We can use tools like Word2Vec to get us similarly occurring concepts, and search engines like Elastic Search to organize our text to make it searchable. We are able to use off the shelf tools like Stanford Core NLP to parse our data for us and other algorithms like Latent Dirichlet Allocation (LDA) to discover clusters and topics in the text. As a consumer, we use NLP every day - from your first google search of the day, to your curated daily news articles delivered to you, your online shopping experience and reading reviews, and your conversational assistants such as OK Google, Alexa, and Siri. NLP is embedded in our everyday lives, and we use it even without realizing it. The latest wave of conversational interfaces or chatbots are adding a human component to conversation, and we are finally blending the 2 approaches to NLP - Rule-Based and Statistical NLP. Where do we go from here? I am excited for the future of NLP, and although we are very far from NLP/AI taking over the world I am optimistic about computational power being more ingrained in our lives to make the world a better and easier place for us. To learn more about NLP and for additional NLP resources check out this cool blog post from Algorithmia. "
    }, {
    "id": 33,
    "url": "http://localhost:4000/blog/2016/NLP-ML/",
    "title": "Natural Language Processing vs. Machine Learning vs. Deep Learning",
    "body": "2016/06/08 - NLP, Machine Learning and Deep Learning are all parts of Artificial Intelligence, which is a part of the greater field of Computer Science. The following image visually illustrates CS, AI and some of the components of AI -  Robotics (AI for motion) Vision (AI for visual space - videos, images) NLP (AI for text) There are other aspects of AI too which are not highlighted in the image - such as speech, which is beyond the scope of this post. Here is what I discuss in this post:  What is Natural Language Processing? Machine Learning Deep Learning Relationship between NLP, ML and Deep LearningWhat is Natural Language Processing?: Natural Language Processing (or NLP) is an area that is a confluence of Artificial Intelligence and linguistics. It involves intelligent analysis of written language. If you have a lot of data written in plain text and you want to automatically get some insights from it, you need to use NLP. Some applications of NLP are:  Sentiment Analysis : Classification of emotion behind text content. e. g. movie reviews are good or bad. How can humans tell if a review is good or bad? Can use use the same features that humans use - presence of describing words (adjectives) such as “great” or “terrible” etc. ? Information extraction : Extracting structured data from text. e. g. relationships between country and name of president, acquisition relationship between buyer and seller etc.  Information retrieval : This is a synonym of search. It is the concept of retrieving the correct document given a query - like Google! For the curious, here is info on how to build your own search engine and some more details on the internals of Lucene (Apache Lucene is an open source search engine that is used in Elastic Search)Here is a more detailed post about NLP - What is Natural Language Processing? Machine Learning: Machine Learning (or ML) is an area of Artificial Intelligence (AI) that is a set of statistical techniques for problem solving. Machine Learning by itself is a set of algorithms that is used to do better NLP, better vision, better robotics etc. It is not an AI field in itself, but a way to solve real AI problems. Today ML is used for self driving cars (vision research from graphic above), fraud detection, price prediction, and even NLP. In order to apply ML techniques to NLP problems, we need to usually convert the unstructured text into a structured format, i. e. tabular format. Deep Learning: Deep Learning (which includes Recurrent Neural Networks, Convolution neural Networks and others) is a type of Machine Learning approach. Deep Learning is an extension of Neural Networks - which is the closest imitation of how the human brains work using neurons. Mathematically it involves running data through a large networks of neurons - each of which has an activation function - the neuron is activated if that threshold is reached - and that value is propagated through the network. Deep Learning is used quite extensively for vision based classification (e. g. distinguishing images of airplanes from images of dogs). Deep Learning can be used for NLP tasks as well. However it is important to note that Deep Learning is a broad term used for a series of algorithms and it is just another tool to solve core AI problems that are highlighted above. Relationship between NLP, ML and Deep Learning: The image below shows graphically how NLP is related ML and Deep Learning. Deep Learning is one of the techniques in the area of Machine Learning - there are several other techniques such as Regression, K-Means, and so on.  ML and NLP have some overlap, as Machine Learning as a tool is often used for NLP tasks. There are several other things that you need for NLP - NER (named entity recognizer), POS Tagged (Parts of peech tagger identifies Nouns, verbs and other part of speech tags in text). NLP has a strong linguistics component (not represented in the image), that requires an understanding of how we use language. The art of understanding language involves understanding humor, sarcasm, subconscious bias in text, etc. Once we can understand that is means to to be sarcastic (yeah right!) we can encode it into a machine learning algorithm to automatically discover similar patterns for us statistically. To summarize, in order to do any NLP, you need to understand language. Language is different for different genres (research papers, blogs, twitter have different writing styles), so there is a strong need of looking at your data manually to get a feel of what it is trying to say to you, and how you - as a human would analyze it. Once you figure out what you are doing as a human reasoning system (ignoring hash tags, using smiley faces to imply sentiment), you can use a relevant ML approach to automate that process and scale it. "
    }, {
    "id": 34,
    "url": "http://localhost:4000/blog/2015/word2vec/",
    "title": "Online Word2Vec for Gensim",
    "body": "2015/08/22 - Word2Vec [1] is a technique for creating vectors of word representations to capture the syntax and semantics of words. The vectors used to represent the words have several interesting features, here are a few:    Addition and subtraction of vectors show how word semantics are captured: e. g. \(king - man + woman = queen\)This example captures the fact that the semantics of $king$ and $queen$ are nicely captured by the word vectors     Similar words have similar word vectors: E. g. $king$ is most similar to - $queen$, $duke$, $duchess$  Here is the description of Gensim Word2Vec, and a few blogs that describe how to use it: Deep Learning with Word2Vec, Deep learning with word2vec and gensim, Word2Vec Tutorial, Word2vec in Python, Part Two: Optimizing, Bag of Words Meets Bags of Popcorn. One of the issues of the Word2Vec algorithm is that it is not able to add more words to vocabulary after an initial training. This approach to ‘freeze vocabulary’ might not work for several situations where we need to train the model in an online manner, by adding and training on new words as they are encountered. Here is a quick description of an online algorithm In this post, I will discuss an online word2vec implementation that I have developed and how to use it to update the vocabulary and learn new word vectors in an online manner. I maintain the code here: https://github. com/rutum/gensim How to use online word2vec: 1) Download the source code from here: https://github. com/rutum/gensim 2) On your local machine, browse to the location of the downloaded code, and install it by typing: #clean already existing installsudo rm -rf build dist gensim/*. pyc#installationsudo python setup. py install3) Now run the following lines of code from ipython or a seperate python file: import gensim. models# setup loggingimport logginglogging. basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging. INFO)# train the basic model with text8-rest, which is all the sentences# without the word - queenmodel = gensim. models. Word2Vec()sentences = gensim. models. word2vec. LineSentence( text8-rest )model. build_vocab(sentences)model. train(sentences)# Evaluation&gt; model. n_similarity([ king ], [ duke ])&gt; 0. 68208604377750204&gt; model. n_similarity([ king ], [ queen ])&gt; KeyError: 'queen'# text8-rest:&gt; model. accuracy( questions-words. txt )2015-08-21 10:56:49,781 : INFO : precomputing L2-norms of word weight vectors2015-08-21 10:56:56,346 : INFO : capital-common-countries: 33. 2% (168/506)2015-08-21 10:57:12,728 : INFO : capital-world: 17. 5% (254/1452)2015-08-21 10:57:15,807 : INFO : currency: 6. 0% (16/268)2015-08-21 10:57:32,402 : INFO : city-in-state: 15. 0% (235/1571)2015-08-21 10:57:35,197 : INFO : family: 50. 0% (136/272)2015-08-21 10:57:43,378 : INFO : gram1-adjective-to-adverb: 6. 0% (45/756)2015-08-21 10:57:46,406 : INFO : gram2-opposite: 12. 4% (38/306)2015-08-21 10:57:59,972 : INFO : gram3-comparative: 34. 6% (436/1260)2015-08-21 10:58:05,865 : INFO : gram4-superlative: 13. 2% (67/506)2015-08-21 10:58:17,331 : INFO : gram5-present-participle: 18. 2% (181/992)2015-08-21 10:58:31,446 : INFO : gram6-nationality-adjective: 37. 5% (514/1371)2015-08-21 10:58:45,533 : INFO : gram7-past-tense: 18. 3% (244/1332)2015-08-21 10:58:55,660 : INFO : gram8-plural: 30. 2% (300/992)2015-08-21 10:59:02,508 : INFO : gram9-plural-verbs: 20. 3% (132/650)2015-08-21 10:59:02,509 : INFO : total: 22. 6% (2766/12234)OK. So far so good. You will notice that I did some more evaluation on this data, by testing it against the same dataset that Google released, to compute the sysntactic and semantic relationships between words. As text8 is a small dataset, we don’t expect it to achieve very high levels of accuracy on this task, however, it will help us discern the difference in learning words in an online manner, vs learning it all in one sitting. You can download the script that I ran from here Now lets update the model with all the sentences containing queen and see if the vector for $queen$ is similar to that of $king$ and $duke$. Notice that the build_vocab function now has an additional argument update=True that add more words to the existing vocabulary. sentences2 = gensim. models. word2vec. LineSentence( text8-queen )model. build_vocab(sentences2, update=True)model. train(sentences2)# Evaluation&gt; model. n_similarity([ king ], [ duke ])&gt; 0. 47693305301957223&gt; model. n_similarity([ king ], [ queen ])&gt; 0. 68197327708244115# text8-rest + text8-queen (using update model)&gt; model. accuracy( questions-words. txt )2015-08-21 11:00:42,571 : INFO : precomputing L2-norms of word weight vectors2015-08-21 11:00:47,892 : INFO : capital-common-countries: 23. 3% (118/506)2015-08-21 11:01:02,583 : INFO : capital-world: 14. 1% (205/1452)2015-08-21 11:01:05,521 : INFO : currency: 4. 5% (12/268)2015-08-21 11:01:21,348 : INFO : city-in-state: 13. 2% (208/1571)2015-08-21 11:01:24,349 : INFO : family: 46. 4% (142/306)2015-08-21 11:01:31,891 : INFO : gram1-adjective-to-adverb: 6. 2% (47/756)2015-08-21 11:01:34,925 : INFO : gram2-opposite: 13. 4% (41/306)2015-08-21 11:01:47,631 : INFO : gram3-comparative: 32. 4% (408/1260)2015-08-21 11:01:52,768 : INFO : gram4-superlative: 11. 7% (59/506)2015-08-21 11:02:02,831 : INFO : gram5-present-participle: 18. 0% (179/992)2015-08-21 11:02:16,823 : INFO : gram6-nationality-adjective: 35. 2% (483/1371)2015-08-21 11:02:31,937 : INFO : gram7-past-tense: 17. 1% (228/1332)2015-08-21 11:02:42,960 : INFO : gram8-plural: 26. 8% (266/992)2015-08-21 11:02:49,822 : INFO : gram9-plural-verbs: 19. 2% (125/650)2015-08-21 11:02:49,823 : INFO : total: 20. 5% (2521/12268)BINGO! Looks like it learned the weights of the vector $queen$ quite well. NOTE: text8-rest, and text8-queen, and text8-all can be downloaded here: http://rutumulkar. com/data/onlinew2v/text8-files. zip. Here is how the files are divided: All sentences from text8 that have queen in them are in text8-queen, and the remaining sentences are in text8-rest. The file text8-all, is a concatenation of text8-rest and text8-queen. Here are the output accuracies that were achieve if we were to train the entire model in one go, as opposed to piecemeal in an online manner. Note that as the amount of data we are using is very little, the accuracy will vary a little due to the initialization parameters. sentences = gensim. models. word2vec. LineSentence( text8-all )model. build_vocab(sentences)model. train(sentences)# text8-all&gt; model. accuracy( questions-words. txt )2015-08-21 11:07:53,811 : INFO : precomputing L2-norms of word weight vectors2015-08-21 11:07:58,595 : INFO : capital-common-countries: 36. 0% (182/506)2015-08-21 11:08:12,343 : INFO : capital-world: 18. 9% (275/1452)2015-08-21 11:08:14,757 : INFO : currency: 4. 9% (13/268)2015-08-21 11:08:28,813 : INFO : city-in-state: 16. 4% (257/1571)2015-08-21 11:08:31,542 : INFO : family: 48. 4% (148/306)2015-08-21 11:08:38,486 : INFO : gram1-adjective-to-adverb: 6. 9% (52/756)2015-08-21 11:08:41,268 : INFO : gram2-opposite: 16. 7% (51/306)2015-08-21 11:08:52,507 : INFO : gram3-comparative: 34. 4% (434/1260)2015-08-21 11:08:57,148 : INFO : gram4-superlative: 12. 8% (65/506)2015-08-21 11:09:06,475 : INFO : gram5-present-participle: 19. 1% (189/992)2015-08-21 11:09:18,681 : INFO : gram6-nationality-adjective: 40. 0% (548/1371)2015-08-21 11:09:30,722 : INFO : gram7-past-tense: 18. 2% (243/1332)2015-08-21 11:09:39,516 : INFO : gram8-plural: 32. 7% (324/992)2015-08-21 11:09:45,498 : INFO : gram9-plural-verbs: 17. 1% (111/650)2015-08-21 11:09:45,499 : INFO : total: 23. 6% (2892/12268)As you can see, the output score does drop a little, when the model is updated in an online manner, as opposed to training everything in one go. The PR for my code can be found here: https://github. com/piskvorky/gensim/pull/435 ###References:###[1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space "
    }, {
    "id": 35,
    "url": "http://localhost:4000/blog/2015/statistics-for-everyone/",
    "title": "Understanding your Data - Basic Statistics",
    "body": "2015/05/10 - Have you ever had to deal with a lot of data, and don’t know where to start? If yes, then this post is for you. In this post I will try to guide you through some basic approaches and operations you can perform to analyze your data, make some basic sense of it, and decide on your approach for deeper analysis of it. I will use python and a small subset of data from the Kaggle Bikesharing Challenge to illustrate my examples. The code for this work can be found at this location. Please take a minute to download python and the sample data before we proceed. Below are the topics that I discuss in this post:  DESCRIPTION OF DATASET MIN MAX AND MEAN VARIABILITY IN DATA VARIANCE STANDARD DEVIATION STANDARD ERROR OF THE MEAN (SEM)DESCRIPTION OF DATASET: The data provided is a CSV file bikesharing. csv, with 5 columns - datetime, season, holiday, workingday, and count.  datetime: The date and time when the statistics were captured season: 1 = spring, 2 = summer, 3 = fall, 4 = winter holiday: whether the day is considered a holiday workingday: whether the day is neither a weekend nor holiday count: the total number of bikes rented on that dayMIN MAX AND MEAN: One of the first analyses one can do with their data, is to find the minimum, maximum and the mean. The mean (or average) number of bikes per day rented in this case is the sum of all bikes rented per day divided by the total number of days: $\bar{f} = \frac{\sum_{i=1}^{n} b_i}{n}$ where $\bar{f}$ is the mean, $b_i$ is the number of bikes rented on day $i$ and $n$ are the total number of days. We can compute these in python using the following code: from datetime import datetimefrom collections import defaultdictimport csvimport numpydata = defaultdict(int)prevdate =   with open( bikesharing. csv ,  r ) as fin:  csvreader = csv. reader(fin)  next(csvreader)  for row in csvreader:    date_object = datetime. strptime(row[0]. split(   )[0], '%m/%d/%y')    currdate = str(date_object. month) +  -  + str(date_object. day)    # Computing the total number of bikes rented in a day    data[currdate] += int(row[4])totals = []for key, value in data. iteritems():  totals. append(value)numpy. min(totals)numpy. max(totals)numpy. mean(totals)output:2752 #min13446 #max9146. 82 #meanIn this case, the mean is 9146. 82. It looks like there are several large values in the data, because the mean is closer to the max than to the min. Maybe the data will provive more insight if we compute the min, max and mean per day, grouped by another factor, like season or weather. Here is some code to compute mean per day grouped by the season: from datetime import datetimeimport csvimport numpydata = {}with open( bikesharing. csv ,  r ) as fin:  csvreader = csv. reader(fin)  next(csvreader)  for row in csvreader:    date_object = datetime. strptime(row[0]. split(   )[0], '%m/%d/%y')    currdate = str(date_object. month) +  -  + str(date_object. day)    if row[1] in data:      if currdate in data[row[1]]:        data[row[1]][currdate] += int(row[4])      else:        data[row[1]]. update({ currdate : int(row[4])})    else:      data[row[1]] = { currdate : int(row[4]) }for key, value in data. iteritems():  print key  totals = []  for k, val in value. iteritems():    totals. append(val)  print numpy. min(totals)  print numpy. max(totals)  print numpy. mean(totals)output:1 #season2752 #min10580 #max5482. 42105263 #mean262521308810320. 7368421378181344611239. 684210545713129829544. 45614035As you can see, the mean varies significantly with the season. It intuitively makes sense because we would expect more people to ride a bike in the summer as compared to the winter, which means a higher mean in the summer than the winter, this also means higher min and max values in the summer than the winter. This data also helps us intuitive guess that season 1, is most likely winter, and season 3 is most likely summer. VARIABILITY IN DATA: The next thing we would like to know, is the variability of the data provided. It is good to know if the data is skewed in a particular direction, or how varied it is. If the data is highly variable, it is hard to determine if the mean changes with different samples of data. Reducing variability is a common goal of designed experiments, and this can be done by finding subsets of data that have low variablity such that samples from each of the subsets produce similar mean value. We already did a little bit of that in the second example above. There are 2 ways of measuring variability: variance and standard deviation. VARIANCE: Variance is defined as the average of the squared differences from the mean. In most experiments, we take a random sample from a population. In this case, we will compute the population variance, which uses all possible data provided. Population variance can be computed as: [\sigma^2 = \frac{\sum_{i=1}^{n} (f_i - \bar{x})^2}{N}] If you needed to compute the sample variance, you can use the following formula: [variance = \frac{\sum_{i=1}^{n} (f_i - \bar{x})^2}{N-1}] where $x_i $ is each instance, $\bar{x}$ is the mean, and $N$ is the total number of features. Dividing by n−1 gives a better estimate of the population standard deviation for the larger parent population than dividing by n, which gives a result which is correct for the sample only. This is known as Bessel’s correction. In our case we will compute population variance using most of the same code as that above, except adding the following line to it: print numpy. var(totals)Output:1 #season2752 #min10580 #max5482. 42105263 #mean2812044. 87535 #variance262521308811239. 68421052953435. 21145378181344611239. 68421051368005. 268745713129829544. 456140352552719. 23053STANDARD DEVIATION: Variance by itself is not particularly insightful, as its units are feature squared and it is not possible to plot it on a graph and compare it with the min, max and mean values. The square root of variance is the standard deviation, and it is a much more insightful metric. The population standard deviation, $\sigma$, is the square root of the variance, $\sigma^2$. In python you can compute variance by adding the following line to the above code: print numpy. var(totals)output:1 #season2752 #min10580 #max5482. 42105263 #mean1676. 91528568 #standard deviation262521308810320. 73684211718. 55614149378181344611239. 68421051169. 617573745713129829544. 456140351597. 72313951A standard deviation close to 0 indicates that the data points tend to be very close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values. In our case, the data is very spread out. Three standard deviations from the mean account for 99. 7% of the sample population being studied, assuming the distribution is normal (bell-shaped). STANDARD ERROR OF THE MEAN (SEM): In this post, we have computed the population mean, however, if one has to compute the sample mean, it is useful to know how accurate this value is in estimating the population mean. SEM is the error in estimating $\mu$. [SEM = \frac{\sigma}{\sqrt{N}}] however, as we often are unable to compute the population standard deviation, we will use teh sample standard deviation instead: [SEM = \frac{s}{\sqrt{N}}] The mean of any given sample is an estimate of the population mean number of features. Two aspects of the population and the sample could affect the variability of the mean number of features of those samples.  If the population of number of features has very small standard deviation, then the samples from that population will have small sample standard deviation the sample means will be close to the population mean and we will have a small standard error of the mean If the population of number of features has a large standard deviation, then the samples from that population will have large sample standard deviation the sample means may be far from the population mean and we will have a large standard error of the meanSo large population variability causes a large standard error of the mean. The estimate of the population mean using 2 observations is less reliable than the estimate using 20 observations, and much less reliable than the estimate of the mean using 100 observations. As N gets bigger, we expect our error in estimating the population mean to get smaller. "
    }, {
    "id": 36,
    "url": "http://localhost:4000/blog/2014/all-about-that-bayes-intro-to-probability/",
    "title": "An Introduction to Probability",
    "body": "2014/07/07 - This post is an introduction to probability theory. Probability theory is the backbone of AI, and the this post attempts to cover these fundamentals, and bring us to Naive Bayes, which is a simple generative classification algorithm for text classification. In this post we will cover the following:  Random Variables Simple Probability Probability of 2 Events Conditional Probability Difference between conditional and joint probability Bayes Rule Naive Bayes Further ReadingRandom VariablesIn this world things keep happening around us. Each event occurring is a Random Variable. A Random Variable is an event, like elections, snow or hail. Random variables have an outcome attached them - the value of which is between 0 and 1. This is the likelihood of that event happening. We hear the outcomes of random variables all the time - There is a 50% chance or precipitation, The Seattle Seahawks have a 90% chance of winning the game. Simple ProbabilityWhere do we get these numbers from? From past data.       Year   2008   2009   2010   2011   2012   2013   2014   2015       Rain   Rainy   Dry   Rainy   Rainy   Rainy   Dry   Dry   Rainy   [p(Rain=Rainy) = \frac{\sum(Rain=Rainy)}{\sum(Rain=Rainy) + \sum(Rain=Dry)} = \frac{5}{8}] [p(Rain=Dry) = \frac{\sum(Rain=Dry)}{\sum(Rain=Rainy) + \sum(Rain=Dry)} = \frac{3}{8}] Probability of 2 EventsWhat is the probability that event A and Event B happening together? Consider the following table, with data about the Rain and Sun received by Seattle for the past few years.       Year   2008   2009   2010   2011   2012   2013   2014   2015       Rain   Rainy   Dry   Rainy   Rainy   Rainy   Dry   Dry   Rainy       Sun   Sunny   Sunny   Sunny   Cloudy   Cloudy   Cloudy   Sunny   Sunny   Using the above information, can you compute what is the probability that it will be Sunny and Rainy in 2016? We can get this number easily from the Joint Distribution     RAIN  Rainy  Dry  SUN  Sunny  3/8  2/8  Cloudy  2/8  1/8In 3 out of the 8 examples above, it is Sunny and Rainy at the same time. Similarly, in 1 out of 8 times it is Cloudy and it is Dry. So we can compute the probability of multiple events happening at the same time using the Joint Distribution. If there are more than 2 variables, the table will be of a higher dimension We can extend this table further include Marginalization. Marginalization is just a fancy word for adding up all the probabilities in each row, and the probabilities in each column respectively.           RAIN        Rainy    Dry    Margin      SUN    Sunny    0. 375    0. 25    0. 625        Cloudy    0. 25    0. 125    0. 375        Margin    0. 625    0. 375    1Why are margins helpful? They remove the effects of one of the two events in the table. So, if we want to know the probability that it will rain (irrespective of other events), we can find it from the marginal table as 0. 625. From Table 1, we can confirm this by computing all the individual instances that it rains - 5/8 = 0. 625 Conditional ProbabilityWhat do we do when one of the outcomes is already given to us? On this new day in 2016, it is very sunny, but what is the probability that it will rain? [P(Rain=Rainy \mid Sun=Sunny)] which is read as - probability that it will rain, given that there is sun. This is computed in the same way as we compute normal probability, but we will just look at the cases where Sun = Sun from Table 1. There are 5 instances of Sun = Sun in Table 1, and in 3 of those cases Rain = Rain. So the probability of [P(Rain=Rainy \mid Sun=Sunny) = 3/5 = 0. 6] We can also compute this from Table 3. Total probability of Sun = 0. 625 (Row 1 Marginal probability). Probability of Rain and Sun = 0. 375 Probability of Rain given Sun = 0. 375/0. 625 = 0. 6 Difference between conditional and joint probabilityConditional and Joint probability are often mistaken for each other because of the similarity in their naming convention. So what is the difference between: $ P(AB) $ and $ P(A \mid B) $ The first is Joint Probability and the second is Conditional Probability. Joint probability computes the probability of 2 events happening together. In the case above - what is the probability that Event A and Event B both happen together? We do not know whether either of these events actually happened, and are computing the probability of both of them happening together. Conditional probability is similar, but with one difference - We already know that one of the events (e. g. Event B) did happen. So we are looking for the probability of Event A, when we know the Event B already happened or that the probability of Event B is 1. This is a subtle but a significantly different way of looking at things. Bayes Rule[P(A B) = P(A) \, P(B \mid A)] [P(B A) = P(B) \, P(A \mid B)] [P(A B) = P(B A)] Equating (4) and (5) [P(A \mid B) = \frac{P(B \mid A) \, P(A)}{P(B)}] This is the Bayes Rule. Bayes Rule is interesting, and significant, because we can use it to discover the conditional probability of something, using the conditional probability going the other direction. For example: to find the probability $ P(death \mid smoking)$ , we can get this unknown from $ P(smoking \mid death) $, which is much easier to collect data for, as it is easier to find out whether the person who died was a smoker or a non smoker. Lets look at some real examples of probability in action. Consider a prosecutor, who wants to know whether to charge someone with a crime, given the forensic evidence of fingerprints, and town population. The data we have is the following:  One person in a town of 100,000 committed a crime. The probability that is he guilty $ P(G) = 0. 00001$, where $P(G)$ is the probability of a person being guilty of having committed a crime The forensics experts tell us, that if someone commits a crime, then they leave behind fingerprints 99% of the time. $P(F \mid G) = 0. 99$, where $P(F \mid G)$ is the probability of fingerprints, given crime is commited There are usually 3 people’s fingerprints in any given location. So $P(F) = 3 * 0. 00001 = 0. 00003$. This is because only 1 in 100,000 people could have their fingerprintsWe need to compute: [P(G \mid F)] Using Bayes Rule we know that: [P(G \mid F) = \frac{P(F \mid G) \, P(G)}{P(F)}] Plugging in the values that we already know: [P(G \mid F) = \frac{0. 99 * 0. 00001}{0. 00003}] [P(G \mid F) = 0. 33] This is a good enough probability to get in touch with the suspect, and get his side of the story. However, when the prosecutor talks to the detective, the detective points out that the suspects actually lives at the scrime scene. This makes it highly likely to find the suspect’s fingerprints in that location. And the new probability of finding fingerprints becomes : $P(F) = 0. 99$ Plugging in those values again into (9), we get: [P(G \mid F) = \frac{P(F \mid G) \, P(G)}{P(F)}] [P(G \mid F) = \frac{0. 99 * 0. 00001}{0. 99}] [P(G \mid F) = 0. 00001] So it completely changes the probability of the suspect being guilty. This example is interesting because we computed the probability of a $P(G \mid F)$ using the probability of $P(F \mid G)$. This is because we have more data from previous solved crimes about how many peple actually leave fingerprints behind, and the correlation of that with them being guilty. Another motivation for using conditional probability, is that conditional probability in one direction is often less stable that the conditional probability in the other direction. For example, the probability of disease given a symptom $P(D \mid S)$ is less stable as compared to probability of symptom given disease $P(S \mid D)$ So, consider a situation where you think that you might have a horrible disease Severenitis. You know that Severenitis is very rare and the probability that someone actually has it is 0. 0001. There is a test for it that is reasonably accurate 99%. You go get the test, and it comes back positive. You think, “oh no! I am 99% likely to have the disease”. Is this correct? Lets do the Math. Let $P(H \leftarrow w)$ be the probability of Health being well, and $P(H \leftarrow s)$ be the probability of Health being sick. Let and $P(T \leftarrow p)$ be the probability of the Test being positive and $P(T \leftarrow n)$ be the probability of the Test being negative. We know that the probability you have the disease is low $P(H \leftarrow s) = 0. 0001$. We also know that the test is 99% accurate. What does this mean? It means that if you are sick, then the test will accurately predict it by 99% $P(T \leftarrow n \mid H \leftarrow w) = 0. 99$ $P(T \leftarrow n \mid H \leftarrow s) = 0. 01$ $P(T \leftarrow p \mid H \leftarrow w) = 0. 01$ $P(T \leftarrow p \mid H \leftarrow s) = 0. 99$ We need to find out the probability that you are sick given that the test is positive or $P(H \leftarrow s \mid T \leftarrow p)$ Using Bayes Rule: [P(H \leftarrow s \mid T \leftarrow p) = \frac{P(T \leftarrow p \mid H \leftarrow s) \, P(H \leftarrow s)}{P(T \leftarrow p)}] We know the numerator, but not the denominator. However, it is easy enough to compute the denominator using some clever math! We know that the total probability of $P(H \leftarrow s \mid T \leftarrow p) + P(H \leftarrow w \mid T \leftarrow p) = 1$ [P(H \leftarrow s \mid T \leftarrow p) = \frac{P(T \leftarrow p \mid H \leftarrow s) \, P(H \leftarrow s)}{P(T \leftarrow p)}] [P(H \leftarrow w \mid T \leftarrow p) = \frac{P(T \leftarrow p \mid H \leftarrow w) \, P(H \leftarrow w)}{P(T \leftarrow p)}] Adding (16) and (17), and equating with (15) we get: [\frac{P(T \leftarrow p \mid H \leftarrow s) \, P(H \leftarrow s)}{P(T \leftarrow p)} + \frac{P(T \leftarrow p \mid H \leftarrow w) \, P(H \leftarrow w)}{P(T \leftarrow p)} = 1] Therefore: [P(T \leftarrow p) = P(T \leftarrow p \mid H \leftarrow s) \, P(H \leftarrow s) + P(T \leftarrow p \mid H \leftarrow w) \, P(H \leftarrow w)] Substituting (7) into (4) we get: [P(H \leftarrow s \mid T \leftarrow p) = \frac{P(T \leftarrow p \mid H \leftarrow s) \, P(H \leftarrow s)}{P(T \leftarrow p \mid H \leftarrow s) \, P(H \leftarrow s) + P(T \leftarrow p \mid H \leftarrow w) \, P(H \leftarrow w)}] [P(H \leftarrow s \mid T \leftarrow p) = \frac{0. 99 \times 0. 0001}{0. 99 \times 0. 0001 + 0. 01 \times 0. 9999}] [= 0. 0098] This is the reason why doctors are hesitant to order expensive tests if it is unlikely tht you have the disease. Even though the test is accurate, rare diseases are so rare that the very rarity dominates the accuracy of the test. Naive BayesWhen someone applies Naive Bayes to a problem, they are assuming conditional independence of all the events. This means: [P(ABC… \mid Z) = P(A \mid Z) \, P(B \mid Z) \, P(C \mid Z) \,…] When this is plugged into Bayes Rules: [P(A \mid BCD…) = \frac{P(BCD…\mid A ) \, P(A)}{P(BCD…)}] [= \frac{P(B\mid A ) \, P(C \mid A) P(D \mid A) … P(A)}{P(BCD…)}]  Let $P(BCD…) = \alpha$ which is the normalization constant. Then,[= \alpha \times P(B\mid A ) \, P(C \mid A) P(D \mid A) … P(A)] What we have done here, is assumed that the events A, B, C etc. are not dependent on each other, thereby reducing a very high dimensional table into several low dimensional tables. If we have 100 features, and each feature can take 2 values, then we would have a table of size $2^{100}$. However, assuming independence of events we reduce this to one hundred 4 element tables.  Naive Bayes is rarely ever true, but it often works because we are not interested in the right probability, but the fact that the correct class has the highest probability. Further Reading  A gentle review of Basic Probability"
    }, {
    "id": 37,
    "url": "http://localhost:4000/blog/2014/build-your-own-search-engine/",
    "title": "Build your own search Engine",
    "body": "2014/05/20 - In this post, I will take you through the steps for calculating the $tf \times idf$ values for all the words in a given document. To implement this, we use a small dataset (or corpus, as NLPers like to call it) form the Project Gutenberg Catalog. This is just a simple toy example on a very small dataset. In real life we use much larger corpora, and need some more sophisticated tools in order to handle large amounts of data. To brush up on the basic concepts of $tf \times idf$ you might want to check out my post on the The Math behind Lucene. For this exercise, we will use the Project Gutenberg Selections which are released as part of NLTK Data. NLTK - Natural Language Toolkit - is a python based module for text processing. The corpus - Project Gutenberg Selections - contains 18 files. Each file is a complete book. Our task is to calculate the $tf \times idf$ of all the words for each of the documents provided. In the end of this exercise we will have 18 documents, with the $tf \times idf$ of each word in each of the documents. Documents with $tf \times idf$ values for each word (or token) are often used (with the vector space model) to compute the similarity between two documents. Such statistics are quite relevant in Information Retrieval, Search Engines, Document Similarity, and so on.  NOTE: All the materials needed for this exercise (code + data) can be downloaded from my github repo. The code is written in perl, and is heavy in regular expressions. I have tried my best to document the code, but if you have any issues, or discover bugs, please do not hesitate in contacting me. I will not go into the details of all my code in this post, but will highlight a few features. This is one of the most important tasks of any NLP application. Discourse often contains non-ascii characters, no alphanumeric characters, spaces, and so on. The first and most important task is to remove these unwanted characters from text, to clean it up. Here is a set of regular expressions (regex) that is helpful for this dataset. However it is not an exhaustive set of regexes. For instance, I have not used any regex to convert UTF8 to ASCII. def test():      #remove endline character      chomp($txt);      #remove extra space characters      $txt =~ s/[\h\v]+/ /g;      #remove caps      $txt =~ tr/[A-Z]/[a-z]/;      #remove non-alphanumeric characters      $txt =~ s/[^a-zA-Z\d\s]//g;The rest of my code can be downloaded from my github repo. The code is relatively easy to understand if you are familiar with regular expressions, and understand perl. I have provided my solutions in the output directory, within the folder $tf \times idf$. The intermediate $tf$ and $idf$ results are also provided in the output folder. It is interesting to note here that words occurring in all the documents have an idf value of 0. This means that their tf*idf value will also be 0, deeming them insignificant in contributing to the document vector. These include words like - a, the, when, if, etc. A longer list can be developed by sorting all the values in the file idf. txt (downloaded from my github repo). The next step, is to use these metrics to compute document similarity. Discovering similar document in a corpus of documents remains one of the most important problems of information retrieval. This is often done by converting documents into document vectors, and comparing the similarity of these vectors to each other using vector similarity metrics. Note: All the code for this can be downloaded from my github repo. This post uses the output generated in the toy example for $tf \times idf$ which is available in this github repo. CREATING DOCUMENT VECTORS After computing the $tf \times idf$ values for each document in a given corpus, we need to go through the exercise to convert these values into a document vector. If you recall some vector basics, a vector constitutes a magnitude and a direction. In our case, the direction is represented by a word in the document and magnitude is the weight or the $tf \times idf$ value of the word in the document. In order to simplify our vector calculations we pre-specify the locations of each word in the array representing the document, creating a sparse vector. For instance consider the vocabulary of the entire corpus to be:John, Mary, Susan, Kendra, sang, for, with, plays We will assign a location to each word in the vocabulary as:       John   Mary   Susan   Kendra   sang   for   with   dances       0   1   2   3   4   5   6   7   Given this data, for the sentence: John sang for Mary, we create the following boolean document vector:       John   Mary   Susan   Kendra   sang   for   with   dances       1   1   0   0   1   1   0   0   Here a 1 represents the existence of a word in the text, and a 0 represents the absence of a word from the text. This example is simply to illustrate the concept of creating a document vector. In our toy example we will replace with 1 values with the tf*idf values of a given word in the given document. Computing the dot Product of two Vectors           John   Mary   Kendra   sang   for   with   dances   Sentence       d1   0   0. 27   0   0   0   0. 1   0. 1   John dances with Mary.        d2   0   0   0. 1   0. 27   0. 27   0   0   John sang for Kendra.        d3   0   0   0. 1   0   0   0. 1   0. 1   John dances with Kendra.    dot product between d1 and d2: [d1 \cdot d2 = d1. John * d2. John + d1. Mary * d2. Mary + …. ] The final similarity scores are:       doc3. txt   doc2. txt   0. 178554901188263       doc3. txt   doc1. txt   0. 377800203993898         doc2. txt   doc3. txt   0. 178554901188263       doc2. txt   doc1. txt   0         doc1. txt   doc3. txt   0. 377800203993898       doc1. txt   doc2. txt   0   Notice that document 1 is most similar to document 3. Document 2 and document 1 have absolutely no similarities. This is because both these documents have just one word in common - John, which is common to all the documents, so has a weight of 0. The code is documented, and can be downloaded from my github repo. Happy Coding! "
    }, {
    "id": 38,
    "url": "http://localhost:4000/blog/2014/core-of-lucene/",
    "title": "The Math behind Lucene",
    "body": "2014/04/20 - Lucene is an open source search engine, that one can use on top of custom data and create your own search engine - like your own personal google. In this post, we will go over the basic math behind Lucene, and how it ranks documents to the input search query. THE BASICS - TF*IDF The analysis of language often brings us in situations where we are required to determine the weight or importance of a given word in a document, to determine the relative importance or similarity of a document to another document. In situations such as this, the first step to remove stop words which are basically words that dont contribute to the general focus of a given article. Most common stop words are words like - a, when, who, what. The list of stop words keeps changing based on the domain of discourse. For instance, in a corpus of articles about the human heart, the word heart could potentially be a stop word due to the sheer frequency in which it is mentioned. It is always a good idea to remove stop words in a given text before processing it. However, once these stop words are removed, one still faces the task of determining the relative importance or weights of the remaining words in the document - lets start talking about tf-idf. Observing the words in a document, an intuitive way to discover the importance of a given word is to count the frequency of the word in the document. This is the Term Frequency or the tf of the word in the given document. tf is often normalized so as to not introduce a bias because of the document size. A normalized tf for a given word in a given document is calculated as: [tf_{w,d} = \frac{C_{w,d}}{\sum C_{w,d}}] where, \(C_{w,d}\) is the word count of word \(w\) in document \(d\) and $\sum C_{w,d}$ is the total count of all words \(w\) in document $d$. However, $tf$ by itself is not enough to capture the importance of a word in a document, as there are numerous words like a, the, with, however, that have a very high frequency, but less importance. So we need to complement this with the Document Frequency of each word - or $df$. Document Frequency is the total number of documents a given word occurs in. If a word occurs in a large number of documents, it has a high $df$. When this $df$ is used to scale the weight of a word in the document, it is called $idf$ or Inverse Document Frequency of that given word. If $N$ is the total number of documents in a given corpus, then the $IDF$ of a given word in the corpus is: [idf_w = log \frac{N}{df_w}] Notice that $tf$ is calculated for a given word in a given document, $IDF$ is calculated for a given word over all the documents. Once we have the $tf$ and $idf$, we can calculate the $tf \times idf$ score for each word to determine the weight of a word in the given document. The score $tf_w \times idf_{w,d}$ assigns each word $w$ a weight in document. Here are some insights about it. $tf_w \times idf_{w,d}$ is highest when w occurs many times within a small number of documents. This means that the word is significant, and can help establish relevance within the small number of documents. $tf_w \times idf_{w,d}$ is lower when the word occurs fewer times in a document, or occurs in many documents. If a word occurs infrequently, it might not be of significance. Similarly if a word occurs in a very large number of documents, it probably will not help in discriminating between the document. $tf_w \times idf_{w,d}$ is lowest when the term occurs in virtually all documents. This covers words like *a*, *an* *the*, *when* etc. which span a large number of documents and have no contribution towards the semantic composition of the document. FROM TF*IDF TO DOCUMENT SIMILARITY Document vectors are created by computing the relative weights of each word in the document. One way to accomplish this is by computing the $tf \times idf$ values of each of the words in the document. When we compute the $tf \times idf$ of each of the words in a document, we end up with documents with a list of features (words) with their values (weights). In a sense, this represents the document vector. The representation of documents as vectors in a common vector space is known as a vector space model, and is the basis of a large number of information retrieval tasks. A standard way of computing the document similarity is to compute the cosine similarity of the vector representations of the documents. If $d_1$ and $d_2$ are two documents, and $V(d_1)$ and $V(d_2)$ are the vector representations of them respectively, then the similarity of $d_1$ and $d_2$ can be measured as the cosine of the angle between $V(d_1)$ and $V(d_2)$ [sim(d_1, d_2) = \frac{V(d_1) \cdot V(d_2)}{\mid V(d_1) \mid \mid V(d_2) \mid}] In this equation, the numerator is the dot product of vectors $V(d1)$ and $V(d2)$, and the denominator is the product of the Euclidean length. Euclidean Length is the sum of squares of the magnitude of each element of the vector. If $V(d_1)$ and $V(d_2)$ are the following, [V(d_1) = [a_1, a_2, a_3, a_4 …]] [V(d_2) = [b_1, b_2, b_3, b_4 …]] The dot product of $V(d_1)$ and $V(d_2)$ is: [V(d_1) . V(d_2) = a_1b_1 + a_2b_2 + a_3b_3 + a_4b_4 + …] The Euclidean length of $V(d_1)$: [\sqrt{a_1^2 + a_2^2 + a_3^2 + a_4^2 + … }] Similarly, the Euclidean length of $V(d_2)$: [\sqrt{b_1^2 + b_2^2 + b_3^2 + b_4^2 + … }] Applying (6), (7) and (8) to (3) we get: [sim(d_1, d_2) = \frac{a_1b_1 + a_2b_2 + a_3b_3 + a_4b_4 + …)}{\sqrt{a_1^2 + a_2^2 + a_3^2 + a_4^2 + … } \sqrt{b_1^2 + b_2^2 + b_3^2 + b_4^2 + … }}] Ok, we have cosine similarity. Now what?What Cosine similarity tells us, is how similar 2 documents are. If the documents are very similar, we have the similarity score closer to 1, but if the documents are completely different, the similarity score is closer to -1. This is the heart of the scoring mechanism that Lucene uses to retrieve similar documents given a search query. Although Lucene does use a couple of other (mostly user defined) constants to fine tune the results, $tf \times idf$ is the heart of how Lucene operates. References:  Introduction to Information Retrieval, by Christopher D. Manning, Prabhakar Raghavan &amp; Hinrich Schütze, Chapter on Scoring, term weighting and the vector space model Information Retrieval Facility"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});