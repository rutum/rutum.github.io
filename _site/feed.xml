<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rutu Mulkar</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 14 Jul 2023 21:59:59 -0700</pubDate>
    <lastBuildDate>Fri, 14 Jul 2023 21:59:59 -0700</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>Machine Learning, Deep Learning and Large Language Models</title>
        <description>&lt;p&gt;Starting the early 2000s, the improvements in hardware to support deep learning networks has lead to a leap in modern deep learning approaches. Deep Learning (Hinton et al. 2006), (Bengio et al. 2007), which is an extension of neural networks,  contain an input, an output, and a large number of hidden layers between the input and output. This type of an architecture is able to capture non-linear relationships in data, and are better at modeling data. Deep learning works much better than their Machine Learning predecessors as shown by their performance in several types of benchmark datasets such as &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD (The Stanford Question Answering Dataset)&lt;/a&gt;, &lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;&gt;GLUE (General Language Understanding Evaluation)&lt;/a&gt;, &lt;a href=&quot;https://sites.research.google/xtreme&quot;&gt;XTREME (&lt;strong&gt;(X)&lt;/strong&gt; Cross-Lingual &lt;strong&gt;Tr&lt;/strong&gt;ansfer &lt;strong&gt;E&lt;/strong&gt;valuation of &lt;strong&gt;M&lt;/strong&gt;ultilingual &lt;strong&gt;E&lt;/strong&gt;ncoders)&lt;/a&gt; and others.  Deep learning is applicable to a large variety of applications ranging from Natural Language Processing, Speech Recognition, Computer Vision etc. For this doc, I will focus on Deep Learning as it pertains to Natural Language Processing, as it gives me the opportunity to delve deeper into LLMs - the newest kid on the block with Deep Learning. This doc is split into the following layout:&lt;/p&gt;

&lt;!-- vscode-markdown-toc --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#from-ml-to-deep-learning&quot;&gt;From ML to Deep Learning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#feature-representation&quot;&gt;Feature Representation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#tokenization-and-subwords&quot;&gt;Tokenization and Subwords&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#non-linear-activation-functions&quot;&gt;Non-Linear Activation functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#optimizers-and-backpropagation&quot;&gt;Optimizers and Backpropagation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#deep-learning-architectures,-regularization-and-attention&quot;&gt;Deep Learning Architectures, Regularization and Attention&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#feed-forward-neural-networks&quot;&gt;Feed Forward Neural Networks&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#recurrent-neural-networks-(rnn)&quot;&gt;Recurrent Neural Networks (RNN)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#long-short-term-memory-networks-(lstm)&quot;&gt;Long Short Term Memory Networks (LSTM)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#regularization-to-avoid-overfitting&quot;&gt;Regularization to avoid Overfitting&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#attention-mechanisms&quot;&gt;Attention Mechanisms&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#transformers-for-transfer-learning-and-contextual-embeddings&quot;&gt;Transformers for Transfer Learning and Contextual Embeddings&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#self-attention-and-transformer-architecture&quot;&gt;Self Attention and Transformer Architecture&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#limitations-of-transformers&quot;&gt;Limitations of Transformers&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#transformer-based-models&quot;&gt;Transformer Based Models&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#from-deep-learning-and-transformers-to-large-language-models&quot;&gt;From Deep Learning and transformers to Large Language Models&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#why-do-llms-work?&quot;&gt;Why Do LLMs work?&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#scaling-laws&quot;&gt;Scaling Laws&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#emergent-abilities-of-llms&quot;&gt;Emergent Abilities of LLMs&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pre-training-of-llms&quot;&gt;Pre-training of LLMs&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#training---support-for-distributed-training&quot;&gt;Training - support for distributed training&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#adaptation-tuning-for-llms&quot;&gt;Adaptation Tuning for LLMs&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#instruction-tuning&quot;&gt;Instruction Tuning&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#alignment-tuning&quot;&gt;Alignment Tuning&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#model-quantization&quot;&gt;Model Quantization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#evaluation-of-llms&quot;&gt;Evaluation of LLMs&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#final-thoughts&quot;&gt;Final Thoughts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#appendix-a:-word-vectors-and-distribution-hypothesis&quot;&gt;Appendix A: Word Vectors and Distribution Hypothesis&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#embeddings-and-vector-semantics&quot;&gt;Embeddings and Vector Semantics&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#distribution-hypothesis-was-studied-a-lot-in-lexical-semantics&quot;&gt;Distribution Hypothesis was studied a lot in lexical semantics&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#**words-as-vectors:**&quot;&gt;&lt;strong&gt;Words as vectors:&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- vscode-markdown-toc-config
	numbering=false
	autoSave=true
	/vscode-markdown-toc-config --&gt;
&lt;!-- /vscode-markdown-toc --&gt;

&lt;p&gt;I describe the foundational improvements that have been made in every aspect of the deep learning architecture to bring us to where we are today. Although compute and data are 2 major areas which have enabled Deep Learning as it is today, my focus for this doc is on architecture and algorithmic improvements.&lt;/p&gt;

&lt;h2 id=&quot;from-ml-to-deep-learning&quot;&gt;&lt;a name=&quot;from-ml-to-deep-learning&quot;&gt;&lt;/a&gt;From ML to Deep Learning&lt;/h2&gt;

&lt;p&gt;A simplified type of a neural network with a single hidden layer, require the following to train a model: Feature representation, loss function (such as cross entropy loss), classification function (such as sigmoid), optimizers (such as gradient descent). Grounding our understanding in terms of this, we can observe significant improvements in each of these areas, which has enabled deep learning to be as prevalent and effective as it is today. Deep Learning has enabled generalization of knowledge, and the possibility of pre-training on generic data and fine tuning to a specific domain (domain adaptation), without the need for any hand coded feature generation. In the rest of this section I will describe some foundational improvements that Deep Learning has provided based for every aspect of a neural network on the following high level areas, which makes deep learning work so well in practice compared to previous approaches.&lt;/p&gt;

&lt;h3 id=&quot;feature-representation&quot;&gt;&lt;a name=&quot;feature-representation&quot;&gt;&lt;/a&gt;Feature Representation&lt;/h3&gt;

&lt;p&gt;In Machine Learning (ML), a rich feature representation is required as input for an ML model to learn from the data. Previously this was done using feature engineering, and hand-coding of features. However, with deep learning, the input is now either pretrained embeddings or the features/embeddings are learned from running text by the deep learning neural network. More details about the evolution of feature representation and vector semantics is provided in &lt;a href=&quot;#appendix-a-word-vectors-and-distribution-hypothesis&quot;&gt;Appendix A&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In 2003 Bengio at. al. [] introduced a revolutionary new intuition of using running text as supervised training data to predict whether a word “a” is more or less likely to show up near another word “b”. This avoids the needs for any hand-labeled data for supervised classification. The authors applied this approach for language modeling and illustrate how this approach improves perplexity (branching factor) of the model over existing n-gram approaches. In 2013, Word2Vec (Mikolov et al. 2013) was introduced, which used running text as input to a binary classification model to to classify whether a word exists in the neighborhood of another word. Word2vec uses 2 approaches to accomplish this: Skip Gram with Negative Sampling (SGNS), Cumulative bag of words (CBOW). With SGNS, for each target word, we treat the neighboring context words (within a window of words) as positive samples, and then randomly sample words from the rest of the lexicon and use them as negative samples. This is then provided as input to the classifier, which distinguishes between the positive and negative samples. The weights that are learned from the classifier are treated as the embeddings. Word2Vec has some shortcomings, as it can only provide static embeddings, and not different embeddings based on contextual information. This means that the words “bank” would be the same embedding irrespective of whether it is mentioned in the context of a  “river bank” or a “financial institution bank”. Word2Vec also had trouble dealing with unknown words, as it tokenized based on words/phrases. Finally, Word2Vec could not handle word dependencies longer than the window of the surrounding text. In 2014 GloVe (Pennington et. al 2014) built on top of the limitations of Word2Vec, and not only used local context (like word2vec) but also global context to capture the the relationship between two words or phrases. GloVe is better than word2vec at handling rare words, because of global relationships between rare words and common words is captured by the model. Like Word2Vec, GloVe also worked on word or phrases as the smallest tokens, and had trouble working with unseen words. In 2017 Fasttext (Bojanowski et al., 2017)](https://aclanthology.org/Q17-1010/) was introduced which works with subword instead of words or phrases. This means that it would handle rare words much better, as it tokenized the words into their subwords. E.g. “let’s” would be broken down into “let” and “‘s” and their embeddings would be learned independently. In 2018 (Peters et.al. 2018) proposed ELMo that is able to capture the concept of contextual embeddings (which addresses the “bank” problem above. ELMo as built on bi-directional RNN layers, capturing the embeddings of it from the forward and backward pass of the RNN. More recently there are newer representations of embeddings using the transformer architecture (such as BERT, TransformerXL, ChatGPT and others).&lt;/p&gt;

&lt;h4 id=&quot;tokenization-and-subwords&quot;&gt;&lt;a name=&quot;tokenization-and-subwords&quot;&gt;&lt;/a&gt;Tokenization and Subwords&lt;/h4&gt;

&lt;p&gt;Creating embeddings at subword level is able to help us deal with unknown words much better than before. Now, we cam compose the definition of a new word based on it’s subcomponents. In 2015, (Sennrich et al. 2015) explored working with &lt;a href=&quot;https://rutumulkar.com/blog/2021/byte-pair-encoding/&quot;&gt;Byte-Pair Encoding (BPE)&lt;/a&gt; (Gage, 1994) that is originally a compression algorithm to encode running text. This helps capture text better than existing tokenization approaches, but as it works with unicode characters (144,697 unicode characters!), the unicode combinations are sparse. In 2016, Google introduced WordPiece (Wu et. al. 2016) which was the internal tokenizer used by BERT. In 2018 (Kudo and Richardson, 2018) introduced SentencePiece as a much more performant, and principled approach for subword encoding as compared to BPE alone. SentencePiece combines BPE with the Unigram model. SentencePiece was used to train T5 Language Model. In (Bostrom and Durett, 2020)(https://arxiv.org/pdf/2004.03720.pdf)) the authors mention that BPE is not an effective way to train LLMs.&lt;/p&gt;

&lt;h3 id=&quot;non-linear-activation-functions&quot;&gt;&lt;a name=&quot;non-linear-activation-functions&quot;&gt;&lt;/a&gt;Non-Linear Activation functions&lt;/h3&gt;

&lt;p&gt;Activation functions are used in the hidden layers of deep learning architecture, which each individual node takes in input from the previous layer, and performs an operation on them. The goal of activation functions is to be able to capture complex data from the input, without loss of information. Existing activation functions, such as Sigmoid cannot represent more complex data representations, and is heavily prone to gradient saturation for values close to 0 or 1. Explaining this a bit more, we know that:&lt;/p&gt;

&lt;p&gt;Sigmoid Activation is  (y = 1 / (1+e^ (-z))), where z = sum (w_i * x_i) + b.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/sigmoid.jpg&quot; alt=&quot;Sigmoid Activation Function: Image Credit - Wikipedia&quot; width=&quot;400&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 1: Sigmoid Activation Function: Image Credit - Wikipedia&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;This z value is converted into a probability by using an activation function such as sigmoid. The problem with sigmoid is that it squashes outliers towards 0 or 1, making it challenging to capture outlier data, as it causes problems getting derivatives and propagating it back to the first layer using Backprop. This problem is also known as the Vanishing Gradients Problem. To address this, deep learning has explored with non-linear activation functions such as Rectified Linear Units or ReLU (Nair and Hinton, 2010). ReLU has linear behavior for positive values, and zero activation for negative values. Using ReLU transforms the input space (such as XOR) into a linear space in the hidden layers, which can then be classified using a linear approach (Goodfellow 2016, page 169)&lt;/p&gt;

&lt;p&gt;Relu Activation Function is: y = ReLU(z) = max(z,0), where z = sum (w_i * x_i) + b.&lt;/p&gt;

&lt;p&gt;Newer activations functions are now used for LLMs. An image with ReLU, GeLU and others along with more details are covered in the &lt;a href=&quot;#from-deep-learning-and-transformers-to-large-language-models&quot;&gt;next section regarding LLMs&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;optimizers-and-backpropagation&quot;&gt;&lt;a name=&quot;optimizers-and-backpropagation&quot;&gt;&lt;/a&gt;Optimizers and Backpropagation&lt;/h3&gt;

&lt;p&gt;Gradient Descent (GD) (Robbins and Monro 1951) is the algorithm to find the global minima in a gradient for all ML and Deep Learning algorithms.&lt;/p&gt;

&lt;p&gt;In 2010, Stoachastic Gradient Descent (SGD) (Bottou, L. 2010), was introduced, which is what is used in practice (however more recent new approaches have evolved that are effective for training LLMs). GD requires all of the data to be processed once, after which it will update it’s parameters. This works for small amounts of data, but it prohibitive when working with large amounts of data for deep learning. In comparison to SD, SDG only requires a single datapoint (or a batch of datapoints - called mini-batch) to be processed before updating the parameters. SGD typically converges faster and is more memory efficient. SDG and GD both work with a constant learning rate (or step size), irrespective of whether it has encounted the same datapoint more frequently or less frequently. As a result, the learning is often more time consuming, or is stuck at a local minima. To address this Adagrad (Adaptive Gradient Descent) (Duchi 2011) performs gradient descent with varying learning rates for different parameters, increasing the learning rate for rare datapoints to push for faster convergence. Adagrad is able to handle sparse data much better than GD or SGD. Adam optimizer (Kingma and Ba, 2015), builds on Adagrad, and combines adaptive learning rates with moment. Adam maintains different learning rates for different parameters, and combines them with the first moment (mean of the gradients, providing the overall direction of the gradient) and the second moment (variance of the gradient, providing the magnitude of the gradient). This helps with faster convergence and adaptation to varied types of gradients. Other honorary mentions for optimizers are RMSProp, AdaDelta, AdaMax etc. which have their own nuances which need to be evaluated before leveraging for a deep learning problem. Improving the optimizers have enabled much faster convergence of Deep Learning Networks, handling sparse data better, and improved exploration of the landscape to avoid local minima.&lt;/p&gt;

&lt;p&gt;Neural networks can contain a large number of stacked layers, where the output of the final layer needs to be propagated back to the first layer for learning. This is done using the error backpropagation or backprop (Rumelhart et al., 1986), (LeCun et. al. 1998).&lt;/p&gt;

&lt;h3 id=&quot;deep-learning-architectures-regularization-and-attention&quot;&gt;&lt;a name=&quot;deep-learning-architectures,-regularization-and-attention&quot;&gt;&lt;/a&gt;Deep Learning Architectures, Regularization and Attention&lt;/h3&gt;

&lt;h4 id=&quot;feed-forward-neural-networks&quot;&gt;&lt;a name=&quot;feed-forward-neural-networks&quot;&gt;&lt;/a&gt;Feed Forward Neural Networks&lt;/h4&gt;

&lt;p&gt;In 2003, (Bengio et al. 2003) first introduced the simple feed-forward language model primarily for language modeling. While previously, we were using n-gram language models, where ‘n’ usually ranged up to 3, neural language models, neural models are able to generalize over a larger context, and generalize better. The first feedforward neural network contained a single hidden layer, and was able to capture long distance dependencies much betters as compared to the n-gram approach and used running text as input to illustrate self-supervision.&lt;/p&gt;

&lt;h4 id=&quot;recurrent-neural-networks-rnn&quot;&gt;&lt;a name=&quot;recurrent-neural-networks-(rnn)&quot;&gt;&lt;/a&gt;Recurrent Neural Networks (RNN)&lt;/h4&gt;

&lt;p&gt;Recurrent Neural Networks are neural networks that have a cycle within the network, where the hidden layer computed in the previous iteration is leveraged as a form of context or memory for the next iteration.. This is very pertinent to language, which is largely dependent on the previous text/utterance. The first RNN language models were by (Mikolov et al., 2010). RNNs are used for Language Modeling (for machine translation), sequence/text classification and several other downstream tasks.&lt;/p&gt;

&lt;p&gt;RNNs have been used for Machine Translation using en Encoder-Decoder architecture (also knows as seq-2-seq models). Here an encoder takes in an input sentence and converts it into an embedding of some form, which is taken as input by the decoder and converted into text in another language. Encoder Decoder architectures have been widely successful and also have been applied to tasks such as question answering, textual entailment, summarization etc. The intuition behind this is that the output text is a function of the input text (e.g. answer is related to the question being asked), even though the output and input both belong to the same language.&lt;/p&gt;

&lt;p&gt;Although RNNs capture the temporal nature of language and dependence on previous words, it has a limitation of not being able to parallelize the processing, as each token can only be processed after the previous token is processed and the weights from the hidden layers are passed to it. Another limitation of this was the problem of vanishing gradients, meaning that the gradients brought from the hidden layer were subjected to so many multiplications, that they eventually ended up becoming 0. This lead to the problem of long distance dependencies not accurately captured.&lt;/p&gt;

&lt;h4 id=&quot;long-short-term-memory-networks-lstm&quot;&gt;&lt;a name=&quot;long-short-term-memory-networks-(lstm)&quot;&gt;&lt;/a&gt;Long Short Term Memory Networks (LSTM)&lt;/h4&gt;

&lt;p&gt;LSTMs (Hochreiter and Schmidhuber, 1997) started being used to mitigate the issues introduced by RNNs, in particular regarding RNNs not being able to address or make use of long distant information. LSTMs introduced gates that selectively passed information from the input layer and also the hidden layer from the previous node.&lt;/p&gt;

&lt;h4 id=&quot;regularization-to-avoid-overfitting&quot;&gt;&lt;a name=&quot;regularization-to-avoid-overfitting&quot;&gt;&lt;/a&gt;Regularization to avoid Overfitting&lt;/h4&gt;

&lt;p&gt;In order to avoid overfitting, various forms of regularization are used. One of the most important ones is called - dropout. Dropout is when we randomly drop some units and their connections from the network during training (Hinton et al. 2012), (Srivastava et al. 2014). Hyperparameter tuning is another important requirement to avoid overfitting or being stuck at a local minima.&lt;/p&gt;

&lt;h4 id=&quot;attention-mechanisms&quot;&gt;&lt;a name=&quot;attention-mechanisms&quot;&gt;&lt;/a&gt;Attention Mechanisms&lt;/h4&gt;

&lt;p&gt;In 2014 (Bahdanau 2014) introduced the concept of Attention in Deep Neural Networks, which addressed the botteneck issue that was introduced by RNNs (where the data at n-1 needed to be processed before data at n so that data n could use the hidden layer as input from data n-1). Using the attention mechanism, data n could get hidden states from all of the previous data points, and not just data n-1. This was explained from the context of a machine translation task where the encoder created hidden layers for all of the items in the input, and all these hidden layers were provided to the decoder in the form of a context vector, which is a function of all the hidden layers.&lt;/p&gt;

&lt;p&gt;There are different types of attention mechanisms. Dot-product attention is one such approach, where all the hidden layers from the past few contexts are combined in the form of a dot product, and taken to the decoding layer of an RNN.&lt;/p&gt;

&lt;h3 id=&quot;transformers-for-transfer-learning-and-contextual-embeddings&quot;&gt;&lt;a name=&quot;transformers-for-transfer-learning-and-contextual-embeddings&quot;&gt;&lt;/a&gt;Transformers for Transfer Learning and Contextual Embeddings&lt;/h3&gt;

&lt;h4 id=&quot;self-attention-and-transformer-architecture&quot;&gt;&lt;a name=&quot;self-attention-and-transformer-architecture&quot;&gt;&lt;/a&gt;Self Attention and Transformer Architecture&lt;/h4&gt;

&lt;p&gt;In 2017, (Vaswani et al., 2017), proposed the original transformer architecture which was based on two lines of prior research: self-attention (Bahdanau 2014) and memory networks (Sukhbaatar et al., 2015). Transformer is based on the concept of attention, and it replaces RNNs and the bottlenecks introduced by it. In the original Transformer paper, it consists of an encoder and a decoder. In the encoder, Transformers use fully connected feed forward neural networks where each input token is connected with all the past tokens in a step called Self-Attention. Using this, and extending it to multi-head attention where each token is performing self-attention in parallel, the encoder is able to capture the dependency relationships between each token in the input. Transformers also add positional encoding (one hot encoding) of the position of the token in the sequence to capture word order, thereby replacing the need for RNN like architecture to encode the sequential dependency of a word/token on previous tokens. The encoder and decoder both contain similar stacked layers of self attention and fully-connected networks. Transformers allow parallel computation (which RNN’s could not). Transformers also introduce Layer Normalization (scaling the dot products after each layer), which is able to address the vanishing gradients problem that RNNs, LSTMs and other approaches could not address. Today, Transformers are the cornerstone all language models for autoregressive generation (gen AI). There are several improvements that have been made to vanilla transformers to train LLMs as they are today.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/transformer.jpg&quot; alt=&quot;&quot; width=&quot;400&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 2: Vanilla Transformer Architecture&lt;/em&gt;
&lt;/p&gt;

&lt;h4 id=&quot;limitations-of-transformers&quot;&gt;&lt;a name=&quot;limitations-of-transformers&quot;&gt;&lt;/a&gt;Limitations of Transformers&lt;/h4&gt;

&lt;p&gt;Attention is quadratic in nature (because at token ‘n’, we are computing context for ‘n’ and all the previous ‘n-1’ tokens. As a result, Transformer architectures have not able to address very long documents. Some approaches that have been introduced to address this approaches like Longformer (Beltagy et. al 2020), where the attention mechanism scales linearly with sequence length. This enables processing much longer texts as compared to the vanilla transformer approach. More recently newer attention mechanisms have been introduced to address the quadratic nature of attention. Details are in the &lt;a href=&quot;#pre-training-of-llms&quot;&gt;Pre-Training section of LLMs&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;transformer-based-models&quot;&gt;&lt;a name=&quot;transformer-based-models&quot;&gt;&lt;/a&gt;Transformer Based Models&lt;/h4&gt;

&lt;p&gt;In 2019, BERT (Devlin et. al. 2019) was introduced which has two objective functions: Masked LM, and NSP (Next Sentence Prediction), so that it would learn bidirectional information from within a sentence, and learn about dependencies between 2 sentences.  It was one of the first initiatives that showed contextual embeddings. Also, honorable mention to ELMo (Peters et. al 2018) which was the first initiative for contextual embeddings, but it did not use the transformer architecture. Other transformer inspired early approaches are:  RoBERTa (Liu et al. 2019), Distilbert (Sanh et. al. 2019), TransformerXL (Dai et. al. 2019), T5 (Raffel 2019).
Deep Learning is able to generalize for unseen data much better than their Machine Learning counterparts. (Erhan et al. 2010) discuss the important of pretraining for deep learning tasks.&lt;/p&gt;

&lt;h2 id=&quot;from-deep-learning-and-transformers-to-large-language-models&quot;&gt;&lt;a name=&quot;from-deep-learning-and-transformers-to-large-language-models&quot;&gt;&lt;/a&gt;From Deep Learning and transformers to Large Language Models&lt;/h2&gt;

&lt;p&gt;More recently researchers have observed that model scaling can lead to an improved model capacity (ability to represent complex patterns that is it trained on) and significant improvement in performance in downstream tasks. It is also discovered that this new era of Large Language Models (that have 10B parameters or more) exhibit some emergent capabilities (such as in context learning), that have not been present in small scale language models (such as BERT, DistilBERT etc - which have Millions of Parameters only).&lt;/p&gt;

&lt;p&gt;In 2018, GPT-1 (Radford et. al. 2018) which stands for &lt;em&gt;Generative Pre-Training&lt;/em&gt; was developed using a generative decoder only Transformer Architecture. GPT-1 adopted an approach of pre-training followed by supervised fine-tuning. GPT-1 is a 117MM parameter model.&lt;/p&gt;

&lt;p&gt;Later in 2019, GPT-2 (Radford et. al. 2019), followed a similar approach as GPT-1, but increased the number of paramerters to 1.5B. It aimed to perform tasks without explicit fine tuning using labeled data. To enable this, the authors introduced a probabilistic approach for multi-task solving &lt;em&gt;p(output|input, task),&lt;/em&gt; where the output is conditioned not only on the input, but also the task.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;&gt;GPT-3&lt;/a&gt; (Brown et. al. 2020) was released in 2020, and it scaled the number of model parameters to 175B. The authors introduce the concept of in-context learning, which uses LLMs in a few-shot or zero shot way. This means that the pre-training and prompting (with in-context information) will help the model converge to an answer within the context of the information provided.&lt;/p&gt;

&lt;p&gt;GPT-4 (OpenAI, 2023) was released in 2023 (March) and it extended text input to multimodal signals. GPT-4 shows strong capabilities in solving complex problems as compared to previous models. A recent study by (Bubeck et. al 2023) showed that GPT-4 can perform better at a variety of tasks of different domains (such as mathematics, coding, vision, medicine, law, psychology and more), and performs very similar to human results. The paper shares that this is the beginning of AGI (Artificial General Intelligence)&lt;/p&gt;

&lt;p&gt;Huggingface recently released the &lt;a href=&quot;https://huggingface.co/bigscience/bloom&quot;&gt;Bloom model&lt;/a&gt; (HuggingFace 2022) which has multilingual support for 46 natural languages and 13 programming languages, and Meta has released LLaMA (Touvron et. al. 2023) has 65B parameters . All of these are generative language models and they have moved us several leaps into NLP tasks such as : question answering, multi-task learning (Radford et. al. 2019), and others, and also have shown emergent abilities that can be observed using prompting. These generative models are also evaluated using prompting, which has lead to a whole new way of debugging language models, and learning about what these LMs know and how they can be leveraged in other areas.&lt;/p&gt;

&lt;p&gt;(Chen et al, 2023) provide a detailed survey of LLMs and what it has taken for us to get this far with them.&lt;/p&gt;

&lt;h3 id=&quot;why-do-llms-work&quot;&gt;&lt;a name=&quot;why-do-llms-work?&quot;&gt;&lt;/a&gt;Why Do LLMs work?&lt;/h3&gt;

&lt;h4 id=&quot;scaling-laws&quot;&gt;&lt;a name=&quot;scaling-laws&quot;&gt;&lt;/a&gt;Scaling Laws&lt;/h4&gt;

&lt;p&gt;KM scaling law (Kaplan et. al. 2020) by open AI proposed a power law relationship of model performance with respect to three major factors - Model Size, Datset Size and amount of training compute.&lt;/p&gt;

&lt;p&gt;The Google DeepMind team (Hoffmann et. al. 2022) proposed another study (Chinchilla Scaling Law) which is an alternative form of scaling for training LLMs.&lt;/p&gt;

&lt;p&gt;Below is an image of how the number of parameter of a model have expanded over the years.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/llms.jpg&quot; alt=&quot;&quot; width=&quot;800&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 4: Image Credit: Stanford LLM Course&lt;/em&gt;
&lt;/p&gt;

&lt;h3 id=&quot;emergent-abilities-of-llms&quot;&gt;&lt;a name=&quot;emergent-abilities-of-llms&quot;&gt;&lt;/a&gt;Emergent Abilities of LLMs&lt;/h3&gt;

&lt;p&gt;LLMs introduced a new set of abilities that were not previously present in the smaller models (such as BERT), but were introduced when models were scaled to a much larger size. Some of these abilities include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In-context learning: This learning ability was formally introduced by GPT-3 (Brown et. al. 2020) where given that a model is provided with some task demonstrations, it can generate the output text by completing the word sequence of the input text.&lt;/li&gt;
  &lt;li&gt;Instruction Following: After fine tuning on instruction datasets, LLMs are able to follow and execute tasks for new datasets.&lt;/li&gt;
  &lt;li&gt;Step-by-Step Reasoning: Using chain-of-thought (CoT)(Chung et. al. 2022) prompting approaches, LLMs are able to solve step by step reasoning steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pre-training-of-llms&quot;&gt;&lt;a name=&quot;pre-training-of-llms&quot;&gt;&lt;/a&gt;Pre-training of LLMs&lt;/h3&gt;

&lt;p&gt;LLMs rely on a massively large corpus of data for training. Most LLMs such as GPT-2 (Radford et. al. 2019) and PaLM (Chowdhery et. al 2022) are trained on generic datasets that are a collection of books, webpages and conversational text, as this generic data and introduce general purpose capabilities to the language model. CommonCrawl is one of the biggest sources of web data, and Reddit corpora is one of the biggest sources of conversational text. Several LLMs such as PaLM (Chowdhery et. al 2022) and Bloom (Huggingface 2022) also use specialized text data for training. This includes data like multilingual text, scientific text (research papers) and code (from stack exchange or Github). In order to train with this data, several preprocessing steps are performed to remove redundant, private data, irrelevant data or toxic data from the text. The text is encoded into subword tokenizers (some of which was discussed in the &lt;a href=&quot;#tokenization-and-subwords&quot;&gt;Subwords Section&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Once the data requirements are established (all LLMs need to have large amounts of data and high quality data for optimal performance), the data is passed through one of the many architectures such as encoder-decoder architecture (used by BART, T5, causal decoder architecture (for next word prediction and used by GPTs, BLOOM), or the more recent Prefix Decoder (used by PaLM) Architecture. There has been a lot of research around the best location of layer normalization, and pre, post or sandwich layer norms are some different approaches used with the architecture.&lt;/p&gt;

&lt;p&gt;Activation functions used by LLMs are different from Deep Learning activations that I discussed in the previous section. These are largely GeLU (Gaussian Error Linear Unit) (Hendrycks 2016) or variants of GLU activation (Shazeer 2020) such as SwiGLU (Shazeer 2020) and GeGLU (Shazeer 2020). Below is an image of GELU activation function as compared to RELU and ELU (Clevert et. al. 2016).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/activations.jpg&quot; alt=&quot;&quot; width=&quot;400&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 5: Image Credit: Papers with Code&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;Position embedding (as absolute positional encoding) as presented in the vanilla Transformer architecture has several new variations proposed, such as relative positional embedding, Rotary position embedding, and AliBi (Press et. al. 2022)&lt;/p&gt;

&lt;p&gt;Several different types of attention mechanisms can be used for LLMs today. For instance, sparse attention approaches are used to address the quadratic computational complexity by the vanilla transformer. GPT-3 uses factoid attention (Child et. al. 2019) where instead of full attention, each query can only be attended to by a subset of tokens based on positions. 
Another type of Attention is multiquery attention (Shazeer 2019) where the same linear transformation matrices are shared by different heads. Another approach is FlashAttention (Dao et. al, 2022) which proposes optimizations based on memory consumption of memory modules on GPU.&lt;/p&gt;

&lt;p&gt;There are 2 common objective functions used - language modeling (predict the next word given the previous word), or denoising autoencoding. During training, GPT-3 (Brown et. al. 2020) and PaLM (Chowdhery et. al 2022) have introduced a new strategy that dynamically increases the batch size during training. New LLMs also adopt a similar strategy with learning rates, where the learning rate is gradually increased to the maximum value, followed by a decay strategy. The optimizers used are ADAM (Kingma and Ba 2015), or variations of it such as ADAMw (Loshchilov and Hutter 2017).&lt;/p&gt;

&lt;h4 id=&quot;training---support-for-distributed-training&quot;&gt;&lt;a name=&quot;training---support-for-distributed-training&quot;&gt;&lt;/a&gt;Training - support for distributed training&lt;/h4&gt;

&lt;p&gt;The training for LLMs is optimized using three commonly used parallel training techniques - data parallism where the training corpus is distributed across GPUs, pipeline parallelism (Huang et. al, 2019) where multiple layers of the transformers are distributed across GPUs and tensor parallelism (where the tensor is decomposed and parallelized).&lt;/p&gt;

&lt;h3 id=&quot;adaptation-tuning-for-llms&quot;&gt;&lt;a name=&quot;adaptation-tuning-for-llms&quot;&gt;&lt;/a&gt;Adaptation Tuning for LLMs&lt;/h3&gt;

&lt;p&gt;Pretraining helps language models acquire general abilities for solving tasks. Most LLMs go through the next step of adaptation. There are 2 types of adaptation: instruction tuning (where abilities of LLMs are converted from language based to task based) and alignment tuning (where the output of the LMM is aligned with human preferences and values).&lt;/p&gt;

&lt;h4 id=&quot;instruction-tuning&quot;&gt;&lt;a name=&quot;instruction-tuning&quot;&gt;&lt;/a&gt;Instruction Tuning&lt;/h4&gt;

&lt;p&gt;In a recent paper, (Chen et. al, 2023) describe several datasets that are available for instruction tuning, and how instruction tuning has helped generalize to unseen tasks. In 2022, (Chung et. al. 2022) first experimented with Chain-of-Thought (CoT) prompting to show commonsense reasoning ability and mathematical reasoning ability.&lt;/p&gt;

&lt;h4 id=&quot;alignment-tuning&quot;&gt;&lt;a name=&quot;alignment-tuning&quot;&gt;&lt;/a&gt;Alignment Tuning&lt;/h4&gt;

&lt;p&gt;Regarding the second type of adaptation - alignment tuning - Open AI recently came out with their paper on InstructGPT (Ouyang et. al 2022) that helps further train models to align their output with human output. 
To align LLMs with human values (such as don’t use profanity, be polite, etc.) , Reinforcement Learning from Human Feedback (RLHF) (Christiano et. al. 2017) was proposed which leverages algorithms like Proximal Policy Evaluation (Schulman et. al. 2017) to adapt LLMs to human feedback. An RLHF system has 3 components - the LLM to be tuned, a reward model, an RL approach for training. In order to efficiently fine tune using RLHF, several efficient parameter tuning approaches are proposed such as Adapter Tuning (Houlsby 2019), Prompt Tuning (Lester et al., 2021), LoRA (Low-Rank Adaptation) etc. LoRA has been widely applied to open source LLMs (such as LLaMA and BLOOM)&lt;/p&gt;

&lt;h3 id=&quot;model-quantization&quot;&gt;&lt;a name=&quot;model-quantization&quot;&gt;&lt;/a&gt;Model Quantization&lt;/h3&gt;

&lt;p&gt;LLMs are challenging to deploy in the real world because of their prohibitive memory footprint. Model Quantization is the approach that is used for reducing the memory footprint of LLMs using popular model compression approaches. There are two quantization approaches - Quantization-aware training (QAT) (this requires a full model retraining) and post-training quantization (PTQ) (this requires no model retraining). Some PTQ approaches include Mixed Precision decomposition (Dettmers et. al. 2022), Per-Tensor Quantization (Xiao et. al. 2023), etc.&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;&lt;a name=&quot;final-thoughts&quot;&gt;&lt;/a&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;We have made tremendous progress in Deep Learning and NLP in the past few years, and although I have tried to cover a lot of the seminal work here, I wanted to emphasize that this is still a small drop in the massive amounts of work and research that has gone into this space. Some more concepts that are interesting and will be covered in future blog posts are - Knowledge Distillation, Quantization, Chain-of-Thought Prompting, in-context learning, Planning and Commonsense Reasoning using LLMs, Prompt Engineering, which hold promise for even bigger leaps into deep learning in the future.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;&lt;a name=&quot;references&quot;&gt;&lt;/a&gt;References&lt;/h2&gt;

&lt;p&gt;[1] (Bahdanau et. al 2015) Bahdanau, D., K. H. Cho, and Y. Bengio. 2015. &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Neural machine translation by jointly learning to align and translate&lt;/a&gt;. ICLR 2015.&lt;/p&gt;

&lt;p&gt;[2] (Beltagy et. al 2020) Iz Beltagy, Matthew E. Peters, Arman Cohan, Longformer: &lt;a href=&quot;https://arxiv.org/abs/2004.05150&quot;&gt;The Long-Document Transformer&lt;/a&gt;, 2004, arXiv&lt;/p&gt;

&lt;p&gt;[3] (Bengio et. al 2003) Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. “&lt;a href=&quot;https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&quot;&gt;A Neural Probabilistic Language Model.&lt;/a&gt;” Journal of Machine Learning Research, vol. 3, 2003, pp. 1137-1155.&lt;/p&gt;

&lt;p&gt;[4] (Bengio et. al. 2007) Bengio, Y., P. Lamblin, D. Popovici, and H. Larochelle. 2007. &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf&quot;&gt;Greedy layer-wise training of deep networks&lt;/a&gt;. NeurIPS.&lt;/p&gt;

&lt;p&gt;[5] (Bottou 2010) Bottou, L. (2010). &lt;a href=&quot;https://leon.bottou.org/publications/pdf/compstat-2010.pdf&quot;&gt;Large-scale machine learning with stochastic gradient descent&lt;/a&gt;. In Proceedings of COMPSTAT’2010 (pp. 177-186). Springer.&lt;/p&gt;

&lt;p&gt;[6] (Brown et. al. 2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. &lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;&gt;Language Models are Few-Shot Learners&lt;/a&gt;. NeurIPS 2020.&lt;/p&gt;

&lt;p&gt;[7] (Bubeck et. al 2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, “&lt;a href=&quot;https://arxiv.org/pdf/2303.12712.pdf&quot;&gt;Sparks of Artificial General Intelligence: Early experiments with GPT-4&lt;/a&gt;”, 2023&lt;/p&gt;

&lt;p&gt;[8] (Chen et al, 2023) Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen, “&lt;a href=&quot;https://arxiv.org/pdf/2303.18223.pdf&quot;&gt;A Survey of Large Language Models&lt;/a&gt;”, 2023, arXiv&lt;/p&gt;

&lt;p&gt;[9] (Child et. al. 2019) Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever, &lt;a href=&quot;https://arxiv.org/pdf/1904.10509.pdf&quot;&gt;Generating Long Sequences with Sparse Transformers&lt;/a&gt;, 2019, CoRR, abs/1904.10509&lt;/p&gt;

&lt;p&gt;[10] (Chowdhery et. al 2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N.M., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.C., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., García, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Díaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K.S., Eck, D., Dean, J., Petrov, S., &amp;amp; Fiedel, N. (2022). &lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM: Scaling Language Modeling with Pathways&lt;/a&gt;. &lt;em&gt;ArXiv, abs/2204.02311&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;[11] (Chung et. al. 2022) Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei, &lt;a href=&quot;https://arxiv.org/pdf/2210.11416.pdf&quot;&gt;Scaling Instruction-Finetuned Language Models&lt;/a&gt;, 2022, arXiv&lt;/p&gt;

&lt;p&gt;[12] (Christiano et. al. 2017) Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei (2017). &lt;a href=&quot;https://arxiv.org/pdf/1706.03741.pdf&quot;&gt;Deep reinforcement learning from human preferences&lt;/a&gt;. Advances in Neural Information Processing Systems 30 (NIPS 2017)&lt;/p&gt;

&lt;p&gt;[13] (Clevert et. al. 2016) &lt;a href=&quot;https://arxiv.org/pdf/1511.07289.pdf&quot;&gt;Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)&lt;/a&gt;, Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter, 2016&lt;/p&gt;

&lt;p&gt;[14] (Dai et. al. 2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. “&lt;a href=&quot;https://arxiv.org/pdf/1901.02860.pdf&quot;&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt;.” Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2978-2988.&lt;/p&gt;

&lt;p&gt;[15] (Dao et. al, 2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré, &lt;a href=&quot;https://arxiv.org/pdf/2205.14135.pdf&quot;&gt;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[16] (Dettmers et. al. 2022) Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer, &lt;a href=&quot;https://arxiv.org/pdf/2208.07339.pdf&quot;&gt;LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale&lt;/a&gt;, 2022, 2208.07339, arXiv&lt;/p&gt;

&lt;p&gt;[17] (Devlin et. al. 2019) Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019. &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;BERT: Pre-training of deep bidirectional transformers for language understanding&lt;/a&gt;. NAACL HLT.&lt;/p&gt;

&lt;p&gt;[18] (Duchi 2011) Duchi, J., Hazan, E., &amp;amp; Singer, Y. (2011). &lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;&gt;Adaptive subgradient methods for online learning and stochastic optimization&lt;/a&gt;. Journal of Machine Learning Research, 12(7), 2121-2159.&lt;/p&gt;

&lt;p&gt;[19] (Erhan et. al. 2010) D. Erhan, A. Courville, Y. Bengio, P. Vincent, “&lt;a href=&quot;https://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf&quot;&gt;Why Does Unsupervised Pre-training Help Deep Learning?&lt;/a&gt;”, &lt;em&gt;Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, PMLR 9:201-208, 2010.&lt;/p&gt;

&lt;p&gt;[20] (Gage 1994) Philip Gage. 1994. &lt;a href=&quot;https://www.derczynski.com/papers/archive/BPE_Gage.pdf&quot;&gt;A New Algorithm for Data Compression&lt;/a&gt;. C Users J., 12(2):23–38, February.&lt;/p&gt;

&lt;p&gt;[21] (Goodfellow et. al. 2016) Goodfellow, I., Y. Bengio, and A. Courville. 2016.&lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;Deep Learning&lt;/a&gt;. MIT Press.&lt;/p&gt;

&lt;p&gt;[22] (Hendrycks 2016) Dan Hendrycks, Kevin Gimpel, &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;&gt;Gaussian Error Linear Units (GELUs)&lt;/a&gt;, 2016, arxiv&lt;/p&gt;

&lt;p&gt;[23] (Hinton et. al. 2006) Hinton, G. E., S. Osindero, and Y.-W. Teh. 2006. &lt;a href=&quot;https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf&quot;&gt;A fast learning algorithm for deep belief nets&lt;/a&gt;. Neural computation, 18(7):1527–1554.&lt;/p&gt;

&lt;p&gt;[24] (Hinton et. al. 2012) G. E. Hinton , N. Srivastava, A. Krizhevsky, I. Sutskever and R. R. Salakhutdinov, &lt;a href=&quot;https://arxiv.org/pdf/1207.0580.pdf&quot;&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt;, CoRR, 2012&lt;/p&gt;

&lt;p&gt;[25] (Hoffmann et. al. 2022) Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre, &lt;a href=&quot;https://arxiv.org/pdf/2203.15556.pdf&quot;&gt;Training Compute-Optimal Large Language Models&lt;/a&gt;, 2022, 2203.15556, arXiv&lt;/p&gt;

&lt;p&gt;[26] (Houlsby 2019) Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly, &lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;&gt;Parameter-Efficient Transfer Learning for NLP&lt;/a&gt;, 2019, CoRR, abs/1902.00751&lt;/p&gt;

&lt;p&gt;[27] (Hochreiter and Schmidhuber, 1997) Hochreiter, S., &amp;amp; Schmidhuber, J. (1997). &lt;a href=&quot;https://www.bioinf.jku.at/publications/older/2604.pdf&quot;&gt;Long short-term memory&lt;/a&gt;. Neural computation, 9(8), 1735-1780.&lt;/p&gt;

&lt;p&gt;[28] (Huang et. al, 2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen, &lt;a href=&quot;https://arxiv.org/pdf/1811.06965.pdf&quot;&gt;GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism&lt;/a&gt;, 2019, arXiv:1811.06965v5&lt;/p&gt;

&lt;p&gt;[29] (Huggingface 2022) Huggingface, &lt;a href=&quot;https://arxiv.org/pdf/2211.05100.pdf&quot;&gt;BLOOM: A 176B-Parameter Open-Access Multilingual Language Model&lt;/a&gt;, 2022 https://arxiv.org/abs/2211.05100&lt;/p&gt;

&lt;p&gt;[30] (Kaplan et. al. 2020) Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess an Rewon Child an Scott Gray and Alec Radford an Jeffrey Wu an Dario Amodei, &lt;a href=&quot;https://arxiv.org/pdf/2001.08361.pdf&quot;&gt;Scaling Laws for Neural Language Models&lt;/a&gt;, CoRR, abs/2001.08361, 2020&lt;/p&gt;

&lt;p&gt;[31] (Kingma and Ba 2015) Kingma, D. and J. Ba. 2015. &lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;&gt;Adam: A method for stochastic optimization&lt;/a&gt;. ICLR 2015&lt;/p&gt;

&lt;p&gt;[32] (Kudo and Richardson, 2018) Kudo, T., &amp;amp; Richardson, J. (2018). &lt;a href=&quot;https://arxiv.org/pdf/1808.06226.pdf&quot;&gt;SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing&lt;/a&gt;. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Brussels, Belgium.&lt;/p&gt;

&lt;p&gt;[33] (LeCun et. al. 1998) LeCun, Y., Bottou, L., Orr, G., &amp;amp; Müller, K. (1998). &lt;a href=&quot;https://cseweb.ucsd.edu/classes/wi08/cse253/Handouts/lecun-98b.pdf&quot;&gt;Efficient backprop. In Neural Networks: Tricks of the Trade&lt;/a&gt; (pp. 9-48). Springer.&lt;/p&gt;

&lt;p&gt;[34] (Lester et al., 2021) Lester, Brian  and Al-Rfou, Rami  and Constant, Noah, &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.243.pdf&quot;&gt;The Power of Scale for Parameter-Efficient Prompt Tuning&lt;/a&gt;, (https://aclanthology.org/2021.emnlp-main.243)&lt;/p&gt;

&lt;p&gt;[35] (Levy and Goldberg 2014) Omer Levy, Yoav Goldberg, &lt;a href=&quot;https://papers.nips.cc/paper_files/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf&quot;&gt;Neural word embedding as implicit matrix factorization&lt;/a&gt;, NIPS’14: Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2 December 2014 Pages 2177–2185&lt;/p&gt;

&lt;p&gt;[36] (Liu et. al. 2019) Liu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. 2019. &lt;a href=&quot;https://arxiv.org/pdf/1907.11692.pdf&quot;&gt;RoBERTa: A robustly optimized BERT pretraining approach&lt;/a&gt;. ArXiv preprint arXiv:1907.11692.&lt;/p&gt;

&lt;p&gt;[37] (Loshchilov and Hutter 2017) &lt;a href=&quot;https://arxiv.org/pdf/1711.05101.pdf&quot;&gt;Fixing Weight Decay Regularization in Adam&lt;/a&gt;. &lt;em&gt;I. Loshchilov, F. Hutter&lt;/em&gt;. 2017. Introduces &lt;strong&gt;AdamW&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;[38] (Mikolov 2013) Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). &lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;. arXiv preprint arXiv:1301.3781.&lt;/p&gt;

&lt;p&gt;[39] (Mikolov et. al. 2010) Mikolov, T., M. Karafiat, L. Burget, J.  Cernock, and S. Khudanpur. 2010. &lt;a href=&quot;https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf&quot;&gt;Recurrent neural network based language model&lt;/a&gt;. INTERSPEECH.&lt;/p&gt;

&lt;p&gt;[40] (Nair and Hinton 2010) V. Nair and G. E. Hinton, “&lt;a href=&quot;https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf&quot;&gt;Rectified linear units improve restricted boltzmann machines&lt;/a&gt;,” in ICML, 2010, pp. 807–814.&lt;/p&gt;

&lt;p&gt;[41] (OpenAI, 2023) &lt;a href=&quot;https://arxiv.org/pdf/2303.08774.pdf&quot;&gt;GPT-4 Technical Report&lt;/a&gt;, 2023, OpenAI&lt;/p&gt;

&lt;p&gt;[42] (Ouyang et. al 2022) Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe, &lt;a href=&quot;https://arxiv.org/pdf/2203.02155.pdf&quot;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;, 2022, arXiv, 2203.02155&lt;/p&gt;

&lt;p&gt;[43] (Pennington et. al, 2014) Pennington, J., Socher, R., &amp;amp; Manning, C. D. (2014). &lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;&gt;GloVe: Global vectors for word representation&lt;/a&gt;. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532-1543).&lt;/p&gt;

&lt;p&gt;[44] (Peters et. al. 2018) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp;amp; Zettlemoyer, L. (2018). &lt;a href=&quot;https://arxiv.org/pdf/1802.05365.pdf&quot;&gt;Deep contextualized word representations&lt;/a&gt;. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT) (pp. 2227-2237).&lt;/p&gt;

&lt;p&gt;[45] (Press et. al. 2022) Press, Ofir and Smith, Noah A and Lewis, Mike, &lt;a href=&quot;https://arxiv.org/pdf/2108.12409.pdf&quot;&gt;Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation&lt;/a&gt;, arXiv preprint arXiv:2108.12409, 2022&lt;/p&gt;

&lt;p&gt;[46] (Radford et. al. 2018) Radford, A., Narasimhan, K., Salimans, T., &amp;amp; Sutskever, I. (2018). &lt;a href=&quot;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&quot;&gt;Improving Language Understanding by Generative Pre-training.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[47] (Radford et. al. 2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. &lt;a href=&quot;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;Language models are unsupervised multitask learners&lt;/a&gt;. OpenAI Blog, 1(8), 2019.&lt;/p&gt;

&lt;p&gt;[48] (Raffel et. al. 2019) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., … &amp;amp; Liu, P. J. (2019). &lt;a href=&quot;https://arxiv.org/pdf/1910.10683.pdf&quot;&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/a&gt;. arXiv preprint arXiv:1910.10683.&lt;/p&gt;

&lt;p&gt;[49] (Robbins and Monro 1951) Robbins, H., &amp;amp; Monro, S. (1951).&lt;a href=&quot;http://www.columbia.edu/~ww2040/8100F16/RM51.pdf&quot;&gt;A stochastic approximation method&lt;/a&gt;. The Annals of Mathematical Statistics, 22(3), 400-407.&lt;/p&gt;

&lt;p&gt;[50] (Rumelhart 1985) D. Rumelhart, G. Hinton, and R. Williams, “&lt;a href=&quot;https://cs.uwaterloo.ca/~y328yu/classics/bp.pdf&quot;&gt;Learning internal representations by error propagation&lt;/a&gt;,” UCSD, Tech. Rep., 1985.&lt;/p&gt;

&lt;p&gt;[51] (Sanh et. al. 2019) Sanh, V., Debut, L., Chaumond, J., &amp;amp; Wolf, T. (2019). &lt;a href=&quot;https://arxiv.org/pdf/1910.01108.pdf&quot;&gt;DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter&lt;/a&gt;. arXiv preprint arXiv:1910.01108.&lt;/p&gt;

&lt;p&gt;[52] (Sennrich et. al 2016) Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). &lt;a href=&quot;https://aclanthology.org/P16-1162.pdf&quot;&gt;Neural machine translation of rare words with subword units&lt;/a&gt;. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), Berlin, Germany.&lt;/p&gt;

&lt;p&gt;[53] (Shazeer 2019) Noam Shazeer, &lt;a href=&quot;https://arxiv.org/pdf/1911.02150.pdf&quot;&gt;Fast Transformer Decoding: One Write-Head is All You Need,&lt;/a&gt;
 abs/1911.02150, 2019&lt;/p&gt;

&lt;p&gt;[54] (Shazeer 2020) Noam Shazeer, &lt;a href=&quot;https://arxiv.org/pdf/2002.05202.pdf&quot;&gt;GLU Variants Improve Transformer&lt;/a&gt;, 2020, CoRR, abs/2002.05202&lt;/p&gt;

&lt;p&gt;[55] (Schulman et. al. 2017) John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov, &lt;a href=&quot;https://arxiv.org/pdf/1707.06347.pdf&quot;&gt;Proximal Policy Optimization Algorithms&lt;/a&gt;, 2017, CoRR, abs/1707.06347&lt;/p&gt;

&lt;p&gt;[56] (Srivastava et. al. 2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov; ‘&lt;a href=&quot;https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf&quot;&gt;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&lt;/a&gt;’ JMLR 15(56):1929−1958, 2014.&lt;/p&gt;

&lt;p&gt;[57] (Sukhbaatar et. al. 2015) Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, ‘&lt;a href=&quot;https://arxiv.org/pdf/1503.08895.pdf&quot;&gt;End-To-End Memory Networks&lt;/a&gt;’, NeurIPS 2015&lt;/p&gt;

&lt;p&gt;[58] (Touvron et. al. 2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample, &lt;a href=&quot;https://arxiv.org/pdf/2302.13971.pdf&quot;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;, 2023, arxiv&lt;/p&gt;

&lt;p&gt;[59] (Vaswani 2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Attention is all you need&lt;/a&gt;,” in NIPS, 2017, pp. 6000–6010.&lt;/p&gt;

&lt;p&gt;[60] (Pennington et al 2014) Pennington, J., R. Socher, and C. D. Manning. 2014. &lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;&gt;GloVe: Global vectors for word representation&lt;/a&gt;. EMNLP.&lt;/p&gt;

&lt;p&gt;[61] (Wu et. al. 2016) &lt;em&gt;Yonghui Wu, M. Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, Yuan Cao, Qin Gao, Klaus Macherey, J. Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Y. Kato, Taku Kudo, H. Kazawa, K. Stevens, George Kurian, Nishant Patil, W. Wang, C. Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, G. Corrado, Macduff Hughes, J. Dean,&lt;/em&gt; &lt;a href=&quot;https://arxiv.org/pdf/1609.08144.pdf&quot;&gt;Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation&lt;/a&gt;. . 2016. Introduces &lt;strong&gt;WordPiece&lt;/strong&gt;. Used by BERT.&lt;/p&gt;

&lt;p&gt;[62] (Xiao et. al. 2023) Guangxuan Xiao and Ji Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han, &lt;a href=&quot;https://arxiv.org/pdf/2211.10438.pdf&quot;&gt;SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models&lt;/a&gt;, 2023, 2211.10438, arXiv&lt;/p&gt;

&lt;h2 id=&quot;appendix-a-word-vectors-and-distribution-hypothesis&quot;&gt;&lt;a name=&quot;appendix-a:-word-vectors-and-distribution-hypothesis&quot;&gt;&lt;/a&gt;Appendix A: Word Vectors and Distribution Hypothesis&lt;/h2&gt;

&lt;h3 id=&quot;embeddings-and-vector-semantics&quot;&gt;&lt;a name=&quot;embeddings-and-vector-semantics&quot;&gt;&lt;/a&gt;Embeddings and Vector Semantics&lt;/h3&gt;

&lt;p&gt;Distributional hypothesis first formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth (1957), 
“Words that occur in similar contexts tend to have similar meanings”&lt;/p&gt;

&lt;h4 id=&quot;distribution-hypothesis-was-studied-a-lot-in-lexical-semantics&quot;&gt;&lt;a name=&quot;distribution-hypothesis-was-studied-a-lot-in-lexical-semantics&quot;&gt;&lt;/a&gt;Distribution Hypothesis was studied a lot in lexical semantics&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Word similarity: Two words that are similar to each other exist in similar contexts&lt;/li&gt;
  &lt;li&gt;Word Relatedness: The meaning of two words can be related in ways other than relatedness similarity. One such class of connections is called word relatedness (Budanitsky association and Hirst, 2006)&lt;/li&gt;
  &lt;li&gt;Topics and Semantic Field: One common kind of relatedness between words is if they belong to the same semantic field semantic field. A semantic field is a set of words which cover a particular semantic domain and bear structured relations with each other. For example, words might be related by being in the semantic field of hospitals (surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof, topic models kitchen, family, bed). Semantic fields are also related to topic models, like Latent Dirichlet Allocation, LDA&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;words-as-vectors&quot;&gt;&lt;a name=&quot;words-as-vectors&quot;&gt;&lt;/a&gt;Words as vectors&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Words as vectors: Osgood’s 1957 idea mentioned above to use a point in three-dimensional space to represent the connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954), and Firth (1957) to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments.&lt;/li&gt;
  &lt;li&gt;Term document matrix: The term-document matrix was first defined as part of the vector space model of information retrieval (Salton, 1971).&lt;/li&gt;
  &lt;li&gt;Term frequency: term frequency (Luhn, 1957):&lt;/li&gt;
  &lt;li&gt;Inverse Document Frequency: (Sparck Jones, 1972)&lt;/li&gt;
  &lt;li&gt;Pointwise Mutual Information: (Fano, 1961), (Church and Hanks 1989, Church and Hanks 1990)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Measurement of similarity:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cosine similarity - inner dot product between vectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;with sparse long vectors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Representing words as 300-dimensional dense vectors requires our classifiers to learn far fewer weights than if we represented words as 50,000-dimensional vectors,&lt;/li&gt;
  &lt;li&gt;and the smaller parameter space possibly helps with generalization and avoiding overfitting.&lt;/li&gt;
  &lt;li&gt;unable to handle unknown words&lt;/li&gt;
  &lt;li&gt;independence assumption (which is untrue, and a very strong assumption to make)&lt;/li&gt;
  &lt;li&gt;Dense vectors may also do a better job of capturing semantics, transfer learning, reasoning and other tasks that were previously hard to do.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dense embeddings like word2vec actually have an elegant mathematical relationship with sparse embeddings like PPMI, in which word2vec can be seen as implicitly optimizing a shifted version of a PPMI matrix &lt;a href=&quot;https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf&quot;&gt;(Levy and Goldberg, 2014c)&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 13 Jul 2023 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/2023/ml-to-llm/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2023/ml-to-llm/</guid>
        
        <category>llm</category>
        
        <category>ml</category>
        
        <category>deep-learning</category>
        
        
        <category>llm</category>
        
        <category>ml</category>
        
        <category>deep-learning</category>
        
      </item>
    
      <item>
        <title>Important books to read for AI</title>
        <description>&lt;p&gt;Here is a list of books that I highly recommend to bolster your foundational knowledge in ML and AI:&lt;/p&gt;

&lt;p&gt;Pattern Recognition and Machine Learning, by Christopher M. Bishop&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot;&gt;Speech and Language Processing&lt;/a&gt;, Dan Jurafsky and James H. Martin&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;Deep learning&lt;/a&gt;, by Ian Goodfellow&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.inference.org.uk/mackay/itila/book.html&quot;&gt;Information Theory&lt;/a&gt;,by David Mackay&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Jun 2023 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/2023/Important-books-for-AI/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2023/Important-books-for-AI/</guid>
        
        <category>nlp</category>
        
        <category>deep-learning</category>
        
        <category>ai</category>
        
        <category>machine-learning</category>
        
        
        <category>nlp</category>
        
        <category>deep-learning</category>
        
        <category>ai</category>
        
        <category>machine-learning</category>
        
      </item>
    
      <item>
        <title>Probability Theory for Natural Language Processing</title>
        <description>&lt;p&gt;A lot of work in &lt;a href=&quot;https://rutumulkar.com/blog/2017/what-is-nlp/&quot;&gt;Natural Language Processing (NLP)&lt;/a&gt; such a creation of &lt;a href=&quot;https://rutumulkar.com/blog/2021/language-models/&quot;&gt;Language Models&lt;/a&gt; is based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Probability_theory&quot;&gt;probability theory&lt;/a&gt;. For the purpose of NLP, knowing about probabilities of words can help us predict the next word, understanding the rarity of words, analyzing and knowing when to ignore common words with respect to a given context - e.g. articles such as “the” and “a” have a very high probability of occurring in any document and add less information to the overall semantics, where as the probability of “supercalifragilisticexpialidocious” is incredibly low, but having it in a sentence provides much more semantics.&lt;/p&gt;

&lt;p&gt;Probability theory deals with predicting how likely something will happen. Below are some concepts to know and understand about probability. 
In this post I will discuss the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#experiment-trial&quot;&gt;Experiment (Trial)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#foundations&quot;&gt;Foundations&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#sample-space&quot;&gt;Sample space&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#event-space&quot;&gt;Event Space&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#disjoint-sets&quot;&gt;Disjoint Sets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#well-founded-probability-space&quot;&gt;Well Founded Probability Space&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#probability-theory&quot;&gt;Probability Theory&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#conditional-probability-and-independence&quot;&gt;Conditional Probability and Independence&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#chain-rule&quot;&gt;Chain Rule&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#independence-of-events&quot;&gt;Independence of Events&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bayes-theorem&quot;&gt;Bayes Theorem&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#random-variables&quot;&gt;Random Variables&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#probability-mass-function&quot;&gt;Probability Mass Function&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#expectation&quot;&gt;Expectation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#variance&quot;&gt;Variance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#standard-deviation&quot;&gt;Standard Deviation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#joint-probability-mass-function&quot;&gt;Joint Probability Mass Function&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#marginal-distribution&quot;&gt;Marginal Distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#relative-frequency&quot;&gt;Relative Frequency&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#distributions&quot;&gt;Distributions&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#continuous-distribution&quot;&gt;Continuous distribution&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#maximum-likelihood-estimate&quot;&gt;Maximum Likelihood Estimate&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#bayesian-updating&quot;&gt;Bayesian Updating&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#likelihood-ratio&quot;&gt;Likelihood Ratio&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#derivation&quot;&gt;Derivation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bayes-optimal-decision&quot;&gt;Bayes Optimal Decision&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;experiment-trial&quot;&gt;Experiment (Trial)&lt;/h1&gt;
&lt;p&gt;An experiment (also known as a trial) is a process by which an observation is made. Rolling a dice is a trial, observing the weather is a trial. The outcome of a trial is called an event.&lt;/p&gt;

&lt;h1 id=&quot;foundations&quot;&gt;Foundations&lt;/h1&gt;
&lt;h2 id=&quot;sample-space&quot;&gt;Sample space&lt;/h2&gt;
&lt;p&gt;The sample space is a collection of all the basic outcomes or events of our experiment.  Sample space can be discrete or continuous. In NLP, the sample space of a given dataset can be the exhausting combinations of words that can occur together as bigrams. In a trial with 2 dice, the sample space is all the combinations in which the dice can have an outcome. A sample space is represented as $\Omega$. $\phi$ represents an event that can never happen.&lt;/p&gt;

&lt;h2 id=&quot;event-space&quot;&gt;Event Space&lt;/h2&gt;
&lt;p&gt;An event space is the set of all the possible subsets of the sample space. The size of an event space is $2^{\Omega}$. An event space is represented as $\mathcal{F}$.&lt;/p&gt;

&lt;h2 id=&quot;disjoint-sets&quot;&gt;Disjoint Sets&lt;/h2&gt;
&lt;p&gt;Disjoint sets are sets that do not share any events with each other. E.g. in NLP if 2 datasets have no common vocabulary or vocabulary sequences, they are disjoint from one another. $A_j$ and $A_k$ are disjoint sets if they satisfy the following condition:&lt;/p&gt;

\[A_j \in \mathcal{F} (A_j \cap A_k = \phi, j \ne k)\]

\[P(\cup_{j=1}^{\inf}A_j) = \sum_{j=1}^{\inf} P(A_j)\]

&lt;h2 id=&quot;well-founded-probability-space&quot;&gt;Well Founded Probability Space&lt;/h2&gt;
&lt;p&gt;A well founded probability space contains:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sample space $\Omega$&lt;/li&gt;
  &lt;li&gt;a $\sigma$ field of events $\mathcal{F}$&lt;/li&gt;
  &lt;li&gt;A probability function (where all the individual probabilities sum to $1$)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;probability-theory&quot;&gt;Probability Theory&lt;/h1&gt;
&lt;h2 id=&quot;conditional-probability-and-independence&quot;&gt;Conditional Probability and Independence&lt;/h2&gt;
&lt;p&gt;Conditional Probability is the heart of &lt;a href=&quot;&quot;&gt;Naive Bayes&lt;/a&gt; algorithm. Conditional Probability measures the probability of an event given another event has occurred.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/cond_prob.png&quot; alt=&quot;Image of Conditional Probability&quot; width=&quot;400&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 1: Conditional Probability&lt;/em&gt;
&lt;/p&gt;

\[P(A|B) = \frac{P(A \cap B)}{P(B)}\]

&lt;p&gt;A simpler way of understanding conditional probability is the following: If event $B$ has definitely happened, how likely is it for event $A$ to also happen? This is answered by the fraction of times $A$ happens when $B$ happens.&lt;/p&gt;

&lt;h2 id=&quot;chain-rule&quot;&gt;Chain Rule&lt;/h2&gt;
&lt;p&gt;Chain rule of probabilities is used for Markov Models. According to chain rule, if we have 2 events $A$ and $B$, the probability of $A \cap B$ can be written as below:&lt;/p&gt;

\[P(A \cap B) = P(A|B) P(B)\]

&lt;p&gt;When events A and B occur together they are written as $P(A \cap B)$ or $P(A B)$&lt;/p&gt;

&lt;p&gt;For $n$ events, $A_1, A_2, \ldots, A_n$, the chain rule is:&lt;/p&gt;

\[P(A_1 \cap A_2 \ldots \cap A_n) = P(A_1) P(A_2|A_1) \ldots P(A_n|\cap_{i=1}^{n-1}A_i)\]

&lt;p&gt;Another notation is:&lt;/p&gt;

\[P(A_1 A_2 \ldots A_n) = P(A_1) P(A_2|A_1) \ldots P(A_n|\prod_{i=1}^{n-1}A_i)\]

&lt;p&gt;Applying this to NLP, we can compute the probability of the sentence “Pete is happy” by computing:&lt;/p&gt;

\[P(Pete, is, happy) = P(Pete) P(is| Pete) P(happy| is, Pete)\]

&lt;p&gt;For very long sentences, such joint probabilities are very hard to compute, e.g. the probability of the sentence “I am happy because I ate salad for lunch on Wednesday” depends on the frequency of this exact sentence in the corpus. It is quite possible that this exact sentence does not exist in our training dataset, cut the component words do exist. In which case it is helpful to make an independence assumption.&lt;/p&gt;

&lt;h2 id=&quot;independence-of-events&quot;&gt;Independence of Events&lt;/h2&gt;

&lt;p&gt;Two events $A$ and $B$ are independent of each other if $P(A B) = P(A) P(B)$. That means that there is no overlap between the two events with respect to Figure 1.&lt;/p&gt;

&lt;p&gt;When applying the independence assumption, $P(A|B) = P(A)$ because the presence of $B$ does not affect the probability of $A$ at all.&lt;/p&gt;

&lt;p&gt;Two events $A$ and $B$ are conditionally independent given an event $C$ where $P(C) &amp;gt; 0$ if:&lt;/p&gt;

\[P(A \cap B|C) = P(A|C) P(B|C)\]

&lt;h2 id=&quot;bayes-theorem&quot;&gt;Bayes Theorem&lt;/h2&gt;
&lt;p&gt;According to Bayes theorem:&lt;/p&gt;

\[P(B|A) = \frac{P(A|B) P(B)}{P(A)}\]

&lt;p&gt;The denominator is a normalizing factor, and it helps produce a probability function.&lt;/p&gt;

&lt;p&gt;If we are simply interested in which event is most likely given A, we can ignore the normalizing constant because it is the same value for all events.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bayes theorem is central to the noisy channel model&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;random-variables&quot;&gt;Random Variables&lt;/h2&gt;
&lt;p&gt;Random Variables map outcomes of random processes to numbers. Random variables are different from regular variables. Random variables can have many values, but regular variables can be solved for a small set of values. The probability $P(random|condition)$ helps with the mathematics notation of probability.&lt;/p&gt;

&lt;p&gt;The value held by a random variable can be discrete of continuous. Discrete is separate values e.g. 1, 2, 3, where as continuous is when the random variable can held any value within an interval e.g. between 0 and 1.&lt;/p&gt;

&lt;h2 id=&quot;probability-mass-function&quot;&gt;Probability Mass Function&lt;/h2&gt;
&lt;p&gt;Probability Mass Function (or PMF) of a discrete random variable $X$ provides the probabilities $P(X=x)$ for all the possible values of $x$.&lt;/p&gt;

&lt;h2 id=&quot;expectation&quot;&gt;Expectation&lt;/h2&gt;
&lt;p&gt;Expectation tells us what is the most likely outcome to expect. Expectation is the mean or average of the random variable:&lt;/p&gt;

\[E(X) = \sum_{x}xP(x)\]

&lt;p&gt;Calculating Expectation is central to Information Theory.&lt;/p&gt;

&lt;h2 id=&quot;variance&quot;&gt;Variance&lt;/h2&gt;
&lt;p&gt;The variance of a random variable is a measure of whether the values of the random variable tend to be consistent over trials or to vary a lot. Variance is represented as $\sigma^2$&lt;/p&gt;

\[\begin{eqnarray}
var(X) &amp;amp;=&amp;amp; E((X-E(X))^2 \nonumber\\
 &amp;amp;=&amp;amp; E(X^2)-E^2(X))
\end{eqnarray}\]

&lt;h2 id=&quot;standard-deviation&quot;&gt;Standard Deviation&lt;/h2&gt;
&lt;p&gt;Standard Deviation is the square root of variance. It is represented as $\sigma$.&lt;/p&gt;

&lt;h2 id=&quot;joint-probability-mass-function&quot;&gt;Joint Probability Mass Function&lt;/h2&gt;
&lt;p&gt;The joint probability of two events $x$ and $y$ is represented as:&lt;/p&gt;

\[P(x,y) = P(X=y, Y=y)\]

&lt;h2 id=&quot;marginal-distribution&quot;&gt;Marginal Distribution&lt;/h2&gt;
&lt;p&gt;Marginal PMFs total up the probability masses for the values of each variable separately.&lt;/p&gt;

\[P_X(x) = \sum_y P(x,y)\]

\[P_Y(y) = \sum_x P(x,y)\]

&lt;h2 id=&quot;relative-frequency&quot;&gt;Relative Frequency&lt;/h2&gt;
&lt;p&gt;The proportion of times a certain outcome occurs is called the relative frequency of the outcome.&lt;/p&gt;

&lt;p&gt;$P$ - probability function&lt;/p&gt;

&lt;p&gt;$p$ - probability mass function&lt;/p&gt;

&lt;p&gt;We take a parametric approach to estimate the probability function in language.&lt;/p&gt;

&lt;p&gt;Non-parametric approaches are used in classification, or when the underlying distribution of the data is unknown.&lt;/p&gt;

&lt;h1 id=&quot;distributions&quot;&gt;Distributions&lt;/h1&gt;
&lt;p&gt;The types of functions for probability mass functions are called distributions&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Discrete Distributions&lt;/strong&gt;: Binomial discrete distributions are a series of trails with 2 outcomes only.
    &lt;ul&gt;
      &lt;li&gt;Multi-nominal Distribution: A special case of binomial distribution is &lt;strong&gt;Multi-nominal Distribution&lt;/strong&gt; where each trial has more than 2 basic outcomes&lt;/li&gt;
      &lt;li&gt;Bernoulli distribution: A special case of discrete distributions where there is only one trial.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Continuous distributions&lt;/strong&gt; Continuous distributions are also known as Normal distributions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;continuous-distribution&quot;&gt;Continuous distribution&lt;/h2&gt;
&lt;p&gt;A continuous distribution is also known as a Normal Distribution or a Bell Curve. We also refer to them as Gaussians (often used in clustering). A Gaussian is represented as:&lt;/p&gt;

\[n(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} e^{\frac{-(x-\mu)}{2\sigma^2}}\]

&lt;p&gt;Where $\sigma$ is the standard deviation and $\mu$ is the mean.&lt;/p&gt;

&lt;p&gt;It is also the probability density of observing a single data point $x$, that is generated from a gaussian distribution.&lt;/p&gt;

&lt;h1 id=&quot;maximum-likelihood-estimate&quot;&gt;Maximum Likelihood Estimate&lt;/h1&gt;
&lt;p&gt;Maximum Likelihood Estimate or MLE helps us identify which values of $\mu$ and $\sigma$ should we use that produce a curve that explains or covers all the data points. It is the way to determine the most likely outcome to a set of trials.&lt;/p&gt;

&lt;h2 id=&quot;bayesian-updating&quot;&gt;Bayesian Updating&lt;/h2&gt;
&lt;p&gt;The process of using prior to get posterior, posterior becomes the new prior, as new data comes.&lt;/p&gt;

\[P(\Theta|data) = P(data|\Theta) \text{ } P(\Theta)\]

&lt;p&gt;Here $\Theta$ is what we are interested in and what we are trying to estimate. It represented the set of parameters. If we are trying to estimate the parameters of a gaussian distribution then $\Theta$ represents both the mean $\mu$ and the standard deviation $\sigma$ and $\Theta = \{\mu, \sigma\}$&lt;/p&gt;

&lt;p&gt;Here $P(\Theta|data)$ is the posterior and $P(\Theta)$ is the prior.&lt;/p&gt;

&lt;p&gt;Prior belief is computed as the following:&lt;/p&gt;

\[P(s|\mu_m) = m^i (1-m)^j\]

&lt;p&gt;Here:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$i$ = outcome of 1 counts&lt;/li&gt;
  &lt;li&gt;$j$ = outcome of 2 counts&lt;/li&gt;
  &lt;li&gt;$\mu_m$ is the model that asserts the outcome&lt;/li&gt;
  &lt;li&gt;$s$ is a sequence of observations&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;likelihood-ratio&quot;&gt;Likelihood Ratio&lt;/h2&gt;
&lt;p&gt;Ratio computed between 2 models to see which of them is more likely to occur.&lt;/p&gt;

&lt;p&gt;People often take log likelihood ratio and see it the result is $&amp;gt;$ 1 or $&amp;lt;$ 1&lt;/p&gt;

&lt;p&gt;The ratio to determine which theory is most likely to occur given a sequence of events $s$.&lt;/p&gt;

&lt;p&gt;Let $v$ be the first theory and $\mu$ be the seond theory.&lt;/p&gt;

\[\frac{P(\mu|s)}{P(v|s)} = \frac{P(s|\mu) P(\mu)}{P(s|v)P(v)}\]

&lt;p&gt;If the ratio is $&amp;gt;1$ we prefer theory $\mu$ (the numerator).
If the ratio is $&amp;lt;1$ we prefer theory $v$ (the denominator).&lt;/p&gt;

&lt;h1 id=&quot;derivation&quot;&gt;Derivation&lt;/h1&gt;
&lt;p&gt;Derivation is the process of finding the maxima and minima of functions.&lt;/p&gt;

&lt;p&gt;Computing the partial derivative WRT $\mu$ and then setting the equation to $0$ gives us the MLE fo $\mu$&lt;/p&gt;

&lt;h1 id=&quot;bayes-optimal-decision&quot;&gt;Bayes Optimal Decision&lt;/h1&gt;
&lt;p&gt;When we pick the best decision theory out of all the available theories that could explain the data, we make the Bayes Optimal Decision.&lt;/p&gt;
</description>
        <pubDate>Sat, 08 May 2021 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/2021/probability-theory/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2021/probability-theory/</guid>
        
        <category>nlp</category>
        
        <category>classification</category>
        
        
        <category>probability</category>
        
      </item>
    
      <item>
        <title>The Foundations of Language Models</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;&gt;Language Models&lt;/a&gt; are models that are trained to predict the next word, given a set of words that are already uttered or written. e.g. Consider the sentence:  &lt;em&gt;“Don’t eat that because it looks…“&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The next word following this will most likely be “disgusting”, or “bad”, but will probably not be “table” or “chair”. &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;&gt;Language Models&lt;/a&gt; are models that assign probabilities to sequences of words to be able to predict the next word given a sequence of words.&lt;/p&gt;

&lt;p&gt;The probability of a word $w$ given some history $h$ is $p(w|h)$.&lt;/p&gt;

&lt;p&gt;In this post I am going to discuss the following concepts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#n-grams&quot;&gt;N-Grams&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#working-example&quot;&gt;Working Example&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#practical-issues&quot;&gt;Practical Issues&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#problems-with-language-models&quot;&gt;Problems with language Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#smoothing&quot;&gt;Smoothing&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#laplace-smoothing&quot;&gt;Laplace Smoothing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#add-k-smoothing&quot;&gt;Add-k smoothing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#backoff-and-interpolation&quot;&gt;Backoff and Interpolation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#evaluating-language-models&quot;&gt;Evaluating Language Models&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#intrinsic-evaluation&quot;&gt;Intrinsic Evaluation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#extrinsic-evaluation&quot;&gt;Extrinsic Evaluation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#perplexity&quot;&gt;Perplexity&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#n-gram-efficiency-considerations&quot;&gt;N-Gram Efficiency considerations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;n-grams&quot;&gt;N-Grams&lt;/h2&gt;
&lt;p&gt;The probability of a word $w$ given the history $h$ is defined as:&lt;/p&gt;

\[p(w|h)\]

&lt;p&gt;as $h$ is several tokens long, we can rephrase it as the probability of the ${n+1}^{th}$ word $w_{n+1}$ depends on the words $w_1, w_2 \cdots w_n$.&lt;/p&gt;

\[p(w_{n+1}|w_1, w_2, w_3 \cdots w_n)\]

&lt;p&gt;One way to answer this is using relative frequency counts. i.e. count the number of times we see $w_1, w_2, w_3 \cdots w_n$ and the number of times we see $w_1, w_2, w_3 \ldots w_n$ followed by $w_{n+1}$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.mathsisfun.com/data/relative-frequency.html&quot;&gt;Relative frequency&lt;/a&gt; is defined as the ratio of an observed sequence to the observed sequence followed by a suffix. Using the concept of relative frequency we can get:&lt;/p&gt;

\[P(w_{n+1}|w_1 w_2 w_3 \ldots w_n) = \frac{C(w_1 w_2 w_3 \cdots w_n w_{n+1})}{C(w_1 w_2 w_3 \ldots w_n)}\]

&lt;p&gt;Where $C(x_1, x_2)$ is the count or the number of times we see a pattern of token $x_1$ followed by $x_2$.&lt;/p&gt;

&lt;p&gt;This approach of getting probabilities from counts works well in many cases, but if we wanted to know the joint probability of an entire sequence of words $p(w_1, w_2, w_3 \ldots w_n)$, we would have to compute - out of all the possible combinations of size $n$ how many are this exact sequence $w_1, w_2, w_3 \ldots w_n$. It fails when the size of the sequence is very long.&lt;/p&gt;

&lt;p&gt;Even the entire web isn’t big enough to compute such probabilities, because there are not enough examples for every word combination even on the world wide web). For this reason, we will have to introduce clever ways of computing probability.&lt;/p&gt;

&lt;p&gt;Let’s decompose this joint probability into a conditional probabilities using the chain rule of probability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Chain_rule_(probability)&quot;&gt;Chain Rule of Probability&lt;/a&gt; helps us decompose a joint probability into a conditional probability of a word given previous words. &lt;a href=&quot;https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/&quot;&gt;Joint Probability&lt;/a&gt; of $n$ words is the probability of $n$ words occurring together. Using the chain rule, we can break down Equation the joint probability of a sequence of tokens into conditional probabilities. Here is how we do it:&lt;/p&gt;

\[\begin{eqnarray}
P(w_1 w_2 w_3 \cdots w_n w_{n+1}) &amp;amp;=&amp;amp; P(w_1) P(w_2|w_1) P(w_3|w_1 w_2) \ldots P(w_n|w_1\ldots w_{n-1}) \nonumber \\ 
&amp;amp;=&amp;amp; \prod_{k=1}^{n+1} P(w_k|w_1 \ldots w_k)
\end{eqnarray}\]

&lt;p&gt;The chain rule still has the constraint of needing the probability of a long sequence of previous words. One idea is to approximate this using the Markov Assumption.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_property&quot;&gt;Markov Assumption&lt;/a&gt; says that the probability of a word only depends on the previous word and not the entire sequence of tokens preceding it. According to this assumption, we can predict a word without looking too much into the previous history of words. So, instead of working with the exact probabilities of a long sequence of preceding words, we can use a small window of preceding words. This is where we introduce a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bigram&quot;&gt;bigram model&lt;/a&gt; (that uses the preceding word only) a &lt;a href=&quot;https://en.wikipedia.org/wiki/Trigram&quot;&gt;trigram model&lt;/a&gt; (that uses the preceding two words) to predict the probability of a sequence of tokens.&lt;/p&gt;

&lt;p&gt;Using the bigram model we can simplify the probability of a sequence of tokens to the following:&lt;/p&gt;

\[P(w_1, w_2, w_3 \cdots w_n, w_{n+1}) = \prod_{k=1}^n P(w_k|w_{k-1})\]

&lt;p&gt;Similarly using the trigram model we can simplify the probability of a sequence of tokens to the following:&lt;/p&gt;

\[P(w_1, w_2, w_3 \cdots w_n, w_{n+1}) = \prod_{k=1}^n P(w_k|w_{k-1} w_{k-2})\]

&lt;p&gt;Now that we have simplified the RHS, we need to compute the probabilities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#:~:text=In%20statistics%2C%20maximum%20likelihood%20estimation,observed%20data%20is%20most%20probable.&quot;&gt;Maximum Likelihood Estimation (MLE)&lt;/a&gt; is an intuitive way of measuring the parameters of an N-gram model, by computing the counts of words or tokens that exist together, normalized by the total counts so that the output is a probability that is between 0 and 1. Relative frequency is one way of measuring the MLE. Relative frequency can be computed using equation (7) below and Figure (1) explains the equation:&lt;/p&gt;

\[P(w_n|w_{n-1}) = \frac{C(w_{n-1}w_n)}{\sum_w C(w_{n-1}w)}\]

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://localhost:4000/assets/images/relative-frequency.png&quot; alt=&quot;Relative Frequency&quot; width=&quot;500&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 1: Relative Frequency Explained&lt;/em&gt;
&lt;/p&gt;

&lt;h3 id=&quot;working-example&quot;&gt;Working Example&lt;/h3&gt;
&lt;p&gt;Let’s build a small language model using bigrams, and use our model to predict the probability of a new sentence. For our toy example, consider a tiny corpus of the following sentences:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;\s&amp;gt;&lt;/p&gt;

  &lt;p&gt;&amp;lt;s&amp;gt; Sam I am &amp;lt;\s&amp;gt;&lt;/p&gt;

  &lt;p&gt;&amp;lt;s&amp;gt; I do not like that Sam I am &amp;lt;\s&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here &amp;lt;s&amp;gt; represents the start of a sentence and &amp;lt;\s&amp;gt; represents the end of the sentence.&lt;/p&gt;

&lt;p&gt;This corpus has the following lexicon or unique words:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;token&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;frequency&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;I&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;am&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sam&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;do&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;not&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;like&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;that&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Computing the conditional probabilities&lt;/p&gt;

\[P(I|&amp;lt;s&amp;gt;) = \frac{2}{4}\]

\[P(Sam|&amp;lt;s&amp;gt;) = \frac{1}{3}\]

\[P(am|I) = \frac{3}{4}\]

&lt;p&gt;We can continue computing probabilities for all different possibilities of bigrams, then for any new sentence such as  the following:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I like Sam&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We can compute the join probability of the tokens of this sentence from the conditional probability of the bigrams $P(like|I)$ and $P(Sam|like)$.&lt;/p&gt;

&lt;h3 id=&quot;practical-issues&quot;&gt;Practical Issues&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Always use Log probabilities. Multiplication in linear space is addition in log space, and we will avoid numerical underflow&lt;/li&gt;
  &lt;li&gt;It is typical to use trigrams instead of bigrams (although we illustrated bigrams in the example above)&lt;/li&gt;
  &lt;li&gt;There are often unknown words in the sentence we want to predict the probability of, and we need to handle that.&lt;/li&gt;
  &lt;li&gt;We often won’t find instances of joint probability in our corpus, and we need to account for that. E.g. in the above example we do not have an instance of ‘sam like’, and so the conditional probability of $P(Sam|like)$ will be $0$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problems-with-language-models&quot;&gt;Problems with language Models&lt;/h2&gt;
&lt;p&gt;Language Models face the issue of &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;&gt;sparsity&lt;/a&gt; - which means that the training corpus is limited and some perfectly acceptable English word sequences are bound to be missing from it. This means that it is possible to have several n-grams with a probability of $0$, but should actually have a non-zero probability.&lt;/p&gt;

&lt;p&gt;Another issue with language models is that the vocabulary the the language model is trained on might not have seen words from the test dataset - introducing the issue of unknown words or &lt;a href=&quot;https://groups.csail.mit.edu/sls/publications/2000/03105.pdf&quot;&gt;out of vocabulary words(OOV)&lt;/a&gt;. One way to deal with OOV words is to replace all words with a frequency below a certain threshold by ‘UNK’. Other ways to deal with this is to use smoothing and discounting techniques.&lt;/p&gt;

&lt;h2 id=&quot;smoothing&quot;&gt;Smoothing&lt;/h2&gt;
&lt;p&gt;There will always be unknown words in the test dataset that the language model will have to work with that sparsity. To keep the language model from assigning zero probabilities to unseen events, we will shave off some probability mass from some more frequent events and give it to the events that we have never seen. This process is called &lt;a href=&quot;https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf&quot;&gt;smoothing&lt;/a&gt; or &lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1086/handouts/cs224n-lecture2-language-models-slides.pdf&quot;&gt;discounting&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A few types of smoothing are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;add-1 smoothing (or &lt;a href=&quot;****&quot;&gt;Laplace smoothing&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;add-k smoothing&lt;/li&gt;
  &lt;li&gt;backoff and interpolation&lt;/li&gt;
  &lt;li&gt;stupid backoff&lt;/li&gt;
  &lt;li&gt;Kneser ney smoothing&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;laplace-smoothing&quot;&gt;Laplace Smoothing&lt;/h3&gt;
&lt;p&gt;Laplace smoothing involves adding $1$ to all of the bigram (or n-gram) counts before we normalize them into probabilities.&lt;/p&gt;

\[P(w_i) = \frac{c_i}{N}\]

&lt;p&gt;If we add $1$ to each probability, and there are $V$ words in the vocabulary, we will need to add $V$ to the denominator as we add $1$ to each numerator.&lt;/p&gt;

\[P_{laplace}(w_i) = \frac{c_i}{N+V} \label{laplace}\]

&lt;p&gt;In equation \ref{laplace}, $w_i$ is the $i^{th}$ word, $N$ is a normalizer (total number of words) and $V$ is the vocabulary size.&lt;/p&gt;

&lt;p&gt;But instead of adding to the numerator and denominator, a better way to is change the numerator only, and show how it affects smoothing by describing an adjusted count $c^*$.&lt;/p&gt;

\[c_i^* = (c_i + 1) \frac{N}{N+V}\]

&lt;p&gt;Now we can convert each adjusted count into a probability by dividing it by $N$ (the total number of tokens)&lt;/p&gt;

&lt;p&gt;Although Laplace smoothing isn’t the best type of smoothing as it gives away a lot of probability mass to infrequent terms, it is still used and is practical for classification.&lt;/p&gt;

&lt;p&gt;** Question: How can you use Language Models for classification? **&lt;/p&gt;

\[P(w_n|w_{n-1}) = \frac{C(w_{n-1} w_n)}{C(w_{n-1})}\]

&lt;p&gt;Using Laplace Smoothing, this becomes:&lt;/p&gt;

\[p^*_{Laplace}(w_n|w_{n-1}) = \frac{C(w_{n-1} w_n) + 1}{\sum_w C(w_{n-1}w) + V}\]

&lt;h3 id=&quot;add-k-smoothing&quot;&gt;Add-k smoothing&lt;/h3&gt;
&lt;p&gt;Add-K smoothing is an alternative to add $1$ smoothing, where we move a bit less of the probability mass from the seen events to the unseen events.&lt;/p&gt;

\[p^*_{add-k}(w_n|w_{n-1}) = \frac{C(w_{n-1} w_n) + k}{\sum_w C(w_{n-1}w) + kV}\]

&lt;p&gt;Add-K requires us to have a method for choosing our k (0.5? 0.1? 0.05?) e.g. one can optimize over a dev set or some other data source.&lt;/p&gt;

&lt;h3 id=&quot;backoff-and-interpolation&quot;&gt;Backoff and Interpolation&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Katz%27s_back-off_model&quot;&gt;Backoff&lt;/a&gt; is an approach for smoothing using which we only backoff to a lower order n-gram when we have zero evidence for a higher level n-gram.&lt;/p&gt;

&lt;p&gt;So, we use a trigram if the evidence is sufficient, but if such a trigram does not exist, we backoff to a bigram, and if the bigram does not exist, we backoff to a unigram.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf&quot;&gt;Interpolation&lt;/a&gt; is an approach is using a mixture of probability estimates from all the n-gram estimators. For instance, if we are looking at trigrams, we would compute its probability by combining the trigram, bigram and unigram counts.&lt;/p&gt;

&lt;p&gt;Interpolation for a trigram can be defined by the following formula:&lt;/p&gt;

\[\hat{p}(w_n|w_{n-2}w_{n-1}) = \lambda_1 p(w_n|w_{n-2}w_{n-1}) + \lambda_2 p(w_n|w_{n-1}) + \lambda_3 p(w_n)\]

&lt;p&gt;where $\sum_i \lambda_i = 1$&lt;/p&gt;

&lt;p&gt;The values of $\lambda$ can be computed by optimizing over a heldout dataset. &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;EM Algorithm (Expectation Maximization Algorithm)&lt;/a&gt; is an iterative learning algorithm that converges on locally optimal $\lambda$s&lt;/p&gt;

&lt;h2 id=&quot;evaluating-language-models&quot;&gt;Evaluating Language Models&lt;/h2&gt;
&lt;p&gt;There are 2 ways to evaluate language models:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;intrinsic evaluation: Evaluation of the model as a measure of how much it improves the application that it is used in.&lt;/li&gt;
  &lt;li&gt;extrinsic evaluation: Measure of the quality of the model independent of any application&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intrinsic-evaluation&quot;&gt;Intrinsic Evaluation&lt;/h3&gt;
&lt;p&gt;For intrinsic evaluation of a language model we need to have:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;training dataset&lt;/li&gt;
  &lt;li&gt;test dataset&lt;/li&gt;
  &lt;li&gt;held out dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We use the language model to compute scores on the test dataset, and use the heldout and training dataset to optimize out language model.&lt;/p&gt;

&lt;h3 id=&quot;extrinsic-evaluation&quot;&gt;Extrinsic Evaluation&lt;/h3&gt;
&lt;p&gt;To compute extrinsic evaluation of a language model, we compute the effect of a new language model on the final end to end product that it is integrated with. Good scores during intrinsic evaluation does not always mean better scores during extrinsic evaluation, which is why both the types of evaluation are important.&lt;/p&gt;

&lt;h4 id=&quot;perplexity&quot;&gt;Perplexity&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Perplexity&quot;&gt;Perplexity&lt;/a&gt; is the measure of computation of the probabilities learned from the training dataset and applied on the test dataset. Perplexity is represented as $PP$ and is measured as the inverse probability of the test set, normalized by the number of words.&lt;/p&gt;

\[\begin{eqnarray}
PP(w) &amp;amp;=&amp;amp; P(w_1 w_2 w_3 \ldots w_n)^{-\frac{1}{n}} \nonumber \\
&amp;amp;=&amp;amp; \sqrt[n]{\frac{1}{P(w_1 w_2 w_3 \ldots w_n)}} \nonumber \\
&amp;amp;=&amp;amp; \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i|w_1 w_2 \ldots w_{i-1})}}
\end{eqnarray}\]

&lt;p&gt;To compute perplexity of a bigram, we can simplify Equation (11) to the following:&lt;/p&gt;

\[PP(w) = \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i|w_{i-1})}}\]

&lt;p&gt;Similarly, to compute perplexity of a bigram, we can simplify Equation (11) to the following:&lt;/p&gt;

\[PP(w) = \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i|w_{i-1} w_{i-2})}}\]

&lt;blockquote&gt;
  &lt;p&gt;Minimizing perplexity is equivalent to maximizing the test set probability. If the perplexity is low, it means that the training data captures the probability of the test set really well.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another way to think about perplexity is to think of it as the weighted average branching factor of the language. The &lt;a href=&quot;https://towardsdatascience.com/perplexity-in-language-models-87a196019a94&quot;&gt;branching factor&lt;/a&gt; of a language is the number of possible next words that can follow any word.&lt;/p&gt;

&lt;p&gt;Intrinsic improvement in perplexity does not guarantee an extrinsic improvement in the performance of the language processing task.&lt;/p&gt;

&lt;h2 id=&quot;n-gram-efficiency-considerations&quot;&gt;N-Gram Efficiency considerations&lt;/h2&gt;
&lt;p&gt;When a language model uses large sets of n-grams, it is important to store the efficiently. Below are some ways to store LMs efficiently:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Words: storing words in 64 bit hash representations, and the actual words are stored on disc as string&lt;/li&gt;
  &lt;li&gt;Probabilities: 4-8 bits instead of 8 byte float&lt;/li&gt;
  &lt;li&gt;n-grams: Stored in reverse &lt;a href=&quot;https://en.wikipedia.org/wiki/Trie&quot;&gt;tries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Approximate language models can be created using techniques such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Bloom_filter&quot;&gt;bloom filters&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;n-grams can be shrunk by pruning i.e. only storing n-grams with counts greater than some threshold.&lt;/li&gt;
  &lt;li&gt;Efficient Language Models such as &lt;a href=&quot;https://github.com/kpu/kenlm&quot;&gt;KenLM&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Use sorted Arrays&lt;/li&gt;
      &lt;li&gt;Efficiently combined probabilities and backoffs into a single value&lt;/li&gt;
      &lt;li&gt;Use merge sorts to efficiently build probability tables in a minimal number of passes through a large corpus&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/3.pdf&quot;&gt;Chapter 3, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition&lt;/a&gt; by Daniel Jurafsky, James H. Martin&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;&gt;Language Models&lt;/a&gt;, Wikipedia&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1086/handouts/cs224n-lecture2-language-models-slides.pdf&quot;&gt;Lecture 2, Language Models&lt;/a&gt;, CS224n Stanford NLP&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf&quot;&gt;NLP Lunch Tutorial: Smoothing&lt;/a&gt;, Bill MacCartney, Stanford NLP&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mathsisfun.com/data/relative-frequency.html&quot;&gt;Relative Frequency&lt;/a&gt;, mathisfun.com&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 24 Apr 2021 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/2021/language-models/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2021/language-models/</guid>
        
        <category>language_models</category>
        
        
        <category>language models</category>
        
      </item>
    
      <item>
        <title>The Comprehensive Guide to Logistic Regression</title>
        <description>&lt;p&gt;In &lt;a href=&quot;https://algorithmia.com/blog/introduction-natural-language-processing-nlp&quot;&gt;Natural Language Processing&lt;/a&gt; (NLP) &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;Logistic Regression&lt;/a&gt; is the baseline supervised ML algorithm for &lt;a href=&quot;https://en.wikipedia.org/wiki/Classification&quot;&gt;classification&lt;/a&gt;. It also has a very close relationship with &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_network&quot;&gt;neural networks&lt;/a&gt; (If you are new to neural networks, start with Logistic Regression to understand the basics.)&lt;/p&gt;

&lt;p&gt;In this post I will discuss the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#components-of-a-classification-system&quot;&gt;Components of a Classification System&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#logistic-regression-phases&quot;&gt;Logistic Regression Phases&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#feature-representation&quot;&gt;Feature Representation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#classification-using-the-sigmoid-function&quot;&gt;Classification using the Sigmoid Function&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#characteristics-of-sigmoid-function&quot;&gt;Characteristics of Sigmoid Function&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#learning-process-in-logistic-regression&quot;&gt;Learning Process in Logistic Regression&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cost-function-cross-entropy-loss&quot;&gt;Cost Function: Cross Entropy Loss&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#cross-entropy&quot;&gt;Cross Entropy&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#convex-optimization-problem&quot;&gt;Convex Optimization Problem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#decision-boundary&quot;&gt;Decision Boundary&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gradient-descent&quot;&gt;Gradient Descent&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#learning-rate&quot;&gt;Learning Rate&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#stochastic-gradient-descent&quot;&gt;Stochastic Gradient Descent&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#batch-training&quot;&gt;Batch Training&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#mini-batch-training&quot;&gt;Mini-Batch Training&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#regularization&quot;&gt;Regularization&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#l2-regularization&quot;&gt;L2 Regularization&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#l1-regularization&quot;&gt;L1 Regularization&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#multinomial-logistic-regression&quot;&gt;Multinomial Logistic Regression&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#loss-function-in-multinomial-logistic-regression&quot;&gt;Loss Function in Multinomial Logistic Regression&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#working-example-of-logistic-regression&quot;&gt;Working Example of Logistic Regression&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gradient-descent-step-1&quot;&gt;Gradient Descent Step 1&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gradient-descent-step-2&quot;&gt;Gradient Descent Step 2&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary-of-logistic-regression&quot;&gt;Summary of Logistic Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#further-reading&quot;&gt;Further Reading&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Logistic Regression is a &lt;a href=&quot;https://medium.com/@akankshamalhotra24/generative-classifiers-v-s-discriminative-classifiers-1045f499d8cc#:~:text=An%20example%20of%20a%20discriminative,decision%20boundary%20for%20the%20model.&quot;&gt;discriminative classifier&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Discriminative models try to learn to &lt;strong&gt;distinguish&lt;/strong&gt; what different classes of data look like.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some examples of discriminative classifiers are &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;Logistic Regression&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_neural_network&quot;&gt;Neural Networks&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Conditional_random_field&quot;&gt;Conditional Random Fields&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Support-vector_machine&quot;&gt;Support Vector Machines&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Naive Bayes&lt;/a&gt;, is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_model&quot;&gt;generative classifier&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Generative models have the goal to &lt;strong&gt;understand&lt;/strong&gt; what different classes of data look like.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some examples of generative classifiers include &lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Naive Bayes&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_network&quot;&gt;Bayesian Networks&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_random_field&quot;&gt;Markov Random Fields&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_model&quot;&gt;Hidden Markov Models&lt;/a&gt;. &lt;a href=&quot;https://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/&quot;&gt;LDA&lt;/a&gt; (Latent Dirichlet Allocation is a generative statistical model for topic modeling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf&quot;&gt;Ng and Jordan, 2001&lt;/a&gt; provide a great analysis of generative vs discriminative models.&lt;/p&gt;

&lt;h1 id=&quot;components-of-a-classification-system&quot;&gt;Components of a Classification System&lt;/h1&gt;

&lt;p&gt;A Machine Learning system for classification has 4 components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Feature Representation: For each input observation $x^{(i)}$, this will be a vector of features $[x_1, x_2, x_3, … , x_n]$&lt;/li&gt;
  &lt;li&gt;A Classification Function: This gets us the probability of the output, given an input. This is denoted by $P(y|x)$. &lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;Sigmoid&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;&gt;Softmax&lt;/a&gt; are tools for classification for Logistic Regression.&lt;/li&gt;
  &lt;li&gt;Objective Function for Learning: This is the function that we want to optimize, usually involving minimizing error on training examples. Cross Entropy Loss is the objective function for Logistic Regression.&lt;/li&gt;
  &lt;li&gt;Algorithm for Optimizing the Objective Function: We use the Stochastic Gradient Descent Algorithm for optimizing over our Objective Function.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;logistic-regression-phases&quot;&gt;Logistic Regression Phases&lt;/h2&gt;

&lt;p&gt;Logistic Regression has two phases:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Training Phase: We train the system (specifically the weights $w$ and bias $b$) using Stochastic Gradient Descent and Cross Entropy Loss&lt;/li&gt;
  &lt;li&gt;Test Phase: Given a text example $x$, we compute $p(y|x)$ and return the higher probability label.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;feature-representation&quot;&gt;Feature Representation&lt;/h1&gt;

&lt;p&gt;A single input observation $x$ can be represented by a vector of features $[x_1, x_2, x_3, \ldots , x_n]$. For Logistic Regression this is usually done by &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_engineering&quot;&gt;feature engineering&lt;/a&gt; which is the process of manually identifying which features are relevant to solve the problem, and convert the text features into real numbers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Feature Interactions&lt;/strong&gt;: Feature Interaction is the combination of different features to form a more complex feature.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Feature Templates&lt;/strong&gt;: Feature Templates is when templates are used to automatically create features using abstract specification of features.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Representation Learning&lt;/strong&gt;: &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;&gt;Representation Learning&lt;/a&gt; is the process of learning features automatically in an unsupervised way from the input. In order to avoid the excessive human effort of feature design, recent NLP efforts are focused on representation learning&lt;/p&gt;

&lt;h1 id=&quot;classification-using-the-sigmoid-function&quot;&gt;Classification using the Sigmoid Function&lt;/h1&gt;

&lt;p&gt;The result of Logistic Regression is:&lt;/p&gt;

\[z = (\sum_{i=1}^n w_i x_i) + b\]

&lt;p&gt;Here, $w_i \cdot x_i$ is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dot_product&quot;&gt;dot product&lt;/a&gt; of vectors $x_i$ and $w_i$ and $z$ is a real number vector ranging from $-\infty$ to $+\infty$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dot Product: The dot product of 2 vectors $a$ and $b$ is written as $a \cdot b$ and is the sum of the products of the corresponding elements of each vector.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$z$ is not a legal probability. To make it a probability, $z$ will pass through the, written as $\sigma(z)$&lt;/p&gt;

\[y = \sigma(z) = \frac{1}{(1+e^{-z})}\]

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/sigmoid.jpg&quot; alt=&quot;Image of Sigmoid Function&quot; width=&quot;400&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 1: Sigmoid Function (Image Credit: Wikipedia)&lt;/em&gt;
&lt;/p&gt;

&lt;h2 id=&quot;characteristics-of-sigmoid-function&quot;&gt;Characteristics of Sigmoid Function&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Sigmoid takes a real valued number and maps it to the range [0, 1] (which is perfect to get a probability)&lt;/li&gt;
  &lt;li&gt;Sigmoid tends to squash outlier values towards 0 or 1&lt;/li&gt;
  &lt;li&gt;Sigmoid is differentiable - which makes it handy for learning (A function is not differentiable if it has a undefined slope or a vertical slope)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For classification into two classes:&lt;/p&gt;

\[\begin{eqnarray}
p(y=1) &amp;amp;=&amp;amp; \sigma(w \cdot x) + b \nonumber \\
    &amp;amp;=&amp;amp; \frac{1}{1+e^{w.x + b}}
\end{eqnarray}\]

\[\begin{eqnarray}
p(y=0) &amp;amp;=&amp;amp; 1 - \sigma(w \cdot x) + b \nonumber \\
&amp;amp;=&amp;amp; 1 - \frac{1}{1+e^{w.x + b}} \nonumber \\
&amp;amp;=&amp;amp;\frac{e^{w.x + b}}{1+e^{w.x + b}}
\end{eqnarray}\]

&lt;h1 id=&quot;learning-process-in-logistic-regression&quot;&gt;Learning Process in Logistic Regression&lt;/h1&gt;

&lt;h2 id=&quot;cost-function-cross-entropy-loss&quot;&gt;Cost Function: Cross Entropy Loss&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://machinelearningmastery.com/cross-entropy-for-machine-learning/&quot;&gt;Cross Entropy Loss&lt;/a&gt; is a function that determines for an observation $x$, how close the output of the classifier $\hat{y}$ is to the correct output $y$. This $Loss$ is expressed as: $ L(\hat{y}, y) $&lt;/p&gt;

&lt;p&gt;$ L(\hat{y}, y)$ is computed via a loss function that prefers the correct class labels of the training examples to be more likely. This is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;conditional maximum likelihood estimation&lt;/a&gt;. We choose parameters $w$ and $b$ that maximize the probability of the true $y$ labels in the training data given the observations $x$.&lt;/p&gt;

&lt;p&gt;Given a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bernoulli_distribution&quot;&gt;Bernoulli Distribution&lt;/a&gt; (a distribution that can only have 2 outcomes):&lt;/p&gt;

\[p(y|x) = \hat{y}^y (1-\hat{y})^{1-y}\]

&lt;p&gt;Taking log on both sides:&lt;/p&gt;

\[\log{p(y|x)} = y\log{\hat{y}} + (1-y)\log{(1-\hat{y})}\]

&lt;p&gt;Equation (6) is what we are trying to maximize. In order to turn this into a loss function (something that we need to minimize), we just flip the sign on Equation $(6)$. The result is Cross Entropy Loss $L_{CE}$.&lt;/p&gt;

\[\begin{eqnarray}
L_{CE}(\hat{y}, y) &amp;amp;=&amp;amp; -\log{ p(y|x)} \nonumber \\

                   &amp;amp;=&amp;amp; -[y \log{\sigma(w \cdot x + b) + (1-y)\log(1-\sigma(w \cdot x + b))}]
\end{eqnarray}\]

&lt;p&gt;Equation (7) is known as the cross entropy loss. It is also the formula for the cross entropy between the true probability distribution $y$ and the estimated distribution $\hat{y}$.&lt;/p&gt;

&lt;h3 id=&quot;cross-entropy&quot;&gt;Cross Entropy&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;&quot;&gt;Cross Entropy&lt;/a&gt; is the measure of the difference between two probability distributions for a given random variable.&lt;/p&gt;

&lt;h3 id=&quot;convex-optimization-problem&quot;&gt;Convex Optimization Problem&lt;/h3&gt;
&lt;p&gt;For logistic regression the loss function is convex, i.e. it has just one minimum. There is no local minima to get stuck in, so gradient descent will always find the minimum. The loss for multi-layer neural networks in non-convex, so it is possible to get stuck in local minima using neural networks.&lt;/p&gt;

&lt;h3 id=&quot;decision-boundary&quot;&gt;Decision Boundary&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;&quot;&gt;Decision Boundary&lt;/a&gt; is the threshold above which $\hat{y} = 1$, and below which $\hat{y} = 0$. This means that the decision boundary decides which class a given instance belongs to, based on the final probability of the item and the value of the decision boundary.&lt;/p&gt;

\[\hat{y} = \left\{
    \begin{array}{ll}
        1 &amp;amp; \mbox{if $p(y=1|x)$ $\gt$ 0.5}\\
        0 &amp;amp; \mbox{otherwise}
    \end{array}
\right.\]

&lt;p&gt;Here 0.5 is the decision boundary, and it is the threshold that decides which class an item belongs to.&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;Gradient Descent&lt;/a&gt; finds the gradient of the loss function at the current point (by taking a differential of it), and then moves in the opposite direction of gradient.&lt;/p&gt;

&lt;h3 id=&quot;learning-rate&quot;&gt;Learning Rate&lt;/h3&gt;
&lt;p&gt;The magnitude of the amount to move in gradient descent is the slope $ \frac{d}{dw} \;  f(x,w)$ weighted by the learning rate $\eta$.&lt;/p&gt;

\[w^{t+1} = w^{t} - \eta \; \frac{d}{dw} \; f(x; w)\]

&lt;p&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;: A parameter that needs to be adjusted:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the learning rate is too high, each step towards learning becomes too large and it overshoots the minimum&lt;/li&gt;
  &lt;li&gt;If the learning rate is too low, each step is too small and it takes a long time to get to the minimum&lt;/li&gt;
  &lt;li&gt;Start $\eta$ at a higher value and slowly decrease it, so that it is a function of the iteration $k$ in training&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic Gradient Descent&lt;/h3&gt;
&lt;p&gt;It is called Stochastic Gradient Descent because it chooses a single random example at a time. It moves weights so that it can improve the performance of that single instance.&lt;/p&gt;

&lt;h3 id=&quot;batch-training&quot;&gt;Batch Training&lt;/h3&gt;
&lt;p&gt;When we use Batch Training, we compute the gradient over the entire dataset. It looks at all the examples for each iteration to decide the next step.&lt;/p&gt;

&lt;h3 id=&quot;mini-batch-training&quot;&gt;Mini-Batch Training&lt;/h3&gt;
&lt;p&gt;We train a group of $m$ examples (where $m$ is 512, or 1024 or similar) that is less than the size of the entire dataset. When $m = 1$ it becomes Stochastic Gradient Descent again. Mini-Batch Training is more efficient than Batch Training and more accurate than Stochastic Gradient Descent. Mini-Batch Gradient Descent is the average of the individual gradients.&lt;/p&gt;

&lt;h2 id=&quot;regularization&quot;&gt;Regularization&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Regularization_(mathematics)&quot;&gt;Regularization&lt;/a&gt; is used to avoid overfitting and to generalize well for the test data. If the model fits the training data too well, it will not be able to handle new cases presented in the test data.&lt;/p&gt;

&lt;p&gt;A regularization term $R(\theta)$ is added to the objective function. i.e. the function that computes $\hat{\theta}$, where $\hat{\theta}$ is the next set of $w$ and $b$ parameters.&lt;/p&gt;

&lt;p&gt;Once we add regularization, we can find $\hat{\theta}$ as:&lt;/p&gt;

\[\hat{\theta} = argmax_{\theta} \sum_{i=0}^m log(p(y^{(i)}\|x^{(i)})) - \alpha R(\theta)\]

&lt;p&gt;Here we are maximizing the log probability instead of minimizing the loss (Equation (7)).&lt;/p&gt;

&lt;p&gt;$R(\theta)$ is used to penalize large weights. The intuition behind this is that if the model matches the training data perfectly, but uses large weights, then it will be penalized more than the case where the model matches the training data a little less, but does so using small weights.&lt;/p&gt;

&lt;h3 id=&quot;l2-regularization&quot;&gt;L2 Regularization&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Tikhonov_regularization&quot;&gt;L2 Regularization&lt;/a&gt; is the euclidean distance of the vector squared from the origin.&lt;/p&gt;

&lt;p&gt;$ R(\theta) = || \theta ||_2^2$ Is the notation of L2 Norm&lt;/p&gt;

&lt;p&gt;$ R(\theta) = \sum_{j=1}^n \theta_j^2$ Is how we can compute L2 Norm&lt;/p&gt;

&lt;p&gt;L2 regularization is easy to optimize because of it’s simple derivative. It prefers weight vectors in smaller weights.&lt;/p&gt;

&lt;h3 id=&quot;l1-regularization&quot;&gt;L1 Regularization&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when&quot;&gt;L1 Regularization&lt;/a&gt; is the linear function named after L1 norm or Manhattan Distance, which is the sum of the absolute values of the weights.&lt;/p&gt;

&lt;p&gt;$R(\theta) = || \theta ||_1$ is the notation of L1 norm&lt;/p&gt;

&lt;p&gt;L1 Regularization is hard to differentiate as the derivative of $| \theta |$ is non continuous at 0. It prefers sparse solutions with larger weights.&lt;/p&gt;

&lt;h1 id=&quot;multinomial-logistic-regression&quot;&gt;Multinomial Logistic Regression&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_logistic_regression&quot;&gt;[Multinomial Logistic Regression](&lt;/a&gt;) is also known as Softmax Regression or Maxent Classifier. In Multinomial Logistic Regression the target variable $y$ ranges over more than two classes.&lt;/p&gt;

&lt;p&gt;In order to support more than 2 classes, Multinomial Logistic Regression uses the &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;&gt;softmax function&lt;/a&gt; instead of the sigmoid function.&lt;/p&gt;

&lt;p&gt;For a vector $Z$ of dimensionality $k$ the softmax function is defined as:&lt;/p&gt;

\[Softmax(Z_i) = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}\]

&lt;p&gt;In the above equation the denominator helps normalize the values into probabilities.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Like Sigmoid, Softmax has the property of squashing values towards 0 or 1&lt;/li&gt;
  &lt;li&gt;If one of the inputs is larger than the others, it will tend to push its probability towards 1, and suppress the probabilities of the smaller inputs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;loss-function-in-multinomial-logistic-regression&quot;&gt;Loss Function in Multinomial Logistic Regression&lt;/h2&gt;
&lt;p&gt;To compute the loss function in Multinomial Regression, we need to account for $k$ classes that a given item can belong to (instead of just 2 classes)
\(\begin{eqnarray}
L_{CE}(\hat{y}, y) &amp;amp;=&amp;amp; - \sum_{k=1}^k 1 \{y=k\} log p(y=k|x) \nonumber \\
&amp;amp;=&amp;amp; - \sum_{k=1}^k 1 \{y=k\} log \frac{e^{w_k \cdot x + b}}{\sum_{j=1}^k e^{w_j \cdot x + b_j}}
\end{eqnarray}\)&lt;/p&gt;

&lt;p&gt;In equation (11) $ 1{ }$ evaluates to 1 if the condition in the brackets is true, and $0$ otherwise.&lt;/p&gt;

&lt;h1 id=&quot;working-example-of-logistic-regression&quot;&gt;Working Example of Logistic Regression&lt;/h1&gt;
&lt;p&gt;Consider a simple scenario where we are doing sentiment analysis of a dataset, and we have 2 classes: positive and negative (there is no neutral class in our hypothetical scenario). In this example we will be using the sigmoid function (because we have only 2 classes, and for simplicity we will not be using any regularization.&lt;/p&gt;

&lt;p&gt;The following are the matrices representing the actual outcome $y$ and the features corresponding to it $x$.&lt;/p&gt;

&lt;p&gt;$y = \begin{pmatrix}y\\1 \\ 0\end{pmatrix} \;\; x = \begin{pmatrix}x1 &amp;amp; x2 \\ 3 &amp;amp; 2\\ 1 &amp;amp; 3\end{pmatrix}$&lt;/p&gt;

&lt;p&gt;In this example we have only 2 features $x_1$ and $x_2$. $x1$ is the number of positive words found in the sentence and $x2$ is the number of negative words found in the sentence. Also, $y=0$ represents negative (or bad sentiment), and $y=1$ represents positive (or good sentiment).&lt;/p&gt;

&lt;p&gt;This stage is feature extraction and representation of our data.&lt;/p&gt;

&lt;p&gt;The next state is to initialize out initial weights. For this example, we are setting 
$w_1 \; = \; w_2 \;= \; b \;= \; 0$.&lt;/p&gt;

&lt;p&gt;($w_1$ is the weight of feature $x_1$ and $w_2$ is the weight of feature $x_2$), $\beta$ is our bias term.&lt;/p&gt;

&lt;p&gt;$\eta \;= \; 0.1$&lt;/p&gt;

&lt;p&gt;Each step in learning for logistic regression is represented by the following formula:&lt;/p&gt;

\[\theta^{t+1} = \theta^t - \eta \; \delta_\theta \; L(f(x^{(i)}, \theta), y^{(i)})\]

&lt;p&gt;Here is a breakdown of each of the components of the formula.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/learning.png&quot; alt=&quot;Learning in Logistic Regression&quot; width=&quot;500&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 2: Each step for Learning in Logistic Regression&lt;/em&gt;
&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent-step-1&quot;&gt;Gradient Descent Step 1&lt;/h3&gt;
&lt;p&gt;Considering the first row, where $y=1$ finding the gradient for this example:&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} \frac{\partial L_{CE}(w, b)}{\partial_{w_1}} \\ \frac{\partial L_{CE}(w, b)}{\partial_{w_1}} \\ \frac{\partial L_{CE}(w, b)}{\partial_{b}} \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (\sigma (w \cdot x + b) - y)) \; x_1 \\ (\sigma (w \cdot x + b) - y)) \; x_2 \\ \sigma (w \cdot x + b) - y \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;We know that initial $w, b = 0$. Substituting these values:&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (\sigma (0) - 1)) \; x_1 \\ (\sigma (0) - y)) \; x_2 \\ \sigma (0) - y \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;Given that $\sigma(0) = 0.5$ (See the sigmoid image above) and $x_1 = 3$ and $x_2 = 2$ for the first example in matrix $x$ (first row in matrix)&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (0.5 - 1) * 3 \\ (0.5 - 1) * 2 \\ 0.5 - 1 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} -1.5 \\ -1.0 \\ -0.5 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;Now that we have the gradient, we can compute $\theta_1$ by moving in the opposite direction as the gradient.&lt;/p&gt;

&lt;p&gt;$ \theta_{1} = \begin{pmatrix} w_1 \\ w_2 \\ b \end{pmatrix} - \eta \begin{pmatrix} -1.5 \\ -1.0 \\ -0.5 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;Given that $w_1 = w_2 = b = 0$ and $\eta = 0.1$:&lt;/p&gt;

&lt;p&gt;$ \theta_{1} = \begin{pmatrix} 0.15 \\ 0.1 \\ 0.05 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;The weights after one step of gradient descent: $w_1 = 0.15$, $w_2 = 0.1$, $b = 0.05$.&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent-step-2&quot;&gt;Gradient Descent Step 2&lt;/h3&gt;
&lt;p&gt;This time we will consider row 2 of our matrix. The weights learned so far are : $w_1 = 0.15$, $w_2 = 0.1$, $b = 0.05$
The values from our matrix are $x_1 = 1$, $x_2=3$, $y=0$.&lt;/p&gt;

&lt;p&gt;From Equation (9) computing the gradient:&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (\sigma (w \cdot x + b) - 0)) \; x_1 \\ (\sigma (w \cdot x + b) - 0)) \; x_2 \\ \sigma (w \cdot x + b) - y \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (\sigma (0.15 * 1 + 0.1 * 3 + 0.05) - 0) \; 1 \\ (\sigma (0.15 * 1 + 0.1 * 3 + 0.05) - 0) \; 3 \\ \sigma (0.15 * 1 + 0.1 * 3 + 0.05) - 0 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (\sigma (0.15 + 0.3 + 0.05) - 0) \; 1 \\ (\sigma (0.15 + 0.3 + 0.05) - 0) \; 3 \\ \sigma (0.15 + 0.3 + 0.05) - 0 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;Sigmoid of 0.5 is 0.622. (&lt;a href=&quot;https://keisan.casio.com/exec/system/15157249643325&quot;&gt;Compute your own sigmoid&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (0.622 - 0) \; 1 \\ (0.622 - 0) \; 3 \\ 0.622 - 0 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} 0.378 \\ 1.134 \\ 0.378 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;Now that we have the gradient, we can compute $\theta_2 $ by moving in the opposite direction as the gradient.&lt;/p&gt;

&lt;p&gt;$ \theta_{2} = \begin{pmatrix} w_1 \\ w_2  \\ b \end{pmatrix} - \eta \begin{pmatrix} 0.378 \\ 1.134 \\ 0.378 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \theta_{2} = \begin{pmatrix} 0.15 \\ 0.1  \\ 0.05 \end{pmatrix} - 0.1 \begin{pmatrix} 0.378 \\ 1.134 \\ 0.378 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \theta_{2} = \begin{pmatrix} 0.15 - 0.0378 \\ 0.1 - 0.1134 \\ 0.05 - 0.0378 \end{pmatrix} $&lt;/p&gt;

&lt;p&gt;$ \theta_{2} = \begin{pmatrix} 0.1122 \\ -0.0134 \\ 0.4622 \end{pmatrix} $&lt;/p&gt;

&lt;p&gt;At the end of step 2, $w_1 = 0.1122$, $w_2 = -0.0134$ and $b = 0.04622$.&lt;/p&gt;

&lt;p&gt;We can continue this process for $k$ number of steps and iterate through the examples again and again till we find the global minimum.&lt;/p&gt;

&lt;h1 id=&quot;summary-of-logistic-regression&quot;&gt;Summary of Logistic Regression&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Each input is composed of a vector $x_1, x_2 \ldots x_n$&lt;/li&gt;
  &lt;li&gt;We compute $\hat{y} = \sigma(w \cdot x + b)&lt;/li&gt;
  &lt;li&gt;Compute loss = $ \hat{y} - y$. We use cross entropy loss to compute this value&lt;/li&gt;
  &lt;li&gt;Compute the gradient of the loss = $\frac{d}{dc}L_{CE}$&lt;/li&gt;
  &lt;li&gt;Sigmoid is replaced by Softmax when we do multinomial Logistic Regression&lt;/li&gt;
  &lt;li&gt;Regularization is used to avoid overfitting and make the model more generalized&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf&quot;&gt;On discriminative vs. generative classifiers: a comparison of logistic regression and naive Bayes&lt;/a&gt; NIPS’01: Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, January 2001 Pages 841–848&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/5.pdf&quot;&gt;Chapter 5, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition&lt;/a&gt; by Daniel Jurafsky, James H. Martin&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/&quot;&gt;Introduction to Latent Dirichlet Allocation&lt;/a&gt;, by Edwin Chen&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 23 Apr 2021 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/2021/logistic-regression/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2021/logistic-regression/</guid>
        
        <category>nlp</category>
        
        <category>classification</category>
        
        
        <category>nlp</category>
        
        <category>classification</category>
        
      </item>
    
      <item>
        <title>What is Byte-Pair Encoding for Tokenization?</title>
        <description>&lt;p&gt;Tokenization is the concept of dividing text into tokens - words (unigrams), or groups of words (n-grams) or even characters. 
Morphology traditionally defines morphemes as the smallest semantic unit. e.g. The word &lt;strong&gt;Unfortunately&lt;/strong&gt; can be broken down as &lt;strong&gt;un - fortun - ate - ly&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[[un [[fortun(e) ]\(_{ROOT}\) ate]\(_{STEM}\)]\(_{STEM}\) ly]\(_{WORD}\)&lt;/p&gt;

&lt;p&gt;Morphology is little studied with deep learning, but Byte Pair Encoding is a way to infer morphology from text. Byte-pair encoding allows us to define tokens automatically from data, instead of precpecifying character or word boundaries. This is especially useful in dealing with unkown words.&lt;/p&gt;

&lt;h2 id=&quot;modern-tokenizers&quot;&gt;Modern Tokenizers&lt;/h2&gt;

&lt;p&gt;Modern tokenizers often automatically induce tokens that include tokens smaller than words - called &lt;strong&gt;Subwords&lt;/strong&gt;. E.g. the subwords “-ly”, “-ing” give us an ideal about the type of the word - which is what subword tokenization aims to do.&lt;/p&gt;

&lt;p&gt;Most tokenizers have two parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;A token learner:&lt;/strong&gt; takes a raw training corpus and indices a vocabulary - a set of tokens.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;A token segmenter:&lt;/strong&gt; takes a raw test sentence and segments it into the tokens in the vocabulary.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Three algorithms are widely used :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Byte Pair Encoding (Sennrick et. al 2016)&lt;/li&gt;
  &lt;li&gt;Unigram Language Modeling (Kudo 2018)&lt;/li&gt;
  &lt;li&gt;Wordpiece (Schuster and Nakajima 2012) and Sentencepiece (Kudo and Richardson, 2018)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Byte Pair Encoding (BPE) is the simplest of the three.&lt;/p&gt;

&lt;h2 id=&quot;byte-pair-encoding-bpe-algorithm&quot;&gt;Byte Pair Encoding (BPE) Algorithm&lt;/h2&gt;

&lt;p&gt;BPE runs within word boundaries. &lt;strong&gt;&lt;em&gt;BPE Token Learning&lt;/em&gt;&lt;/strong&gt; begins with a vocabulary that is just the set of individual characters (tokens). It then runs over a training corpus ‘k’ times and each time, it merges 2 tokens that occur the most frequently in text. e.g. ‘e’ and ‘r’ are merged into a single token ‘er’ when they occur together in the same order.&lt;/p&gt;

&lt;p&gt;At the end of ‘k’ iterations, the algorithm produces a list of most frequent ‘k’ tokens along with the original set of characters.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;byte_pair_encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
	 &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;characters&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
	 &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	 	&lt;span class=&quot;n&quot;&gt;c_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;most&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frequent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;adjacent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_list&lt;/span&gt;
	 	&lt;span class=&quot;n&quot;&gt;c_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_right&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# create a new bigram
&lt;/span&gt;	 	&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_new&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# add the bigram to teh vocabulary
&lt;/span&gt;	 	&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;occurence&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_right&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_new&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# update the corpus
&lt;/span&gt;	 &lt;span class=&quot;nf&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once the &lt;strong&gt;token learner&lt;/strong&gt; learns the vocabulary, the &lt;strong&gt;token parser&lt;/strong&gt; is used to tokenize a test sentence from teh learned tokens that were leraned from teh training data.&lt;/p&gt;

&lt;p&gt;In real applications of BPE algorithms BPE is run with many thousands of merges such that most words are represented as tokens and only the rare words are represented by their parts.&lt;/p&gt;

&lt;p&gt;Byte-Pair Encoding was originally a compression algorithm where we replace the most frequent byte pair with a new byte - thereby compressing the data.&lt;/p&gt;

&lt;p&gt;For further reading check out this &lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture12-subwords.pdf&quot;&gt;NLP class slides from Stanford&lt;/a&gt; or this &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/2.pdf&quot;&gt;chapter on Text Normalization from the Jurafsky and Martin Textbook&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;T. Kudo. Subword Regularization: &lt;a href=&quot;https://arxiv.org/pdf/1804.10959.pdf&quot;&gt;Improving Neural Network Translation Models with Multiple Subword Candidates&lt;/a&gt;. 2018&lt;/li&gt;
  &lt;li&gt;T. Kudo and J. Richardson. &lt;a href=&quot;https://arxiv.org/pdf/1808.06226.pdf&quot;&gt;SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing&lt;/a&gt;. 2018&lt;/li&gt;
  &lt;li&gt;M. Schuster and K. Nakajima. &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37842.pdf&quot;&gt;Japanese and Korea Voice Search&lt;/a&gt;. 2012&lt;/li&gt;
  &lt;li&gt;R. Sennrich, B. Haddow and A. Birch. &lt;a href=&quot;http://aclweb.org/anthology/P16-1162&quot;&gt;Neural Machine Translation of Rare Words with Subword Units&lt;/a&gt;. ACL 2016&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 28 Jan 2021 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/2021/byte-pair-encoding/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2021/byte-pair-encoding/</guid>
        
        <category>tokenization</category>
        
        
        <category>tokenization</category>
        
      </item>
    
      <item>
        <title>Willpower</title>
        <description>&lt;p&gt;In this book &lt;a href=&quot;https://roybaumeister.com/&quot;&gt;Roy Baumeister&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/John_Tierney_(journalist)&quot;&gt;John Tierney&lt;/a&gt; talk about Willpower, why it is important, how successful people have implemented the power of willpower in their own lives, and how to build and practice our own muscle of “willpower”.&lt;/p&gt;

&lt;p&gt;Success has attributed to 2 qualities: intelligence and self control. Researchers have not been able to permanently increase intelligence, but they have discovered how to improve self-control.&lt;/p&gt;

&lt;p&gt;Most major problems center in self control - e.g. compulsive spending, impulsive violence, unhealthy diet etc.&lt;/p&gt;

&lt;p&gt;People spend about a quarter of their waking hours resisting desires - at least 4 hours per day. And that doesn’t include the instances in which willpower is exercised.&lt;/p&gt;

&lt;p&gt;Every successful person has the quality of self-control, but it is rarely ever mentioned as a reason for success (It is almost always mentioned as the reason for failure).&lt;/p&gt;

&lt;p&gt;In this book Baumeister and Tierney talk about how people who have mastered self control are more successful, in all aspects of their life as compared to their counterparts.&lt;/p&gt;

&lt;p&gt;Ultimately self-control lets you relax because it removes stress and enables you to conserve willpower for the important challenges.&lt;/p&gt;

&lt;h3 id=&quot;is-willpower-more-than-a-metaphor&quot;&gt;Is willpower more than a metaphor?&lt;/h3&gt;

&lt;p&gt;“Ego Depletion” - Baumeister’s term for describing people’s diminished capacity to regulate their thoughts, feelings and actions.&lt;/p&gt;

&lt;p&gt;When the brain was wired up with an EEG to monitor the anterior cingulate cortex (or the conflict monitoring system or error detection system), it was discovered that people who act against what they actually feel, face ego depletion and are not able to do subsequent tasks correctly. E.g. 50% of the people were showed a sad movie but were asked to suppress their emotions. These people went on to not being able to do unrelated tasks properly after the movie, compared to their counterparts who did not go through ego-depletion.&lt;/p&gt;

&lt;p&gt;Studying thousands of people inside and outside a laboratory, experiments consistently demonstrated:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;you have a finite amount of willpower that becomes depleted as you use it&lt;/li&gt;
  &lt;li&gt;you use the same stock of willpower for all manner of tasks&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can divide the uses of willpower into 4 broad categories:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;control of thoughts: earworms such as a song stuck in your head, to ways in which we deceive ourselves (e.g. Tiger woods convinced himself that monogamy didn’t apply to him, bankers convinced themselves that there was no problem issuing loans to people having no income and no assetts)&lt;/li&gt;
  &lt;li&gt;control of emotions: controlling feeings - e.g. trying to escape bad moods&lt;/li&gt;
  &lt;li&gt;impulse control: the ability to resist temptations like alcohol, tobacco, Cinnabons and cocktail waitresses&lt;/li&gt;
  &lt;li&gt;performance control: Focusing your energy on the task at hand&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are trying to make a big change in your life that requires willpower - make sure to focus only on that change. The limited willpower that one has is used for all aspects of their life - no matter how unrelated. e.g. quitting alcohol, saying good things about bullys - both share the same willpower resources.&lt;/p&gt;

&lt;p&gt;Willpower isn’t just a metaphor. There is power driving this virtue.&lt;/p&gt;

&lt;h3 id=&quot;where-does-the-power-in-willpower-come-from&quot;&gt;Where does the power in Willpower come from?&lt;/h3&gt;

&lt;p&gt;There is a very strong correlation between glucose in the body and willpower. The more glucose that is converted from the bloodstream and pumped into the brain, the higher is the person’s willpower. No glucose, no willpower.&lt;/p&gt;

&lt;p&gt;When people have more demands for self-control in their daily life, their hunger for sweets increases. Not hunger for food in general, but the hunger for sweets.&lt;/p&gt;

&lt;p&gt;Don’t try to skimp on calories when working on serious problems (more serious than being overweight). Sugar can be used effectively to boost your self control right before a brief challenge - like a math test.&lt;/p&gt;

&lt;p&gt;To get your glucose, go for the “slow burn” - i.e. food that takes a while to digest (low glycemic index) and can sustain you got a longer time.&lt;/p&gt;

&lt;p&gt;Sleep also affect how effective the body is to create glucose. Which is why having a good nights sleep is beneficial for willpower.&lt;/p&gt;
</description>
        <pubDate>Sun, 12 Jan 2020 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/2020/willpower/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2020/willpower/</guid>
        
        
      </item>
    
      <item>
        <title>The Hard Thing about Hard Things</title>
        <description>&lt;p&gt;This book is one of the first books to read as an entrepreneur. &lt;a href=&quot;https://en.wikipedia.org/wiki/Ben_Horowitz&quot;&gt;Ben Horowitz&lt;/a&gt; candidly talks about the struggles, and hard things about starting, running and growing companies. Here is a &lt;a href=&quot;https://techcrunch.com/2014/03/03/the-hard-thing-about-hard-things-ben-horowitzs-honest-and-real-take-on-entrepreneurship/&quot;&gt;Techcrunch article&lt;/a&gt; reviewing this book.&lt;/p&gt;

&lt;h3 id=&quot;the-beginning&quot;&gt;The beginning&lt;/h3&gt;

&lt;p&gt;A bully told Ben to ask another kid for his wagon, and if the other kid refuses, to spit in his face and call him profanity. Ben reluctantly went to the other kid and politely asked to play with his wagon, and the other kid obliged. This is how Ben met his best friend - by doing something uncomfortable. It taught him not to judge things by their surfaces, and that there are no shortcuts to anything.&lt;/p&gt;

&lt;p&gt;Some tidbits of learning:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The most important rule of raising money privately: Look for a market of one. You only need one investor to say yes, so it is better to ignore the other thirty who say “no”&lt;/li&gt;
  &lt;li&gt;The best things about startups is that you only ever experience two emotions - euphoria and terror, and the lack of sleep enhances them both.&lt;/li&gt;
  &lt;li&gt;You need 2 kinds of friends in your life - the one you can call when things are great and who can be happy for you, and one you can call when things are going bad and who is your lifeline in that bad time.&lt;/li&gt;
  &lt;li&gt;Have mentors who have gone though some horrible and devastating circumstances so that they can advice you how to avoid them&lt;/li&gt;
  &lt;li&gt;Startup CEOs should not play the odds. WHen you are building a company, you must believe that there is an answer and you cannot pay attention to your odds of finding it.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;when-things-fall-apart&quot;&gt;When things fall apart&lt;/h3&gt;

&lt;p&gt;The secret to being a successful CEO is the ability to focus and make the best move when there are no good moves.&lt;/p&gt;

&lt;p&gt;Struggle in your company is not failure - but it often causes failure - if you are not strong enough to endure it.&lt;/p&gt;

&lt;p&gt;How to deal with struggle:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Divide and conquer - seek help, don’t put it all on your own shoulders&lt;/li&gt;
  &lt;li&gt;Don’t underestimate the complexity - this is chess not checkers.&lt;/li&gt;
  &lt;li&gt;Play long enough and you might get lucky&lt;/li&gt;
  &lt;li&gt;Do not take it personally&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CEOs should tell it like it is&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ignoring the negatives does not pain a realistic picture and makes you appear less trustworthy&lt;/li&gt;
  &lt;li&gt;If there is a problem, it is better for the team to find a solution together, rather that the CEO to shelter the team from it&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you ever need to fire your employees, executives, or demote a friend - do it the right way.&lt;/p&gt;

&lt;p&gt;When you are in a trouble, nobody really cares - not employees, customers, media or investors. Your time is best spent on figuring out a solution, thank worrying about what anyone thinks of you.&lt;/p&gt;

&lt;h3 id=&quot;take-care-of-the-people-the-products-and-the-profits---in-that-order&quot;&gt;Take care of the people, the products and the profits - in that order&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;when hiring executives, hire for strength rather than lack of weakness.&lt;/li&gt;
  &lt;li&gt;If you are looking for a genius who might not be the best cultural fit, know what you are getting into, and hire them for the genius that they bring to the team&lt;/li&gt;
  &lt;li&gt;Being a good company doesn’t matter when things go well, but it can be the difference between life and death when things go wrong. Things always go wrong!&lt;/li&gt;
  &lt;li&gt;It is important to train your employees
    &lt;ul&gt;
      &lt;li&gt;It improves productivity&lt;/li&gt;
      &lt;li&gt;Sets expectations&lt;/li&gt;
      &lt;li&gt;Improves quality and throughput&lt;/li&gt;
      &lt;li&gt;Increases employee retention as they will strive hard to reach specific targets&lt;/li&gt;
      &lt;li&gt;Best place to start is management and new employees&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Good Product Managers&lt;/th&gt;
      &lt;th&gt;Bad Product Managers&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Know the marker, product, product line and competition extremely well&lt;/td&gt;
      &lt;td&gt;Work with excuses - not enough funding, too much competition etc.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Crisply define the target and the “what”&lt;/td&gt;
      &lt;td&gt;Feel good about themselves when they figure out the “how”&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Create collateral such as FAQs presentations&lt;/td&gt;
      &lt;td&gt;Complain about spending their time answering questions&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Take written positions on important issues&lt;/td&gt;
      &lt;td&gt;Use verbal communication&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Focus the team on revenue and customers&lt;/td&gt;
      &lt;td&gt;Focus the team on how many features the competitors are building&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Decompose problems into segments&lt;/td&gt;
      &lt;td&gt;Combine all problems into one&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Think about the story they want to be written in the press&lt;/td&gt;
      &lt;td&gt;Think about covering every feature and being technically accurate with the press&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Articulate simply and clearly&lt;/td&gt;
      &lt;td&gt;Never explain the obvious&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Disciplined execution of status reports&lt;/td&gt;
      &lt;td&gt;Don’t value discipline&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Should you hire your friends employees?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;probably not, if you don’t want to do the same to you&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Issues with hiring big company executives:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Rhythm mismatch - exec is used to waiting for incoming communication rather than taking action&lt;/li&gt;
  &lt;li&gt;Skill-set mismatch - Large company execs are good at managing complex organizations, process improvement etc, which isn’t quite where a smaller organization skill requirement is.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These problems can be allayed by screening for mismatches early on and asking questions like:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;what will you do in your first month on the job&lt;/li&gt;
  &lt;li&gt;how will your new job differ from your current job&lt;/li&gt;
  &lt;li&gt;why do you want to join a small company&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once a candidate is on board, aggressively integrate them in your company by doing the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;force them to create and produce immediately&lt;/li&gt;
  &lt;li&gt;make sure they “get it”&lt;/li&gt;
  &lt;li&gt;make sure they connect with and initiate connections with everyone in the organization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While hiring executives:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Know what you want&lt;/li&gt;
  &lt;li&gt;Run a process the figures out the right match&lt;/li&gt;
  &lt;li&gt;Sometimes make the lonely decision that a CEO has to make&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Like technical debt, be careful of management debt. Here are a few ways in which management debt can arise:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Putting two people together to manage&lt;/li&gt;
  &lt;li&gt;Overcompensation a key employee because they get another job offer&lt;/li&gt;
  &lt;li&gt;No performance management of employee feedback process&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;concerning-the-growing-concern&quot;&gt;Concerning the growing concern&lt;/h3&gt;

&lt;p&gt;Sometimes an organization doesn’t need a solution, it just needs clarity. E.g. Ben’s employees were uncomfortable with the amount of profanity used at the company. He offered an explanation and suggestion to use it in the right context of work and urgency, and not to intimidate or sexually harass anymore. He compared it to the word “cupcake”. It is okay to say - “Those cupcakes you baked were delicious”, but it is not okay to say “Hey cupcakes, you look fine in those jeans. “&lt;/p&gt;

&lt;p&gt;No one likes corporate politics, however it often unintentionally develops in an organization by sometimes incentivizing political behavior (unwittingly). E.g. if a senior executive requests a pay raise, with a competitive offer at hand, a CEO might offer them additional compensation to have them stay. Although it might seem innocent, this is strong incentive for political behavior as the CEO is rewarding behavior that has nothing to do with advancing the business. This is bad because:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;others in the staff would like to ask for raises as well and this doesn’t seem performance driven but arbitrary&lt;/li&gt;
  &lt;li&gt;less aggressive members might be denied off cycle raises&lt;/li&gt;
  &lt;li&gt;people start optimizing over total compensation rather than growth of the product&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;How to minimize politics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hire people with the right kind of ambition and not necessarily political.&lt;/li&gt;
  &lt;li&gt;Build strict process for potentially political issues and do not deviate&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Smart people are sometimes bad employees:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;when they are disempowereed&lt;/li&gt;
  &lt;li&gt;when they are fundamentally a rebel&lt;/li&gt;
  &lt;li&gt;when they are immature/naive&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hiring senior people (older people - who have been there) to your startup is like taking performance enhancing drugs. If all goes well, you will achieve incredible new heights. If all goes wrong, you will start degenerating from the inside out.&lt;/p&gt;

&lt;p&gt;It is challenging to hire senior employees:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;they come with their own culture&lt;/li&gt;
  &lt;li&gt;they know how to work the system&lt;/li&gt;
  &lt;li&gt;you do not know the job as well as they do&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is important to have effective one on ones with employees. Here are a few questions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if we could improve in any way, how could we do it?&lt;/li&gt;
  &lt;li&gt;what is the #1 problem with our organization? why?&lt;/li&gt;
  &lt;li&gt;what is not fun about working here?&lt;/li&gt;
  &lt;li&gt;who is really kicking ass in the company? who do you admire?&lt;/li&gt;
  &lt;li&gt;if you were me, what changes would you make?&lt;/li&gt;
  &lt;li&gt;what don’t you like about the product?&lt;/li&gt;
  &lt;li&gt;what is the biggest opportunity that we are missing out on?&lt;/li&gt;
  &lt;li&gt;what are we not doing that we should be doing?&lt;/li&gt;
  &lt;li&gt;are you happy working here?&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;When you are building a company and a product, you should have the following covered:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;You should be at least 10 times better than the competition (one 2 or 3 times better)&lt;/li&gt;
    &lt;li&gt;You should take the market before anyone else does&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you are in the right direction with your product and market, company culture is the next important thing because it:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;distinguishes you from the competitors&lt;/li&gt;
  &lt;li&gt;ensures and defines operating values&lt;/li&gt;
  &lt;li&gt;helps identify employees who fit with your mission&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;E.g. Amazon has frugality as their culture as they made employees use cheap doors from Home Depot as desks. 
E.g. Facebook has the motto of “Move fast and break things”, which make people stop and realize that they need to move fast and innovate, which might lead to breaking things - which is accepted.&lt;/p&gt;

&lt;h3 id=&quot;how-to-lead-even-when-you-dont-know-where-you-are-going&quot;&gt;How to lead even when you don’t know where you are going&lt;/h3&gt;

&lt;p&gt;As an entrepreneur one needs to stop focusing on the things you did wrong, or might do wrong.&lt;/p&gt;

&lt;p&gt;The most difficult skill to learn as a CEO is the ability to manage their own psychology. Everything else is relatively straightforward as compared to keeping the mind in check.&lt;/p&gt;

&lt;p&gt;As  CEO - nothing prepares you to run a company (not being a manager, etc). You learn how to run a company, by running a company.&lt;/p&gt;

&lt;p&gt;If CEOs were graded on a curve, the mean on the test would be 22 out of 100. This is because:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;there is always broken stuff, and CEOs either take it too personally, or not personally enough&lt;/li&gt;
  &lt;li&gt;It is a lonely job being a CEO&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Great CEOs face the pain, and the torture. Great CEos don’t quit.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;There is no difference in how you feel when you are a hero or a coward. The fear is the same. The only difference is what you do with that fear that makes you either a hero, or a coward.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Great CEOs need to trust themselves and might have to go against the wishes of their team - for hiring, etc. for the benefit of the company.&lt;/p&gt;

&lt;p&gt;There are 2 types of CEOs in the world:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ONES - managers who are happier setting the direction of the company
    &lt;ul&gt;
      &lt;li&gt;most founding CEOs tend to be ones&lt;/li&gt;
      &lt;li&gt;enjoy making decisions&lt;/li&gt;
      &lt;li&gt;strategic minds and enjoy the 8 dimensional chess with their competitors&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;TWOS - those who enjoy making the company perform at the highest level
    &lt;ul&gt;
      &lt;li&gt;enjoy the process of making the company run well&lt;/li&gt;
      &lt;li&gt;often have difficulty with the strategic process itself&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A good CEO needs to have the qualities of ONES and TWOS. Most organizations are run by ONES and have a team of TWOS.&lt;/p&gt;

&lt;p&gt;What makes people follow a leader?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The ability to articulate the vision&lt;/li&gt;
  &lt;li&gt;The right kind of ambition&lt;/li&gt;
  &lt;li&gt;The ability to achieve the vision&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are 2 modes of being a CEO:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;when facing an imminent threat - Wartime CEO&lt;/li&gt;
  &lt;li&gt;when it is business as usual - peacetime CEO
It is an art to be able to be a wartime and peacetime CEO.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It takes a long time to develop CEO skillset, and it is hard to tell whether someone can develop it or not.&lt;/p&gt;

&lt;p&gt;How to give feedback as a CEO:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;be authentic&lt;/li&gt;
  &lt;li&gt;some from the right place (you want them to succeed)&lt;/li&gt;
  &lt;li&gt;don’t get personal&lt;/li&gt;
  &lt;li&gt;don’t embarrass anyone in front of their peers&lt;/li&gt;
  &lt;li&gt;feedback is not one size-fits all&lt;/li&gt;
  &lt;li&gt;be direct, but not mean&lt;/li&gt;
  &lt;li&gt;feedback is a dialog, not a monologue&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;first-rule-of-entrepreneurship---there-are-no-rules&quot;&gt;First rule of entrepreneurship - there are no rules&lt;/h3&gt;

&lt;p&gt;In technology business you rarely know everything upfront, and letting people be creative can be the difference between magical and mediocre.&lt;/p&gt;

</description>
        <pubDate>Sat, 11 Jan 2020 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/2020/the-hard-thing-about-hard-things/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2020/the-hard-thing-about-hard-things/</guid>
        
        
      </item>
    
      <item>
        <title>How to Win Friends and Influence People</title>
        <description>&lt;p&gt;This book is a timeless classic by &lt;a href=&quot;https://en.wikipedia.org/wiki/Dale_Carnegie&quot;&gt;Dale Carnegie&lt;/a&gt;. In this book Carnegie goes into great depth about how to win hearts of everyone around you - employees, employers, students, mentors, friends, spouses, children etc. Carnegie provides countless examples and specific action to be taken to practice a lifestyle that involves winning friends and influencing people.&lt;/p&gt;

&lt;h3 id=&quot;fundamental-techniques-in-handling-people&quot;&gt;Fundamental Techniques in Handling People&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1. Don’t criticize, condemn, or complain:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Criticism is futile and it makes the opposite person on the defensive and make them strive to justify themselves. Any fool can criticize, condemn and complain- and most fools do. But it takes character and self-control to be understanding and forgiving.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Give honest and sincere appreciation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There is only one way to get anyone to do anything - by making them want to do it. There is no other way. 
Dr. John Dewey said that the deepest urge in human nature is “the desire to be important”. This desire to be important makes poverty-stricken grocery clerk to study law books (Lincoln), or for Dickens to be inspired to write his immortal novels.&lt;/p&gt;

&lt;p&gt;Charles Schwab was the first person in America to be paid a million dollars a year - because of his ability to deal with people. Schwab considered his greatest asset to be his ability to arouse enthusiasm among his people. The way he did it was with appreciation and encouragement.&lt;/p&gt;

&lt;p&gt;All humans need a nourishing dose of self esteem. And providing others this nourishment of and nurturing in the form of kind words of appreciation is greatly accepted and appreciated.&lt;/p&gt;

&lt;p&gt;Honest appreciation is not to be confused with flattery. Flattery is shallow, selfish and insincere.&lt;/p&gt;

&lt;p&gt;True appreciation is honest, and is sincere, without expecting anything in return but a smile from the recipient and to make someones day better.&lt;/p&gt;

&lt;p&gt;People crave the feeling of importance. Make someone feel important and they will think well of you. Diminish someone’s importance and they will resent you.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Arouse in the other person an eager want&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Appeal to the other person’s interests. Virtually all people care more about what they want than what you want. You wouldn’t go fishing with cheesecake as a lure, since fish don’t like cheesecake. Go fishing with worms.&lt;/p&gt;

&lt;p&gt;Keep asking yourself - “what is it that this person wants?”
Everyone has something they can teach you, and you benefit by figuring out what that is. This belief leads to a genuine interest and appreciation for other people.&lt;/p&gt;

&lt;p&gt;Angry people are often angry because they feel unheard. Once you sympathize with them, they will soften their anger substantially.&lt;/p&gt;

&lt;p&gt;If there is one secret to success, it lies in the ability to get the other persons point of view and see things from that person’s angle as well as your own.&lt;/p&gt;

&lt;p&gt;The world is so full of people who are grabbing and self-seeking, that an unselfish individual who is willing to help others just to be helpful has a huge advantage and no competition.&lt;/p&gt;

&lt;h3 id=&quot;six-ways-to-make-people-like-you&quot;&gt;Six Ways to Make People Like You&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;1. Become genuinely interested in other people&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It is interesting to see that the dog is the only animal that does not “work” - unlike hen (lays eggs), cow (gives milk), canary (sings). Dogs make a living nothing but love.&lt;/p&gt;

&lt;p&gt;You can make more friends in 2 months by being interested in other people that you can in 2 years by trying to get other people interested in you.&lt;/p&gt;

&lt;p&gt;Everyone loves to be listened to and be appreciated. When we start with the recipient and their needs and wants, there is a much bigger and better chance for them to be amiable and warm and kind to you.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Smile&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Smiling makes a good first impression. If you don’t feel like smiling, force yourself to smile (the action of smiling brings out the feeling of happiness and makes one want to smile more.)&lt;/p&gt;

&lt;p&gt;Everyone in the world is looking for happiness, and the sure way to find it is by looking inwards and finding that in your heart.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. A person loves their name.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A person’s name is the most important word in any language to them. Use it often and respect it. 
Remember a name and call it easily, and you have paid an effective compliment to the recipient (that their name is worth remembering), forget it or misspell it and you have placed yourself at a sharp disadvantage.&lt;/p&gt;

&lt;p&gt;Andrew Carnegie remembered and honored many of his workers name, because of which there was never a strike or an issue under his leadership. All the employees felt loved and heard.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Be a good listener. Encourage others to talk about themselves&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When people are heard, it helps dissolve anger and helps them feel heard. Many a times just listening to a disgruntled customer helps deescalate the situation.&lt;/p&gt;

&lt;p&gt;Being a good listener is also a good way of becoming a good conversationalist. The more you listen to someone, the more they feel heard and important, and the more they enjoy your company.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. Talk in terms of the other person’s interests&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If your recipient is interested in books, starting a conversation with that interest is a great ice-breaker, and establishes a wonderful common ground to setup a communication pattern/style.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6. Make the other person feel important - and do it sincerely&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Doing this will make people like you instantaneously. Everyone has admirable qualities, and finding those and highlighting them - without having an agenda to sell to them - but only focusing on making them happy - will bring you a long way in building a personal connection with them. 
This approach could be used with someone you have just met, or others who you have known for years.&lt;/p&gt;

&lt;p&gt;Emerson said - “Every man that I meet is mu superior in some way. In that, I learn of him.” Finding that superior quality and highlighting it with honestly is a wonderful quality that makes people like you instantaneously.&lt;/p&gt;

&lt;h3 id=&quot;how-to-win-people-to-your-way-of-thinking&quot;&gt;How to Win People to Your Way of Thinking&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. The only way to get the best of an argument is to avoid it.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you argue and rankle and contradict, you may achieve a victory sometimes, but it will be an empty victory because you will never get your opponent’s good will.&lt;/p&gt;

&lt;p&gt;Buddha said - Hatred is never ended by hatred but by love. And a misunderstanding is never ended by argument but by tact, diplomacy, conciliation and a sympathetic desire to see the other person’s point of view.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Show respect to the other person’s opinions. Never say, “You’re wrong”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Instead, approach with an open-minded view: “I may be wrong. I often am. And if I’m wrong, I want to change and be right. Let’s discuss the facts.”
Praise the other person for a trait that will help resolve the argument - like their patience, open-mindedness, fairness, and receptivity to new facts.&lt;/p&gt;

&lt;p&gt;Understand that the other person has a valid view of the situation. If you were born as them with their brain and undergoing their experiences, you would by definition feel the same way they do. Your job is to understand what led them to believe what they believe.
Express sympathy for their situation. “You have the absolute right to be upset. If I were in your shoes, I would be too.”&lt;/p&gt;

&lt;p&gt;To influence people to do things, praise and appreciation are more effective than orders.
Don’t start by criticizing or complaining. This makes them defensive and rationalize their actions. Instead, praising them lowers their defenses, and they’ll be more receptive to your feedback.&lt;/p&gt;

&lt;p&gt;It is important to be diplomatic and look at all points of view.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. If you are wrong, admit it quickly and emphatically&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Humility and acceptance of being wrong is a huge virtue. It readily disarms the recipient and moves the conversation from “who is right”, to “how do we fix this”? This strategy readily brings people together, and often brings the opposite person to your rescue by dismissing your mistakes as - “not that big”, and “easy to fix”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Begin in a friendly way&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Starting any conversation with anger or antagonism will only make the recipient feel a similar level of anger. Beginning a tough conversation in a friendly way prompts the recipient to match your disposition, making it much more conducive to reach an agreement quickly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. Get the other person saying “yes” immediately&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When talking to people, begin with topics and things that you know you agree on. Keep emphasizing that you both have the same goals, and the same destination, and just a slight difference in method to get to the goal.&lt;/p&gt;

&lt;p&gt;Getting listeners to a “yes” quickly gets their psychological process moving in the affirmative direction.&lt;/p&gt;

&lt;p&gt;It takes a lot of energy and effort to convert a “no” into a “yes” (even if it is about different topics). But starting with “yes” and agreeing on topics makes it easier to agree on other topics.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6. Let the other person do a great deal of the talking&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Listen first. Give your opponents a chance to talk through. Do NOT interrupt as they’re speaking.&lt;/p&gt;

&lt;p&gt;Ask people where they feel the problems are. Ask for their opinions on how best to proceed. Ask lots of questions instead of stating commands.&lt;/p&gt;

&lt;p&gt;When people are talking, they can often talk themselves out of the negatives, and focus on the positives.&lt;/p&gt;

&lt;p&gt;Letting others talk always leads to a more favorable outcome as compared to forcing them to listen to you and follow your direction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7. Let the other person feel that the idea is his or hers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A manager of an automobile company asked his sales people what qualities they expected out of him, and wrote them out. He then asked them what qualities he ought to expect out of them - to which they readily listed qualities they should have. The manager made the sales people come up with their own goals for qualities they should possess - which made them want to achieve it because it was their own idea.&lt;/p&gt;

&lt;p&gt;Similarly, an artist used to create sketches for buyers and was going through a rough spell with acceptance of his work. So he took some half finished sketches to the buyer and asked the buyer how they expected the sketches to be completed. With the buyers feedback the artist was able to make all the sketches to the buyers liking and get back into the grove of selling all of his sketches.&lt;/p&gt;

&lt;p&gt;To get results, one needs to lead the recipient into thinking in your way, and then get them to reach your thought process/idea on their own accord.&lt;/p&gt;

&lt;p&gt;It is very effective to execute an idea when the stakeholders believe that they have come to the ideas on their own accord.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8. Try honestly to see things from the other person’s perspective&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A woman was 6 weeks behind in her car payments, and received a call from the man who was handling her account. Accepting the worst, she thought of the situation from his perspective rather than her own, and admitted to being late on her payment - and possible a very challenging customer for the man - and thereby apologizing. Her empathy to his situation softened him a little and he worked with her to come up with a better way to deal with the car payments.&lt;/p&gt;

&lt;p&gt;Looking at things from another persons perspective sheds new light on the situation - which is always a good position to be in when problem solving.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;9. Be sympathetic with the other person’s ideas and desires&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There is a magic phrase to stop arguments, eliminate ill feeling, create good will and make the other person listen attentively:&lt;/p&gt;

&lt;p&gt;“I don’t blame you one bit for feeling the way you do. If I were you I would probably be feeling the same way.”&lt;/p&gt;

&lt;p&gt;Using a phrase like this can soften the most antagonistic, defensive or cantankerous people.&lt;/p&gt;

&lt;p&gt;Three-fourths of the people that one meets are hungry for sympathy, and if you give it to them - they will love you.&lt;/p&gt;

&lt;p&gt;Responding to anger or insult with kindness is another way of accepting the opposite persons viewpoint and agreeing with it - a pretty effective way to shut down a bully.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;10. Appeal to the nobler motives&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;J. Pierpont Morgan observed that a person usually has 2 reasons for doing a thing: One that sounds good, and a real one.&lt;/p&gt;

&lt;p&gt;In order to change people - appeal to the nobler motive.&lt;/p&gt;

&lt;p&gt;E.g. is someone is trying to break a lease (against the lease agreement), appeal to their nobler virtue of being a person of their word, about morals and ethics, as opposed to creating a scene and being angry about the possibility of legal action.&lt;/p&gt;

&lt;p&gt;When you don’t know anything about someone, assume that they are sincere, honest and truthful and proceed conversations from that perspective.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;11. Dramatize your ideas&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is the day of dramatization. Merely stating a truth isn’t enough. The truth has to be made vivid, interesting, dramatic. You have to use showmanship. The movies do it, TV does it, and you have to do it if you want attention.&lt;/p&gt;

&lt;p&gt;“You are losing pennies to the dollar”  he said as he dramatically dropped several pennies to the grown. The dramatization had a much bigger impact as compared to simply mentioning the statement.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;12. Throw down a challenge&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Charles Schwab said - “the way to get things done, is to simulate competition. Not in a sordid money-getting way, but in the desire to excel. “&lt;/p&gt;

&lt;p&gt;Simulating the desire to excel enables create healthy competition to be better - than ourselves, than our competition.&lt;/p&gt;

&lt;p&gt;Frederic Herzberg (behavioral scientist) discovered through his work that the most important motivator for people was the actual work - not benefits, not working conditions, and not fringe benefits - but the actual work.&lt;/p&gt;

&lt;h3 id=&quot;be-a-leader-how-to-change-people-without-giving-offense-or-arousing-resentment&quot;&gt;Be a Leader: How to Change People Without Giving Offense or Arousing Resentment&lt;/h3&gt;
&lt;p&gt;A leaders job often includes changing your people’s attitudes and behavior. Some suggestions to accomplish this:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Begin with praise and honest appreciation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It is always listen to unpleasant things after we have heard some praise about our good points. So if any feedback needs to be given, start with good points of the recipient, and how great they are, and then follow up with criticisms. People are more likely to make improvements when information is presented to them in a kind manner.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Call attention to people’s mistakes indirectly&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is an effective way to criticize, and yet not be hated for it. Charles Schwab was passing through one of his steel mills when he noticed some of his employees smoking right below the “no smoking” sign. Instead of calling them out on this, he used an alternate technique. He gave each one of his employees a cigar and then said - “I’ll appreciate it, boys, if you smoke this on the outside.” He was able to get his message through and simultaneously made his employees feel important. It is hard not to love a person like that.&lt;/p&gt;

&lt;p&gt;Using the word “but” after giving praise totally negates the praises, and makes them feel like empty statements. E.g. “We are proud of you for your grades this semester, but you could have gotten better grades if you worked harder in Algebra” vs “We are proud of you for your grades this semester, and you can get even better grades if you continue working this way.”&lt;/p&gt;

&lt;p&gt;The second approach is way more effective than the first one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Talk about your own mistakes before criticizing the other person&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It isn’t nearly as difficult to listen to your own faults, when the person telling you your faults is admitting that he too is far from impeccable.&lt;/p&gt;

&lt;p&gt;E.g. Dale Carnegie needed to criticize his secretary for something, but before he said anything, he took a step back to consider the situation from her perspective and point of you and age, and realized that when he was that age, he made many more mistakes than her. He started off his criticisms by mentioning that fact, and then gently suggesting the request he had of her - it was readily accepted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Ask questions instead of giving direct orders&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Asking questions makes the order more palatable, and often simulates the creativity of the persons whom you ask. People are more likely to accept an order if they have had a part in the decision that caused the order to be issued.&lt;/p&gt;

&lt;p&gt;E.g. “what do you think of this?”, “Do you think this would work?”&lt;/p&gt;

&lt;p&gt;It might be more time consuming in the short term, but much more effective in the long term.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. Let the other person save face&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We bulldoze over the feelings of others, getting our own way, finding faults, issuing threats, criticizing a child or an employee in front of others, without even considering the hurt to the other persons pride.&lt;/p&gt;

&lt;p&gt;A Fresh aviation pioneer once said = “I have no right to do or say anything that diminishes a man in his own eyes. What matters is not what I think of him, but what he thinks of himself. Hurting a man in his dignity is a crime. “&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6. Praise the slightest improvement and praise every improvement. Be “hearty in your approbation and lavish in your praise”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you praise someone, instead of finding faults, people start capitalizing on their praise and improving what they are already good at. People appreciate it when they are called out on their positive qualities.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7. Give the other person a fine reputation to live up to&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you want to improve a person in a certain respect, act as though that particular trait were already one of this or her outstanding characteristics.&lt;/p&gt;

&lt;p&gt;E.g. You are a phenomenal mechanic, and you have done great work in the past few years. Off late things are not the same way, and maybe we can come up with a solution for it?&lt;/p&gt;

&lt;p&gt;Making the recipient realize their own qualities helps them build their own self esteem and set up a target for them to reach - all by their own initiative.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8. Use encouragement. Make the fault seem easy to correct&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It is way easier to climb a molehill instead of a mountain. If the fault is describe as the magnitude of a molehill, instead of a mountain, a person is more likely to readily work on it to fix it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;9. Make the other person happy about doing the things you suggest&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;E.g. if you cannot do a speaking engagement, suggest alternatives who are more likely to accept. It gives the recipient the chance to focus on next steps rather than being disappointed about your refusal.&lt;/p&gt;

&lt;p&gt;Think about what the other person wants, and try to help the best you can to mitigate their own problems and satisfy their wants - while you make your suggestions.&lt;/p&gt;
</description>
        <pubDate>Mon, 06 Jan 2020 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/2020/how-to-win-friends-influence-people/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2020/how-to-win-friends-influence-people/</guid>
        
        
      </item>
    
      <item>
        <title>Start with Why</title>
        <description>&lt;p&gt;This book is by &lt;a href=&quot;https://simonsinek.com/&quot;&gt;Simon Sinek&lt;/a&gt;. In this book Sinek talks about finding the elusive - “why” - which is the primordial reason for developing brand loyalty, making friends, selling, and all other aspects of our life.&lt;/p&gt;

&lt;p&gt;Great leaders are able to inspire other people to act and make a difference. Leaders give their following a sense of purpose and belonging - making people do the work for themselves - without an external benefit to be gained.&lt;/p&gt;

&lt;p&gt;Apple, Disney, Harley Davidson, and Southwest airlines are prime examples of companies whose leadership inspires the employees, customers and even the stock holders to align with their WHY.&lt;/p&gt;

&lt;h3 id=&quot;a-world-that-doesnt-start-with-why&quot;&gt;A world that doesn’t start with why&lt;/h3&gt;
&lt;p&gt;It is important to start with the basics - like a Japanese car manufacturer that has focused on each part being perfect, unlike an American car manufacturer that employs a person at the end of an assembly line who uses a mallet to make the assembly perfect. Working inside out - bottoms up - helps with coming with a a clean message, a focused outlook and a more refined product.&lt;/p&gt;

&lt;p&gt;If the WHY (or purpose) isn’t very well defined, people resort to alternatives of manipulation to get customers to take action. Some of these are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Price:&lt;/strong&gt; Reducing price to attract customers&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Promotions:&lt;/strong&gt; Running promotions - such as rebates to gain an edge over competition&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fear:&lt;/strong&gt; Fear of the alternative if item is not purchased. (fear motivates to move away from something horrible)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Aspiration:&lt;/strong&gt; Showing a shiny object to move towards (Aspiration motivates to move towards something desirable)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Peer Pressure:&lt;/strong&gt; Friends, family, competition or celebrity using a product&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Novelty:&lt;/strong&gt; New and interesting features in a given product&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Manipulation techniques do not inspire repeat business or loyalty. It is a short term transaction oriented approach to sales. Having loyal employee and customer base not only reduces costs but also provides massive peace of mind.&lt;/p&gt;

&lt;p&gt;Manipulations work, but they are not sustainable and do not drive loyalty. There is an alternative.&lt;/p&gt;

&lt;h3 id=&quot;an-alternative-perspective&quot;&gt;An alternative perspective&lt;/h3&gt;
&lt;p&gt;As an alternative to manipulation Sinek suggests inspiring rather than manipulating. In order to inspire one needs to know their purpose.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Golden Circle&lt;/strong&gt;
&lt;img src=&quot;https://rutumulkar.com/assets/images/golden_circle.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sinek introduces the concept of &lt;a href=&quot;https://rutumulkar.com/assets/images/golden_circle.jpg&quot;&gt;Golden Circle&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;WHAT: Every company knows what they do and can describe the service they provide
HOW: Some companies know HOW they do WHAT they provide. This is often known as the differentiator
WHY: Very few companies know WHY they do WHAT they are doing. This defines the purpose, cause or belief that people can associate with. This is the belief that resonates with customers, employees and even investors.&lt;/p&gt;

&lt;p&gt;Sinek illustrates how starting marketing or advertising with the WHY helps the consumer connect with the brand and provides the reason to buy, and the actual product is just proof of the WHY.&lt;/p&gt;

&lt;p&gt;People do not buy WHAT you do, they buy WHY you do it.&lt;/p&gt;

&lt;p&gt;More qualified and products often fail, when up against people and products that have a much better defined WHY. E.g. Wright Brothers were the first people to build an airplane, even though Samuel Langley was much more qualified and funded to build an airplane. 
Other examples include Creative Technology Ltd. which was much more better positioned to sell a music player as compared to Apple - which was largely a computer brand at that point.&lt;/p&gt;

&lt;p&gt;Companies that have a clear sense of WHY never worry about competition. They don’t think of themselves as being like everyone else and they don’t have to “convince” anyone of their value.&lt;/p&gt;

&lt;p&gt;Having good quality products (or WHAT you sell) is ofcourse very important, but when the WHY is clearly defined, the WHAT sells much more easily.&lt;/p&gt;

&lt;p&gt;If a customer feels inspired to buy a product rather than manipulated, they will be able to verbalize the reasons why they think what they bought is better and inspires loyalty in the product and brand.&lt;/p&gt;

&lt;p&gt;Knowing your WHY is not the only way to be successful, but it is the only way to maintain a lasting success.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;It is all biological&lt;/strong&gt;
It is biological for humans to feel belonged. Our desire to feel like we belong is so powerful that we will go to great length and do irrational things.&lt;/p&gt;

&lt;p&gt;Such gut decisions don’t happen in your stomach. The limbic brain is responsible for making these gut decisions and feelings of trust and loyalty. However the limbic brain has no capacity for language - which is why it cannot quite describe the actual reasons for making the decision.&lt;/p&gt;

&lt;p&gt;Figuring out the WHY is hard work. e.g. laundry detergent manufactures took a long time to discover that people resonated with the WHY of smelling fresh laundry rather than the “whiteness” of their clean laundry.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clarity, discipline and consistency&lt;/strong&gt;
To understand the WHY - clarity is most important. In order to get to HOW to achieve your WHY one needs discipline.&lt;/p&gt;

&lt;p&gt;Make HOW actionable - Nouns are not actionable, verb are actionable. E.g. instead of “integrity” say “always do the right thing”.&lt;/p&gt;

&lt;p&gt;Consistency between WHAT and WHY - When the why is well defined, and the message is authentic, there is clear consistency between the product and the purpose of the product.&lt;/p&gt;

&lt;h3 id=&quot;leaders-need-a-following&quot;&gt;Leaders need a following&lt;/h3&gt;
&lt;p&gt;Leads can inspire a following if there is trust. With trust comes value - real value, not just value equated with money.&lt;/p&gt;

&lt;p&gt;To inspire trust, we need to find employees who connect with your WHY, so that they cab believe in what you believe. When you fill an organization with good fits, people who believe what you believe, success just happens - everyone is moving towards the same goal.&lt;/p&gt;

&lt;p&gt;Everyone in the planet is passionate about something, but it is not the same things that we are all passionate about - so it is important to find the right people who are passionate about the same things are you are.&lt;/p&gt;

&lt;p&gt;Great companies don’t hire skilled people are motivate them, they hire already motivated people and inspire them.&lt;/p&gt;

&lt;p&gt;Average companies give their people something to work on. In contrast, the most innovative organizations give their people something to work towards - that way people are achieving their own personal WHY while in alignment with the company’s WHY.&lt;/p&gt;

&lt;p&gt;Trust comes from knowing how good someone is at what they do. It also comes from influence of others and their recommendations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Law of Diffusion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://rutumulkar.com/assets/images/diffusion.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;According to law of diffusion, the entire population is made up of 5 segments:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;2.5% are innovators&lt;/li&gt;
  &lt;li&gt;13.5% are early adopters&lt;/li&gt;
  &lt;li&gt;34% are early majority&lt;/li&gt;
  &lt;li&gt;34% are late majority&lt;/li&gt;
  &lt;li&gt;16% are laggards&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When you have a new product, it is always good strategy to first market to the early adopters, rather than the majority. The early adopters offer loyalty and a following, which can be used to target the early and late majority of the market. Tivo is a company that was perfectly positioned, but made the grave mistake of targeting the majority, before building their loyal customer base and eventually did not succeed.&lt;/p&gt;

&lt;p&gt;According to the &lt;strong&gt;Law of Diffusion&lt;/strong&gt;, mass-market success can only be achieved after you penetrate between 15% to 18% of the market.&lt;/p&gt;

&lt;p&gt;Building a loyal customer base means the the people are willing to suffer some inconvenience or pay a premium in to do business with you. They perceive value in what you do, and have trust and belief in what you do. They are the ones who, on their own volition, will tell others about you. These are the people who share your beliefs and what to incorporate your ideas.&lt;/p&gt;

&lt;h3 id=&quot;how-to-rally-those-who-believe&quot;&gt;How to rally those who believe&lt;/h3&gt;
&lt;p&gt;Energy of leaders excites, but charisma of leaders inspires. The goal is to amplify the source of inspiration. Figure out a way to amplify the WHY of the CEO to the HOW that is implemented by the next level of people.&lt;/p&gt;

&lt;p&gt;Those who know WHY need those who know HOW. Just having an idea and purpose doesn’t help, one needs to know how to bring it to reality.&lt;/p&gt;

&lt;p&gt;HOW-types doesn’t need WHY-types to do well. But WHY-types for all their vision and imagination, often get the short end of the stick. Without HOW-type people to complement them WHY-types might end up as starving visionaries.&lt;/p&gt;

&lt;p&gt;Being authentic is the core of communicating the WHY. Say it only if you believe it.&lt;/p&gt;

&lt;p&gt;The ability of some companies not to just succeed but to repeat their success is due to the loyal following they command, the throngs of people who root for their success.&lt;/p&gt;

&lt;p&gt;Greatfulness is repeatable - for instance look at &lt;a href=&quot;https://en.wikipedia.org/wiki/Ron_Bruder&quot;&gt;Ron Bruder&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When a company is small, it revolves around the personality of the founder. There is no debate that the founder’s personality is the personality of the company. As the company grows, the CEO’s job is to personify the WHY. To ooze it, to talk it, to preach it. E.g. There is no difference between Sir Richard Branson’s personality and that of Virgin’s personality. Similarly there is no difference between Steve Jobs’ personality and Apple’s personality.&lt;/p&gt;

&lt;p&gt;The struggle that so many companies have to differentiate and communicate their true value to the outside world is not a business problem, it is a biology problem. Like people struggle with putting words to emotions, companies also struggle with that aspect of communication. We rely on metaphors, imagery and analogies to communicate how we feel.&lt;/p&gt;

&lt;p&gt;The goal for any business - Communicate clearly and you will be understood.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Celery Test&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Celery test is a simple test to evaluate exactly the WHAT and HOW is aligned to your WHY. E.g. people might suggest you to buy Oreo cookies, cake, coconut milk and celery to help your business. However, if your WHY is to eat healthy - Oreo cookies and cake will not pass the celery test.&lt;/p&gt;

&lt;p&gt;Using the celery test to evaluate your choices will help in making the right choices for the business. The more celery you use, the more trust you can build.&lt;/p&gt;

&lt;h3 id=&quot;the-biggest-challenge-is-success&quot;&gt;The biggest challenge is success&lt;/h3&gt;
&lt;p&gt;The single greatest challenge any organization will face is success. All companies start out small, and as they grow the founders use their gut to make the decisions for their company. As the company grows, it becomes virtually impossible for the single person to make all the right decisions for the company.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;School Bus Test&lt;/strong&gt;
If the founder or a leader were to be hit by a school bus, would the organization continue to thrive at the same pace without them?&lt;/p&gt;

&lt;p&gt;To pass the school bus test the founders WHY must be extracted and integrated into the culture of the company. A strong succession plan should aim to find a leader inspired by the founding cause and ready to lead it into the next generation.&lt;/p&gt;

&lt;p&gt;Figuring out what to measure - the right KPI’s - is what defines goals and what gets done. E.g. Christina Halbridge changed the business of debt collection from being aggressive to building a connection with the recipient. She measured the effectiveness based on the number of thank you cards her employees sent. They were incredibly successful not only because of the WHY, but also because she found the right way to measure the WHY.&lt;/p&gt;

&lt;h3 id=&quot;discover-why&quot;&gt;Discover why&lt;/h3&gt;
&lt;p&gt;Discovering WHY is hard work. One can discover WHY by looking back and analyzing the reason behind why some projects wok and others don’t.&lt;/p&gt;

&lt;p&gt;If you follow your WHY, others will follow you. When you compete against others, no one wants to help you. But when you compete against yourself, everyone wants to help you.&lt;/p&gt;

&lt;p&gt;Leadership requires two things: a vision of the world that does not yet exist, and the ability to communicate it. Leaders inspire action.&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Jan 2020 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/blog/2020/start-with-why/</link>
        <guid isPermaLink="true">http://localhost:4000/blog/2020/start-with-why/</guid>
        
        
      </item>
    
  </channel>
</rss>
