<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>rutum</title>
    <description>Your Site Description
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 14 Jul 2023 20:00:18 -0700</pubDate>
    <lastBuildDate>Fri, 14 Jul 2023 20:00:18 -0700</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>Machine Learning, Deep Learning and Large Language Models</title>
        <description>&lt;p&gt;Starting the early 2000s, the improvements in hardware to support deep learning networks has lead to a leap in modern deep learning approaches. Deep Learning (Hinton et al. 2006), (Bengio et al. 2007), which is an extension of neural networks,  contain an input, an output, and a large number of hidden layers between the input and output. This type of an architecture is able to capture non-linear relationships in data, and are better at modeling data. Deep learning works much better than their Machine Learning predecessors as shown by their performance in several types of benchmark datasets such as &lt;a href=&quot;https://rajpurkar.github.io/SQuAD-explorer/&quot;&gt;SQuAD (The Stanford Question Answering Dataset)&lt;/a&gt;, &lt;a href=&quot;https://gluebenchmark.com/leaderboard&quot;&gt;GLUE (General Language Understanding Evaluation)&lt;/a&gt;, &lt;a href=&quot;https://sites.research.google/xtreme&quot;&gt;XTREME (&lt;strong&gt;(X)&lt;/strong&gt; Cross-Lingual &lt;strong&gt;Tr&lt;/strong&gt;ansfer &lt;strong&gt;E&lt;/strong&gt;valuation of &lt;strong&gt;M&lt;/strong&gt;ultilingual &lt;strong&gt;E&lt;/strong&gt;ncoders)&lt;/a&gt; and others.  Deep learning is applicable to a large variety of applications ranging from Natural Language Processing, Speech Recognition, Computer Vision etc. For this doc, I will focus on Deep Learning as it pertains to Natural Language Processing, as it gives me the opportunity to delve deeper into LLMs - the newest kid on the block with Deep Learning. This doc is split into the following layout:&lt;/p&gt;

&lt;p&gt;I describe the foundational improvements that have been made in every aspect of the deep learning architecture to bring us to where we are today. Although compute and data are 2 major areas which have enabled Deep Learning as it is today, my focus for this doc is on architecture and algorithmic improvements.&lt;/p&gt;

&lt;h2 id=&quot;from-ml-to-deep-learning&quot;&gt;&lt;a name=&quot;from-ml-to-deep-learning&quot;&gt;&lt;/a&gt;From ML to Deep Learning&lt;/h2&gt;

&lt;p&gt;A simplified type of a neural network with a single hidden layer, require the following to train a model: Feature representation, loss function (such as cross entropy loss), classification function (such as sigmoid), optimizers (such as gradient descent). Grounding our understanding in terms of this, we can observe significant improvements in each of these areas, which has enabled deep learning to be as prevalent and effective as it is today. Deep Learning has enabled generalization of knowledge, and the possibility of pre-training on generic data and fine tuning to a specific domain (domain adaptation), without the need for any hand coded feature generation. In the rest of this section I will describe some foundational improvements that Deep Learning has provided based for every aspect of a neural network on the following high level areas, which makes deep learning work so well in practice compared to previous approaches.&lt;/p&gt;

&lt;h3 id=&quot;feature-representation&quot;&gt;&lt;a name=&quot;feature-representation&quot;&gt;&lt;/a&gt;Feature Representation&lt;/h3&gt;

&lt;p&gt;In Machine Learning (ML), a rich feature representation is required as input for an ML model to learn from the data. Previously this was done using feature engineering, and hand-coding of features. However, with deep learning, the input is now either pretrained embeddings or the features/embeddings are learned from running text by the deep learning neural network. More details about the evolution of feature representation and vector semantics is provided in &lt;a href=&quot;#appendix-a-word-vectors-and-distribution-hypothesis&quot;&gt;Appendix A&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In 2003 Bengio at. al. [] introduced a revolutionary new intuition of using running text as supervised training data to predict whether a word “a” is more or less likely to show up near another word “b”. This avoids the needs for any hand-labeled data for supervised classification. The authors applied this approach for language modeling and illustrate how this approach improves perplexity (branching factor) of the model over existing n-gram approaches. In 2013, Word2Vec (Mikolov et al. 2013) was introduced, which used running text as input to a binary classification model to to classify whether a word exists in the neighborhood of another word. Word2vec uses 2 approaches to accomplish this: Skip Gram with Negative Sampling (SGNS), Cumulative bag of words (CBOW). With SGNS, for each target word, we treat the neighboring context words (within a window of words) as positive samples, and then randomly sample words from the rest of the lexicon and use them as negative samples. This is then provided as input to the classifier, which distinguishes between the positive and negative samples. The weights that are learned from the classifier are treated as the embeddings. Word2Vec has some shortcomings, as it can only provide static embeddings, and not different embeddings based on contextual information. This means that the words “bank” would be the same embedding irrespective of whether it is mentioned in the context of a  “river bank” or a “financial institution bank”. Word2Vec also had trouble dealing with unknown words, as it tokenized based on words/phrases. Finally, Word2Vec could not handle word dependencies longer than the window of the surrounding text. In 2014 GloVe (Pennington et. al 2014) built on top of the limitations of Word2Vec, and not only used local context (like word2vec) but also global context to capture the the relationship between two words or phrases. GloVe is better than word2vec at handling rare words, because of global relationships between rare words and common words is captured by the model. Like Word2Vec, GloVe also worked on word or phrases as the smallest tokens, and had trouble working with unseen words. In 2017 Fasttext (Bojanowski et al., 2017)](https://aclanthology.org/Q17-1010/) was introduced which works with subword instead of words or phrases. This means that it would handle rare words much better, as it tokenized the words into their subwords. E.g. “let’s” would be broken down into “let” and “‘s” and their embeddings would be learned independently. In 2018 (Peters et.al. 2018) proposed ELMo that is able to capture the concept of contextual embeddings (which addresses the “bank” problem above. ELMo as built on bi-directional RNN layers, capturing the embeddings of it from the forward and backward pass of the RNN. More recently there are newer representations of embeddings using the transformer architecture (such as BERT, TransformerXL, ChatGPT and others).&lt;/p&gt;

&lt;h4 id=&quot;tokenization-and-subwords&quot;&gt;&lt;a name=&quot;tokenization-and-subwords&quot;&gt;&lt;/a&gt;Tokenization and Subwords&lt;/h4&gt;

&lt;p&gt;Creating embeddings at subword level is able to help us deal with unknown words much better than before. Now, we cam compose the definition of a new word based on it’s subcomponents. In 2015, (Sennrich et al. 2015) explored working with &lt;a href=&quot;https://rutumulkar.com/blog/2021/byte-pair-encoding/&quot;&gt;Byte-Pair Encoding (BPE)&lt;/a&gt; (Gage, 1994) that is originally a compression algorithm to encode running text. This helps capture text better than existing tokenization approaches, but as it works with unicode characters (144,697 unicode characters!), the unicode combinations are sparse. In 2016, Google introduced WordPiece (Wu et. al. 2016) which was the internal tokenizer used by BERT. In 2018 (Kudo and Richardson, 2018) introduced SentencePiece as a much more performant, and principled approach for subword encoding as compared to BPE alone. SentencePiece combines BPE with the Unigram model. SentencePiece was used to train T5 Language Model. In (Bostrom and Durett, 2020)(https://arxiv.org/pdf/2004.03720.pdf)) the authors mention that BPE is not an effective way to train LLMs.&lt;/p&gt;

&lt;h3 id=&quot;non-linear-activation-functions&quot;&gt;&lt;a name=&quot;non-linear-activation-functions&quot;&gt;&lt;/a&gt;Non-Linear Activation functions&lt;/h3&gt;

&lt;p&gt;Activation functions are used in the hidden layers of deep learning architecture, which each individual node takes in input from the previous layer, and performs an operation on them. The goal of activation functions is to be able to capture complex data from the input, without loss of information. Existing activation functions, such as Sigmoid cannot represent more complex data representations, and is heavily prone to gradient saturation for values close to 0 or 1. Explaining this a bit more, we know that:&lt;/p&gt;

&lt;p&gt;Sigmoid Activation is  (y = 1 / (1+e^ (-z))), where z = sum (w_i * x_i) + b.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/sigmoid.jpg&quot; alt=&quot;Sigmoid Activation Function: Image Credit - Wikipedia&quot; width=&quot;400&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 1: Sigmoid Activation Function: Image Credit - Wikipedia&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;This z value is converted into a probability by using an activation function such as sigmoid. The problem with sigmoid is that it squashes outliers towards 0 or 1, making it challenging to capture outlier data, as it causes problems getting derivatives and propagating it back to the first layer using Backprop. This problem is also known as the Vanishing Gradients Problem. To address this, deep learning has explored with non-linear activation functions such as Rectified Linear Units or ReLU (Nair and Hinton, 2010). ReLU has linear behavior for positive values, and zero activation for negative values. Using ReLU transforms the input space (such as XOR) into a linear space in the hidden layers, which can then be classified using a linear approach (Goodfellow 2016, page 169)&lt;/p&gt;

&lt;p&gt;Relu Activation Function is: y = ReLU(z) = max(z,0), where z = sum (w_i * x_i) + b.&lt;/p&gt;

&lt;p&gt;Newer activations functions are now used for LLMs. An image with ReLU, GeLU and others along with more details are covered in the &lt;a href=&quot;#from-deep-learning-and-transformers-to-large-language-models&quot;&gt;next section regarding LLMs&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;optimizers-and-backpropagation&quot;&gt;&lt;a name=&quot;optimizers-and-backpropagation&quot;&gt;&lt;/a&gt;Optimizers and Backpropagation&lt;/h3&gt;

&lt;p&gt;Gradient Descent (GD) (Robbins and Monro 1951) is the algorithm to find the global minima in a gradient for all ML and Deep Learning algorithms.&lt;/p&gt;

&lt;p&gt;In 2010, Stoachastic Gradient Descent (SGD) (Bottou, L. 2010), was introduced, which is what is used in practice (however more recent new approaches have evolved that are effective for training LLMs). GD requires all of the data to be processed once, after which it will update it’s parameters. This works for small amounts of data, but it prohibitive when working with large amounts of data for deep learning. In comparison to SD, SDG only requires a single datapoint (or a batch of datapoints - called mini-batch) to be processed before updating the parameters. SGD typically converges faster and is more memory efficient. SDG and GD both work with a constant learning rate (or step size), irrespective of whether it has encounted the same datapoint more frequently or less frequently. As a result, the learning is often more time consuming, or is stuck at a local minima. To address this Adagrad (Adaptive Gradient Descent) (Duchi 2011) performs gradient descent with varying learning rates for different parameters, increasing the learning rate for rare datapoints to push for faster convergence. Adagrad is able to handle sparse data much better than GD or SGD. Adam optimizer (Kingma and Ba, 2015), builds on Adagrad, and combines adaptive learning rates with moment. Adam maintains different learning rates for different parameters, and combines them with the first moment (mean of the gradients, providing the overall direction of the gradient) and the second moment (variance of the gradient, providing the magnitude of the gradient). This helps with faster convergence and adaptation to varied types of gradients. Other honorary mentions for optimizers are RMSProp, AdaDelta, AdaMax etc. which have their own nuances which need to be evaluated before leveraging for a deep learning problem. Improving the optimizers have enabled much faster convergence of Deep Learning Networks, handling sparse data better, and improved exploration of the landscape to avoid local minima.&lt;/p&gt;

&lt;p&gt;Neural networks can contain a large number of stacked layers, where the output of the final layer needs to be propagated back to the first layer for learning. This is done using the error backpropagation or backprop (Rumelhart et al., 1986), (LeCun et. al. 1998).&lt;/p&gt;

&lt;h3 id=&quot;deep-learning-architectures-regularization-and-attention&quot;&gt;&lt;a name=&quot;deep-learning-architectures,-regularization-and-attention&quot;&gt;&lt;/a&gt;Deep Learning Architectures, Regularization and Attention&lt;/h3&gt;

&lt;h4 id=&quot;feed-forward-neural-networks&quot;&gt;&lt;a name=&quot;feed-forward-neural-networks&quot;&gt;&lt;/a&gt;Feed Forward Neural Networks&lt;/h4&gt;

&lt;p&gt;In 2003, (Bengio et al. 2003) first introduced the simple feed-forward language model primarily for language modeling. While previously, we were using n-gram language models, where ‘n’ usually ranged up to 3, neural language models, neural models are able to generalize over a larger context, and generalize better. The first feedforward neural network contained a single hidden layer, and was able to capture long distance dependencies much betters as compared to the n-gram approach and used running text as input to illustrate self-supervision.&lt;/p&gt;

&lt;h4 id=&quot;recurrent-neural-networks-rnn&quot;&gt;&lt;a name=&quot;recurrent-neural-networks-(rnn)&quot;&gt;&lt;/a&gt;Recurrent Neural Networks (RNN)&lt;/h4&gt;

&lt;p&gt;Recurrent Neural Networks are neural networks that have a cycle within the network, where the hidden layer computed in the previous iteration is leveraged as a form of context or memory for the next iteration.. This is very pertinent to language, which is largely dependent on the previous text/utterance. The first RNN language models were by (Mikolov et al., 2010). RNNs are used for Language Modeling (for machine translation), sequence/text classification and several other downstream tasks.&lt;/p&gt;

&lt;p&gt;RNNs have been used for Machine Translation using en Encoder-Decoder architecture (also knows as seq-2-seq models). Here an encoder takes in an input sentence and converts it into an embedding of some form, which is taken as input by the decoder and converted into text in another language. Encoder Decoder architectures have been widely successful and also have been applied to tasks such as question answering, textual entailment, summarization etc. The intuition behind this is that the output text is a function of the input text (e.g. answer is related to the question being asked), even though the output and input both belong to the same language.&lt;/p&gt;

&lt;p&gt;Although RNNs capture the temporal nature of language and dependence on previous words, it has a limitation of not being able to parallelize the processing, as each token can only be processed after the previous token is processed and the weights from the hidden layers are passed to it. Another limitation of this was the problem of vanishing gradients, meaning that the gradients brought from the hidden layer were subjected to so many multiplications, that they eventually ended up becoming 0. This lead to the problem of long distance dependencies not accurately captured.&lt;/p&gt;

&lt;h4 id=&quot;long-short-term-memory-networks-lstm&quot;&gt;&lt;a name=&quot;long-short-term-memory-networks-(lstm)&quot;&gt;&lt;/a&gt;Long Short Term Memory Networks (LSTM)&lt;/h4&gt;

&lt;p&gt;LSTMs (Hochreiter and Schmidhuber, 1997) started being used to mitigate the issues introduced by RNNs, in particular regarding RNNs not being able to address or make use of long distant information. LSTMs introduced gates that selectively passed information from the input layer and also the hidden layer from the previous node.&lt;/p&gt;

&lt;h4 id=&quot;regularization-to-avoid-overfitting&quot;&gt;&lt;a name=&quot;regularization-to-avoid-overfitting&quot;&gt;&lt;/a&gt;Regularization to avoid Overfitting&lt;/h4&gt;

&lt;p&gt;In order to avoid overfitting, various forms of regularization are used. One of the most important ones is called - dropout. Dropout is when we randomly drop some units and their connections from the network during training (Hinton et al. 2012), (Srivastava et al. 2014). Hyperparameter tuning is another important requirement to avoid overfitting or being stuck at a local minima.&lt;/p&gt;

&lt;h4 id=&quot;attention-mechanisms&quot;&gt;&lt;a name=&quot;attention-mechanisms&quot;&gt;&lt;/a&gt;Attention Mechanisms&lt;/h4&gt;

&lt;p&gt;In 2014 (Bahdanau 2014) introduced the concept of Attention in Deep Neural Networks, which addressed the botteneck issue that was introduced by RNNs (where the data at n-1 needed to be processed before data at n so that data n could use the hidden layer as input from data n-1). Using the attention mechanism, data n could get hidden states from all of the previous data points, and not just data n-1. This was explained from the context of a machine translation task where the encoder created hidden layers for all of the items in the input, and all these hidden layers were provided to the decoder in the form of a context vector, which is a function of all the hidden layers.&lt;/p&gt;

&lt;p&gt;There are different types of attention mechanisms. Dot-product attention is one such approach, where all the hidden layers from the past few contexts are combined in the form of a dot product, and taken to the decoding layer of an RNN.&lt;/p&gt;

&lt;h3 id=&quot;transformers-for-transfer-learning-and-contextual-embeddings&quot;&gt;&lt;a name=&quot;transformers-for-transfer-learning-and-contextual-embeddings&quot;&gt;&lt;/a&gt;Transformers for Transfer Learning and Contextual Embeddings&lt;/h3&gt;

&lt;h4 id=&quot;self-attention-and-transformer-architecture&quot;&gt;&lt;a name=&quot;self-attention-and-transformer-architecture&quot;&gt;&lt;/a&gt;Self Attention and Transformer Architecture&lt;/h4&gt;

&lt;p&gt;In 2017, (Vaswani et al., 2017), proposed the original transformer architecture which was based on two lines of prior research: self-attention (Bahdanau 2014) and memory networks (Sukhbaatar et al., 2015). Transformer is based on the concept of attention, and it replaces RNNs and the bottlenecks introduced by it. In the original Transformer paper, it consists of an encoder and a decoder. In the encoder, Transformers use fully connected feed forward neural networks where each input token is connected with all the past tokens in a step called Self-Attention. Using this, and extending it to multi-head attention where each token is performing self-attention in parallel, the encoder is able to capture the dependency relationships between each token in the input. Transformers also add positional encoding (one hot encoding) of the position of the token in the sequence to capture word order, thereby replacing the need for RNN like architecture to encode the sequential dependency of a word/token on previous tokens. The encoder and decoder both contain similar stacked layers of self attention and fully-connected networks. Transformers allow parallel computation (which RNN’s could not). Transformers also introduce Layer Normalization (scaling the dot products after each layer), which is able to address the vanishing gradients problem that RNNs, LSTMs and other approaches could not address. Today, Transformers are the cornerstone all language models for autoregressive generation (gen AI). There are several improvements that have been made to vanilla transformers to train LLMs as they are today.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/transformer.jpg&quot; alt=&quot;&quot; width=&quot;400&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 2: Vanilla Transformer Architecture&lt;/em&gt;
&lt;/p&gt;

&lt;h4 id=&quot;limitations-of-transformers&quot;&gt;&lt;a name=&quot;limitations-of-transformers&quot;&gt;&lt;/a&gt;Limitations of Transformers&lt;/h4&gt;

&lt;p&gt;Attention is quadratic in nature (because at token ‘n’, we are computing context for ‘n’ and all the previous ‘n-1’ tokens. As a result, Transformer architectures have not able to address very long documents. Some approaches that have been introduced to address this approaches like Longformer (Beltagy et. al 2020), where the attention mechanism scales linearly with sequence length. This enables processing much longer texts as compared to the vanilla transformer approach. More recently newer attention mechanisms have been introduced to address the quadratic nature of attention. Details are in the &lt;a href=&quot;#pre-training-of-llms&quot;&gt;Pre-Training section of LLMs&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;transformer-based-models&quot;&gt;&lt;a name=&quot;transformer-based-models&quot;&gt;&lt;/a&gt;Transformer Based Models&lt;/h4&gt;

&lt;p&gt;In 2019, BERT (Devlin et. al. 2019) was introduced which has two objective functions: Masked LM, and NSP (Next Sentence Prediction), so that it would learn bidirectional information from within a sentence, and learn about dependencies between 2 sentences.  It was one of the first initiatives that showed contextual embeddings. Also, honorable mention to ELMo (Peters et. al 2018) which was the first initiative for contextual embeddings, but it did not use the transformer architecture. Other transformer inspired early approaches are:  RoBERTa (Liu et al. 2019), Distilbert (Sanh et. al. 2019), TransformerXL (Dai et. al. 2019), T5 (Raffel 2019).
Deep Learning is able to generalize for unseen data much better than their Machine Learning counterparts. (Erhan et al. 2010) discuss the important of pretraining for deep learning tasks.&lt;/p&gt;

&lt;h2 id=&quot;from-deep-learning-and-transformers-to-large-language-models&quot;&gt;&lt;a name=&quot;from-deep-learning-and-transformers-to-large-language-models&quot;&gt;&lt;/a&gt;From Deep Learning and transformers to Large Language Models&lt;/h2&gt;

&lt;p&gt;More recently researchers have observed that model scaling can lead to an improved model capacity (ability to represent complex patterns that is it trained on) and significant improvement in performance in downstream tasks. It is also discovered that this new era of Large Language Models (that have 10B parameters or more) exhibit some emergent capabilities (such as in context learning), that have not been present in small scale language models (such as BERT, DistilBERT etc - which have Millions of Parameters only).&lt;/p&gt;

&lt;p&gt;In 2018, GPT-1 (Radford et. al. 2018) which stands for &lt;em&gt;Generative Pre-Training&lt;/em&gt; was developed using a generative decoder only Transformer Architecture. GPT-1 adopted an approach of pre-training followed by supervised fine-tuning. GPT-1 is a 117MM parameter model.&lt;/p&gt;

&lt;p&gt;Later in 2019, GPT-2 (Radford et. al. 2019), followed a similar approach as GPT-1, but increased the number of paramerters to 1.5B. It aimed to perform tasks without explicit fine tuning using labeled data. To enable this, the authors introduced a probabilistic approach for multi-task solving &lt;em&gt;p(output|input, task),&lt;/em&gt; where the output is conditioned not only on the input, but also the task.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;&gt;GPT-3&lt;/a&gt; (Brown et. al. 2020) was released in 2020, and it scaled the number of model parameters to 175B. The authors introduce the concept of in-context learning, which uses LLMs in a few-shot or zero shot way. This means that the pre-training and prompting (with in-context information) will help the model converge to an answer within the context of the information provided.&lt;/p&gt;

&lt;p&gt;GPT-4 (OpenAI, 2023) was released in 2023 (March) and it extended text input to multimodal signals. GPT-4 shows strong capabilities in solving complex problems as compared to previous models. A recent study by (Bubeck et. al 2023) showed that GPT-4 can perform better at a variety of tasks of different domains (such as mathematics, coding, vision, medicine, law, psychology and more), and performs very similar to human results. The paper shares that this is the beginning of AGI (Artificial General Intelligence)&lt;/p&gt;

&lt;p&gt;Huggingface recently released the &lt;a href=&quot;https://huggingface.co/bigscience/bloom&quot;&gt;Bloom model&lt;/a&gt; (HuggingFace 2022) which has multilingual support for 46 natural languages and 13 programming languages, and Meta has released LLaMA (Touvron et. al. 2023) has 65B parameters . All of these are generative language models and they have moved us several leaps into NLP tasks such as : question answering, multi-task learning (Radford et. al. 2019), and others, and also have shown emergent abilities that can be observed using prompting. These generative models are also evaluated using prompting, which has lead to a whole new way of debugging language models, and learning about what these LMs know and how they can be leveraged in other areas.&lt;/p&gt;

&lt;p&gt;(Chen et al, 2023) provide a detailed survey of LLMs and what it has taken for us to get this far with them.&lt;/p&gt;

&lt;h3 id=&quot;why-do-llms-work&quot;&gt;&lt;a name=&quot;why-do-llms-work?&quot;&gt;&lt;/a&gt;Why Do LLMs work?&lt;/h3&gt;

&lt;h4 id=&quot;scaling-laws&quot;&gt;&lt;a name=&quot;scaling-laws&quot;&gt;&lt;/a&gt;Scaling Laws&lt;/h4&gt;

&lt;p&gt;KM scaling law (Kaplan et. al. 2020) by open AI proposed a power law relationship of model performance with respect to three major factors - Model Size, Datset Size and amount of training compute.&lt;/p&gt;

&lt;p&gt;The Google DeepMind team (Hoffmann et. al. 2022) proposed another study (Chinchilla Scaling Law) which is an alternative form of scaling for training LLMs.&lt;/p&gt;

&lt;p&gt;Below is an image of how the number of parameter of a model have expanded over the years.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/llms.jpg&quot; alt=&quot;&quot; width=&quot;800&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 4: Image Credit: Stanford LLM Course&lt;/em&gt;
&lt;/p&gt;

&lt;h3 id=&quot;emergent-abilities-of-llms&quot;&gt;&lt;a name=&quot;emergent-abilities-of-llms&quot;&gt;&lt;/a&gt;Emergent Abilities of LLMs&lt;/h3&gt;

&lt;p&gt;LLMs introduced a new set of abilities that were not previously present in the smaller models (such as BERT), but were introduced when models were scaled to a much larger size. Some of these abilities include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In-context learning: This learning ability was formally introduced by GPT-3 (Brown et. al. 2020) where given that a model is provided with some task demonstrations, it can generate the output text by completing the word sequence of the input text.&lt;/li&gt;
  &lt;li&gt;Instruction Following: After fine tuning on instruction datasets, LLMs are able to follow and execute tasks for new datasets.&lt;/li&gt;
  &lt;li&gt;Step-by-Step Reasoning: Using chain-of-thought (CoT)(Chung et. al. 2022) prompting approaches, LLMs are able to solve step by step reasoning steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pre-training-of-llms&quot;&gt;&lt;a name=&quot;pre-training-of-llms&quot;&gt;&lt;/a&gt;Pre-training of LLMs&lt;/h3&gt;

&lt;p&gt;LLMs rely on a massively large corpus of data for training. Most LLMs such as GPT-2 (Radford et. al. 2019) and PaLM (Chowdhery et. al 2022) are trained on generic datasets that are a collection of books, webpages and conversational text, as this generic data and introduce general purpose capabilities to the language model. CommonCrawl is one of the biggest sources of web data, and Reddit corpora is one of the biggest sources of conversational text. Several LLMs such as PaLM (Chowdhery et. al 2022) and Bloom (Huggingface 2022) also use specialized text data for training. This includes data like multilingual text, scientific text (research papers) and code (from stack exchange or Github). In order to train with this data, several preprocessing steps are performed to remove redundant, private data, irrelevant data or toxic data from the text. The text is encoded into subword tokenizers (some of which was discussed in the &lt;a href=&quot;#tokenization-and-subwords&quot;&gt;Subwords Section&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Once the data requirements are established (all LLMs need to have large amounts of data and high quality data for optimal performance), the data is passed through one of the many architectures such as encoder-decoder architecture (used by BART, T5, causal decoder architecture (for next word prediction and used by GPTs, BLOOM), or the more recent Prefix Decoder (used by PaLM) Architecture. There has been a lot of research around the best location of layer normalization, and pre, post or sandwich layer norms are some different approaches used with the architecture.&lt;/p&gt;

&lt;p&gt;Activation functions used by LLMs are different from Deep Learning activations that I discussed in the previous section. These are largely GeLU (Gaussian Error Linear Unit) (Hendrycks 2016) or variants of GLU activation (Shazeer 2020) such as SwiGLU (Shazeer 2020) and GeGLU (Shazeer 2020). Below is an image of GELU activation function as compared to RELU and ELU (Clevert et. al. 2016).&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/activations.jpg&quot; alt=&quot;&quot; width=&quot;400&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 5: Image Credit: Papers with Code&lt;/em&gt;
&lt;/p&gt;

&lt;p&gt;Position embedding (as absolute positional encoding) as presented in the vanilla Transformer architecture has several new variations proposed, such as relative positional embedding, Rotary position embedding, and AliBi (Press et. al. 2022)&lt;/p&gt;

&lt;p&gt;Several different types of attention mechanisms can be used for LLMs today. For instance, sparse attention approaches are used to address the quadratic computational complexity by the vanilla transformer. GPT-3 uses factoid attention (Child et. al. 2019) where instead of full attention, each query can only be attended to by a subset of tokens based on positions. 
Another type of Attention is multiquery attention (Shazeer 2019) where the same linear transformation matrices are shared by different heads. Another approach is FlashAttention (Dao et. al, 2022) which proposes optimizations based on memory consumption of memory modules on GPU.&lt;/p&gt;

&lt;p&gt;There are 2 common objective functions used - language modeling (predict the next word given the previous word), or denoising autoencoding. During training, GPT-3 (Brown et. al. 2020) and PaLM (Chowdhery et. al 2022) have introduced a new strategy that dynamically increases the batch size during training. New LLMs also adopt a similar strategy with learning rates, where the learning rate is gradually increased to the maximum value, followed by a decay strategy. The optimizers used are ADAM (Kingma and Ba 2015), or variations of it such as ADAMw (Loshchilov and Hutter 2017).&lt;/p&gt;

&lt;h4 id=&quot;training---support-for-distributed-training&quot;&gt;&lt;a name=&quot;training---support-for-distributed-training&quot;&gt;&lt;/a&gt;Training - support for distributed training&lt;/h4&gt;

&lt;p&gt;The training for LLMs is optimized using three commonly used parallel training techniques - data parallism where the training corpus is distributed across GPUs, pipeline parallelism (Huang et. al, 2019) where multiple layers of the transformers are distributed across GPUs and tensor parallelism (where the tensor is decomposed and parallelized).&lt;/p&gt;

&lt;h3 id=&quot;adaptation-tuning-for-llms&quot;&gt;&lt;a name=&quot;adaptation-tuning-for-llms&quot;&gt;&lt;/a&gt;Adaptation Tuning for LLMs&lt;/h3&gt;

&lt;p&gt;Pretraining helps language models acquire general abilities for solving tasks. Most LLMs go through the next step of adaptation. There are 2 types of adaptation: instruction tuning (where abilities of LLMs are converted from language based to task based) and alignment tuning (where the output of the LMM is aligned with human preferences and values).&lt;/p&gt;

&lt;h4 id=&quot;instruction-tuning&quot;&gt;&lt;a name=&quot;instruction-tuning&quot;&gt;&lt;/a&gt;Instruction Tuning&lt;/h4&gt;

&lt;p&gt;In a recent paper, (Chen et. al, 2023) describe several datasets that are available for instruction tuning, and how instruction tuning has helped generalize to unseen tasks. In 2022, (Chung et. al. 2022) first experimented with Chain-of-Thought (CoT) prompting to show commonsense reasoning ability and mathematical reasoning ability.&lt;/p&gt;

&lt;h4 id=&quot;alignment-tuning&quot;&gt;&lt;a name=&quot;alignment-tuning&quot;&gt;&lt;/a&gt;Alignment Tuning&lt;/h4&gt;

&lt;p&gt;Regarding the second type of adaptation - alignment tuning - Open AI recently came out with their paper on InstructGPT (Ouyang et. al 2022) that helps further train models to align their output with human output. 
To align LLMs with human values (such as don’t use profanity, be polite, etc.) , Reinforcement Learning from Human Feedback (RLHF) (Christiano et. al. 2017) was proposed which leverages algorithms like Proximal Policy Evaluation (Schulman et. al. 2017) to adapt LLMs to human feedback. An RLHF system has 3 components - the LLM to be tuned, a reward model, an RL approach for training. In order to efficiently fine tune using RLHF, several efficient parameter tuning approaches are proposed such as Adapter Tuning (Houlsby 2019), Prompt Tuning (Lester et al., 2021), LoRA (Low-Rank Adaptation) etc. LoRA has been widely applied to open source LLMs (such as LLaMA and BLOOM)&lt;/p&gt;

&lt;h3 id=&quot;model-quantization&quot;&gt;&lt;a name=&quot;model-quantization&quot;&gt;&lt;/a&gt;Model Quantization&lt;/h3&gt;

&lt;p&gt;LLMs are challenging to deploy in the real world because of their prohibitive memory footprint. Model Quantization is the approach that is used for reducing the memory footprint of LLMs using popular model compression approaches. There are two quantization approaches - Quantization-aware training (QAT) (this requires a full model retraining) and post-training quantization (PTQ) (this requires no model retraining). Some PTQ approaches include Mixed Precision decomposition (Dettmers et. al. 2022), Per-Tensor Quantization (Xiao et. al. 2023), etc.&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;&lt;a name=&quot;final-thoughts&quot;&gt;&lt;/a&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;We have made tremendous progress in Deep Learning and NLP in the past few years, and although I have tried to cover a lot of the seminal work here, I wanted to emphasize that this is still a small drop in the massive amounts of work and research that has gone into this space. Some more concepts that are interesting and will be covered in future blog posts are - Knowledge Distillation, Quantization, Chain-of-Thought Prompting, in-context learning, Planning and Commonsense Reasoning using LLMs, Prompt Engineering, which hold promise for even bigger leaps into deep learning in the future.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;&lt;a name=&quot;references&quot;&gt;&lt;/a&gt;References&lt;/h2&gt;

&lt;p&gt;[1] (Bahdanau et. al 2015) Bahdanau, D., K. H. Cho, and Y. Bengio. 2015. &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;Neural machine translation by jointly learning to align and translate&lt;/a&gt;. ICLR 2015.&lt;/p&gt;

&lt;p&gt;[2] (Beltagy et. al 2020) Iz Beltagy, Matthew E. Peters, Arman Cohan, Longformer: &lt;a href=&quot;https://arxiv.org/abs/2004.05150&quot;&gt;The Long-Document Transformer&lt;/a&gt;, 2004, arXiv&lt;/p&gt;

&lt;p&gt;[3] (Bengio et. al 2003) Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. “&lt;a href=&quot;https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&quot;&gt;A Neural Probabilistic Language Model.&lt;/a&gt;” Journal of Machine Learning Research, vol. 3, 2003, pp. 1137-1155.&lt;/p&gt;

&lt;p&gt;[4] (Bengio et. al. 2007) Bengio, Y., P. Lamblin, D. Popovici, and H. Larochelle. 2007. &lt;a href=&quot;https://proceedings.neurips.cc/paper_files/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf&quot;&gt;Greedy layer-wise training of deep networks&lt;/a&gt;. NeurIPS.&lt;/p&gt;

&lt;p&gt;[5] (Bottou 2010) Bottou, L. (2010). &lt;a href=&quot;https://leon.bottou.org/publications/pdf/compstat-2010.pdf&quot;&gt;Large-scale machine learning with stochastic gradient descent&lt;/a&gt;. In Proceedings of COMPSTAT’2010 (pp. 177-186). Springer.&lt;/p&gt;

&lt;p&gt;[6] (Brown et. al. 2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. &lt;a href=&quot;https://arxiv.org/pdf/2005.14165.pdf&quot;&gt;Language Models are Few-Shot Learners&lt;/a&gt;. NeurIPS 2020.&lt;/p&gt;

&lt;p&gt;[7] (Bubeck et. al 2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, “&lt;a href=&quot;https://arxiv.org/pdf/2303.12712.pdf&quot;&gt;Sparks of Artificial General Intelligence: Early experiments with GPT-4&lt;/a&gt;”, 2023&lt;/p&gt;

&lt;p&gt;[8] (Chen et al, 2023) Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen, “&lt;a href=&quot;https://arxiv.org/pdf/2303.18223.pdf&quot;&gt;A Survey of Large Language Models&lt;/a&gt;”, 2023, arXiv&lt;/p&gt;

&lt;p&gt;[9] (Child et. al. 2019) Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever, &lt;a href=&quot;https://arxiv.org/pdf/1904.10509.pdf&quot;&gt;Generating Long Sequences with Sparse Transformers&lt;/a&gt;, 2019, CoRR, abs/1904.10509&lt;/p&gt;

&lt;p&gt;[10] (Chowdhery et. al 2022) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N.M., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.C., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., García, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.M., Pillai, T.S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Díaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K.S., Eck, D., Dean, J., Petrov, S., &amp;amp; Fiedel, N. (2022). &lt;a href=&quot;https://arxiv.org/pdf/2204.02311.pdf&quot;&gt;PaLM: Scaling Language Modeling with Pathways&lt;/a&gt;. &lt;em&gt;ArXiv, abs/2204.02311&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;[11] (Chung et. al. 2022) Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei, &lt;a href=&quot;https://arxiv.org/pdf/2210.11416.pdf&quot;&gt;Scaling Instruction-Finetuned Language Models&lt;/a&gt;, 2022, arXiv&lt;/p&gt;

&lt;p&gt;[12] (Christiano et. al. 2017) Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei (2017). &lt;a href=&quot;https://arxiv.org/pdf/1706.03741.pdf&quot;&gt;Deep reinforcement learning from human preferences&lt;/a&gt;. Advances in Neural Information Processing Systems 30 (NIPS 2017)&lt;/p&gt;

&lt;p&gt;[13] (Clevert et. al. 2016) &lt;a href=&quot;https://arxiv.org/pdf/1511.07289.pdf&quot;&gt;Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)&lt;/a&gt;, Djork-Arné Clevert, Thomas Unterthiner, Sepp Hochreiter, 2016&lt;/p&gt;

&lt;p&gt;[14] (Dai et. al. 2019) Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. “&lt;a href=&quot;https://arxiv.org/pdf/1901.02860.pdf&quot;&gt;Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context&lt;/a&gt;.” Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2978-2988.&lt;/p&gt;

&lt;p&gt;[15] (Dao et. al, 2022) Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré, &lt;a href=&quot;https://arxiv.org/pdf/2205.14135.pdf&quot;&gt;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[16] (Dettmers et. al. 2022) Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer, &lt;a href=&quot;https://arxiv.org/pdf/2208.07339.pdf&quot;&gt;LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale&lt;/a&gt;, 2022, 2208.07339, arXiv&lt;/p&gt;

&lt;p&gt;[17] (Devlin et. al. 2019) Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019. &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;BERT: Pre-training of deep bidirectional transformers for language understanding&lt;/a&gt;. NAACL HLT.&lt;/p&gt;

&lt;p&gt;[18] (Duchi 2011) Duchi, J., Hazan, E., &amp;amp; Singer, Y. (2011). &lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;&gt;Adaptive subgradient methods for online learning and stochastic optimization&lt;/a&gt;. Journal of Machine Learning Research, 12(7), 2121-2159.&lt;/p&gt;

&lt;p&gt;[19] (Erhan et. al. 2010) D. Erhan, A. Courville, Y. Bengio, P. Vincent, “&lt;a href=&quot;https://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf&quot;&gt;Why Does Unsupervised Pre-training Help Deep Learning?&lt;/a&gt;”, &lt;em&gt;Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, PMLR 9:201-208, 2010.&lt;/p&gt;

&lt;p&gt;[20] (Gage 1994) Philip Gage. 1994. &lt;a href=&quot;https://www.derczynski.com/papers/archive/BPE_Gage.pdf&quot;&gt;A New Algorithm for Data Compression&lt;/a&gt;. C Users J., 12(2):23–38, February.&lt;/p&gt;

&lt;p&gt;[21] (Goodfellow et. al. 2016) Goodfellow, I., Y. Bengio, and A. Courville. 2016.&lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;Deep Learning&lt;/a&gt;. MIT Press.&lt;/p&gt;

&lt;p&gt;[22] (Hendrycks 2016) Dan Hendrycks, Kevin Gimpel, &lt;a href=&quot;https://arxiv.org/abs/1606.08415&quot;&gt;Gaussian Error Linear Units (GELUs)&lt;/a&gt;, 2016, arxiv&lt;/p&gt;

&lt;p&gt;[23] (Hinton et. al. 2006) Hinton, G. E., S. Osindero, and Y.-W. Teh. 2006. &lt;a href=&quot;https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf&quot;&gt;A fast learning algorithm for deep belief nets&lt;/a&gt;. Neural computation, 18(7):1527–1554.&lt;/p&gt;

&lt;p&gt;[24] (Hinton et. al. 2012) G. E. Hinton , N. Srivastava, A. Krizhevsky, I. Sutskever and R. R. Salakhutdinov, &lt;a href=&quot;https://arxiv.org/pdf/1207.0580.pdf&quot;&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/a&gt;, CoRR, 2012&lt;/p&gt;

&lt;p&gt;[25] (Hoffmann et. al. 2022) Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre, &lt;a href=&quot;https://arxiv.org/pdf/2203.15556.pdf&quot;&gt;Training Compute-Optimal Large Language Models&lt;/a&gt;, 2022, 2203.15556, arXiv&lt;/p&gt;

&lt;p&gt;[26] (Houlsby 2019) Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly, &lt;a href=&quot;https://arxiv.org/pdf/1902.00751.pdf&quot;&gt;Parameter-Efficient Transfer Learning for NLP&lt;/a&gt;, 2019, CoRR, abs/1902.00751&lt;/p&gt;

&lt;p&gt;[27] (Hochreiter and Schmidhuber, 1997) Hochreiter, S., &amp;amp; Schmidhuber, J. (1997). &lt;a href=&quot;https://www.bioinf.jku.at/publications/older/2604.pdf&quot;&gt;Long short-term memory&lt;/a&gt;. Neural computation, 9(8), 1735-1780.&lt;/p&gt;

&lt;p&gt;[28] (Huang et. al, 2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen, &lt;a href=&quot;https://arxiv.org/pdf/1811.06965.pdf&quot;&gt;GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism&lt;/a&gt;, 2019, arXiv:1811.06965v5&lt;/p&gt;

&lt;p&gt;[29] (Huggingface 2022) Huggingface, &lt;a href=&quot;https://arxiv.org/pdf/2211.05100.pdf&quot;&gt;BLOOM: A 176B-Parameter Open-Access Multilingual Language Model&lt;/a&gt;, 2022 https://arxiv.org/abs/2211.05100&lt;/p&gt;

&lt;p&gt;[30] (Kaplan et. al. 2020) Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess an Rewon Child an Scott Gray and Alec Radford an Jeffrey Wu an Dario Amodei, &lt;a href=&quot;https://arxiv.org/pdf/2001.08361.pdf&quot;&gt;Scaling Laws for Neural Language Models&lt;/a&gt;, CoRR, abs/2001.08361, 2020&lt;/p&gt;

&lt;p&gt;[31] (Kingma and Ba 2015) Kingma, D. and J. Ba. 2015. &lt;a href=&quot;https://arxiv.org/pdf/1412.6980.pdf&quot;&gt;Adam: A method for stochastic optimization&lt;/a&gt;. ICLR 2015&lt;/p&gt;

&lt;p&gt;[32] (Kudo and Richardson, 2018) Kudo, T., &amp;amp; Richardson, J. (2018). &lt;a href=&quot;https://arxiv.org/pdf/1808.06226.pdf&quot;&gt;SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing&lt;/a&gt;. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Brussels, Belgium.&lt;/p&gt;

&lt;p&gt;[33] (LeCun et. al. 1998) LeCun, Y., Bottou, L., Orr, G., &amp;amp; Müller, K. (1998). &lt;a href=&quot;https://cseweb.ucsd.edu/classes/wi08/cse253/Handouts/lecun-98b.pdf&quot;&gt;Efficient backprop. In Neural Networks: Tricks of the Trade&lt;/a&gt; (pp. 9-48). Springer.&lt;/p&gt;

&lt;p&gt;[34] (Lester et al., 2021) Lester, Brian  and Al-Rfou, Rami  and Constant, Noah, &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.243.pdf&quot;&gt;The Power of Scale for Parameter-Efficient Prompt Tuning&lt;/a&gt;, (https://aclanthology.org/2021.emnlp-main.243)&lt;/p&gt;

&lt;p&gt;[35] (Levy and Goldberg 2014) Omer Levy, Yoav Goldberg, &lt;a href=&quot;https://papers.nips.cc/paper_files/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf&quot;&gt;Neural word embedding as implicit matrix factorization&lt;/a&gt;, NIPS’14: Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2 December 2014 Pages 2177–2185&lt;/p&gt;

&lt;p&gt;[36] (Liu et. al. 2019) Liu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. 2019. &lt;a href=&quot;https://arxiv.org/pdf/1907.11692.pdf&quot;&gt;RoBERTa: A robustly optimized BERT pretraining approach&lt;/a&gt;. ArXiv preprint arXiv:1907.11692.&lt;/p&gt;

&lt;p&gt;[37] (Loshchilov and Hutter 2017) &lt;a href=&quot;https://arxiv.org/pdf/1711.05101.pdf&quot;&gt;Fixing Weight Decay Regularization in Adam&lt;/a&gt;. &lt;em&gt;I. Loshchilov, F. Hutter&lt;/em&gt;. 2017. Introduces &lt;strong&gt;AdamW&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;[38] (Mikolov 2013) Mikolov, T., Chen, K., Corrado, G., &amp;amp; Dean, J. (2013). &lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;. arXiv preprint arXiv:1301.3781.&lt;/p&gt;

&lt;p&gt;[39] (Mikolov et. al. 2010) Mikolov, T., M. Karafiat, L. Burget, J.  Cernock, and S. Khudanpur. 2010. &lt;a href=&quot;https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf&quot;&gt;Recurrent neural network based language model&lt;/a&gt;. INTERSPEECH.&lt;/p&gt;

&lt;p&gt;[40] (Nair and Hinton 2010) V. Nair and G. E. Hinton, “&lt;a href=&quot;https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf&quot;&gt;Rectified linear units improve restricted boltzmann machines&lt;/a&gt;,” in ICML, 2010, pp. 807–814.&lt;/p&gt;

&lt;p&gt;[41] (OpenAI, 2023) &lt;a href=&quot;https://arxiv.org/pdf/2303.08774.pdf&quot;&gt;GPT-4 Technical Report&lt;/a&gt;, 2023, OpenAI&lt;/p&gt;

&lt;p&gt;[42] (Ouyang et. al 2022) Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe, &lt;a href=&quot;https://arxiv.org/pdf/2203.02155.pdf&quot;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;, 2022, arXiv, 2203.02155&lt;/p&gt;

&lt;p&gt;[43] (Pennington et. al, 2014) Pennington, J., Socher, R., &amp;amp; Manning, C. D. (2014). &lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;&gt;GloVe: Global vectors for word representation&lt;/a&gt;. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532-1543).&lt;/p&gt;

&lt;p&gt;[44] (Peters et. al. 2018) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp;amp; Zettlemoyer, L. (2018). &lt;a href=&quot;https://arxiv.org/pdf/1802.05365.pdf&quot;&gt;Deep contextualized word representations&lt;/a&gt;. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT) (pp. 2227-2237).&lt;/p&gt;

&lt;p&gt;[45] (Press et. al. 2022) Press, Ofir and Smith, Noah A and Lewis, Mike, &lt;a href=&quot;https://arxiv.org/pdf/2108.12409.pdf&quot;&gt;Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation&lt;/a&gt;, arXiv preprint arXiv:2108.12409, 2022&lt;/p&gt;

&lt;p&gt;[46] (Radford et. al. 2018) Radford, A., Narasimhan, K., Salimans, T., &amp;amp; Sutskever, I. (2018). &lt;a href=&quot;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&quot;&gt;Improving Language Understanding by Generative Pre-training.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[47] (Radford et. al. 2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. &lt;a href=&quot;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;Language models are unsupervised multitask learners&lt;/a&gt;. OpenAI Blog, 1(8), 2019.&lt;/p&gt;

&lt;p&gt;[48] (Raffel et. al. 2019) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., … &amp;amp; Liu, P. J. (2019). &lt;a href=&quot;https://arxiv.org/pdf/1910.10683.pdf&quot;&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/a&gt;. arXiv preprint arXiv:1910.10683.&lt;/p&gt;

&lt;p&gt;[49] (Robbins and Monro 1951) Robbins, H., &amp;amp; Monro, S. (1951).&lt;a href=&quot;http://www.columbia.edu/~ww2040/8100F16/RM51.pdf&quot;&gt;A stochastic approximation method&lt;/a&gt;. The Annals of Mathematical Statistics, 22(3), 400-407.&lt;/p&gt;

&lt;p&gt;[50] (Rumelhart 1985) D. Rumelhart, G. Hinton, and R. Williams, “&lt;a href=&quot;https://cs.uwaterloo.ca/~y328yu/classics/bp.pdf&quot;&gt;Learning internal representations by error propagation&lt;/a&gt;,” UCSD, Tech. Rep., 1985.&lt;/p&gt;

&lt;p&gt;[51] (Sanh et. al. 2019) Sanh, V., Debut, L., Chaumond, J., &amp;amp; Wolf, T. (2019). &lt;a href=&quot;https://arxiv.org/pdf/1910.01108.pdf&quot;&gt;DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter&lt;/a&gt;. arXiv preprint arXiv:1910.01108.&lt;/p&gt;

&lt;p&gt;[52] (Sennrich et. al 2016) Sennrich, R., Haddow, B., &amp;amp; Birch, A. (2016). &lt;a href=&quot;https://aclanthology.org/P16-1162.pdf&quot;&gt;Neural machine translation of rare words with subword units&lt;/a&gt;. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), Berlin, Germany.&lt;/p&gt;

&lt;p&gt;[53] (Shazeer 2019) Noam Shazeer, &lt;a href=&quot;https://arxiv.org/pdf/1911.02150.pdf&quot;&gt;Fast Transformer Decoding: One Write-Head is All You Need,&lt;/a&gt;
 abs/1911.02150, 2019&lt;/p&gt;

&lt;p&gt;[54] (Shazeer 2020) Noam Shazeer, &lt;a href=&quot;https://arxiv.org/pdf/2002.05202.pdf&quot;&gt;GLU Variants Improve Transformer&lt;/a&gt;, 2020, CoRR, abs/2002.05202&lt;/p&gt;

&lt;p&gt;[55] (Schulman et. al. 2017) John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov, &lt;a href=&quot;https://arxiv.org/pdf/1707.06347.pdf&quot;&gt;Proximal Policy Optimization Algorithms&lt;/a&gt;, 2017, CoRR, abs/1707.06347&lt;/p&gt;

&lt;p&gt;[56] (Srivastava et. al. 2014) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov; ‘&lt;a href=&quot;https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf&quot;&gt;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&lt;/a&gt;’ JMLR 15(56):1929−1958, 2014.&lt;/p&gt;

&lt;p&gt;[57] (Sukhbaatar et. al. 2015) Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, ‘&lt;a href=&quot;https://arxiv.org/pdf/1503.08895.pdf&quot;&gt;End-To-End Memory Networks&lt;/a&gt;’, NeurIPS 2015&lt;/p&gt;

&lt;p&gt;[58] (Touvron et. al. 2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample, &lt;a href=&quot;https://arxiv.org/pdf/2302.13971.pdf&quot;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;, 2023, arxiv&lt;/p&gt;

&lt;p&gt;[59] (Vaswani 2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “&lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Attention is all you need&lt;/a&gt;,” in NIPS, 2017, pp. 6000–6010.&lt;/p&gt;

&lt;p&gt;[60] (Pennington et al 2014) Pennington, J., R. Socher, and C. D. Manning. 2014. &lt;a href=&quot;https://nlp.stanford.edu/pubs/glove.pdf&quot;&gt;GloVe: Global vectors for word representation&lt;/a&gt;. EMNLP.&lt;/p&gt;

&lt;p&gt;[61] (Wu et. al. 2016) &lt;em&gt;Yonghui Wu, M. Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, Yuan Cao, Qin Gao, Klaus Macherey, J. Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Y. Kato, Taku Kudo, H. Kazawa, K. Stevens, George Kurian, Nishant Patil, W. Wang, C. Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, G. Corrado, Macduff Hughes, J. Dean,&lt;/em&gt; &lt;a href=&quot;https://arxiv.org/pdf/1609.08144.pdf&quot;&gt;Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation&lt;/a&gt;. . 2016. Introduces &lt;strong&gt;WordPiece&lt;/strong&gt;. Used by BERT.&lt;/p&gt;

&lt;p&gt;[62] (Xiao et. al. 2023) Guangxuan Xiao and Ji Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han, &lt;a href=&quot;https://arxiv.org/pdf/2211.10438.pdf&quot;&gt;SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models&lt;/a&gt;, 2023, 2211.10438, arXiv&lt;/p&gt;

&lt;h2 id=&quot;appendix-a-word-vectors-and-distribution-hypothesis&quot;&gt;&lt;a name=&quot;appendix-a:-word-vectors-and-distribution-hypothesis&quot;&gt;&lt;/a&gt;Appendix A: Word Vectors and Distribution Hypothesis&lt;/h2&gt;

&lt;h3 id=&quot;embeddings-and-vector-semantics&quot;&gt;&lt;a name=&quot;embeddings-and-vector-semantics&quot;&gt;&lt;/a&gt;Embeddings and Vector Semantics&lt;/h3&gt;

&lt;p&gt;Distributional hypothesis first formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth (1957), 
“Words that occur in similar contexts tend to have similar meanings”&lt;/p&gt;

&lt;h4 id=&quot;distribution-hypothesis-was-studied-a-lot-in-lexical-semantics&quot;&gt;&lt;a name=&quot;distribution-hypothesis-was-studied-a-lot-in-lexical-semantics&quot;&gt;&lt;/a&gt;Distribution Hypothesis was studied a lot in lexical semantics&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Word similarity: Two words that are similar to each other exist in similar contexts&lt;/li&gt;
  &lt;li&gt;Word Relatedness: The meaning of two words can be related in ways other than relatedness similarity. One such class of connections is called word relatedness (Budanitsky association and Hirst, 2006)&lt;/li&gt;
  &lt;li&gt;Topics and Semantic Field: One common kind of relatedness between words is if they belong to the same semantic field semantic field. A semantic field is a set of words which cover a particular semantic domain and bear structured relations with each other. For example, words might be related by being in the semantic field of hospitals (surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof, topic models kitchen, family, bed). Semantic fields are also related to topic models, like Latent Dirichlet Allocation, LDA&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;words-as-vectors&quot;&gt;&lt;a name=&quot;words-as-vectors&quot;&gt;&lt;/a&gt;Words as vectors&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Words as vectors: Osgood’s 1957 idea mentioned above to use a point in three-dimensional space to represent the connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954), and Firth (1957) to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments.&lt;/li&gt;
  &lt;li&gt;Term document matrix: The term-document matrix was first defined as part of the vector space model of information retrieval (Salton, 1971).&lt;/li&gt;
  &lt;li&gt;Term frequency: term frequency (Luhn, 1957):&lt;/li&gt;
  &lt;li&gt;Inverse Document Frequency: (Sparck Jones, 1972)&lt;/li&gt;
  &lt;li&gt;Pointwise Mutual Information: (Fano, 1961), (Church and Hanks 1989, Church and Hanks 1990)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Measurement of similarity:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cosine similarity - inner dot product between vectors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;with sparse long vectors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Representing words as 300-dimensional dense vectors requires our classifiers to learn far fewer weights than if we represented words as 50,000-dimensional vectors,&lt;/li&gt;
  &lt;li&gt;and the smaller parameter space possibly helps with generalization and avoiding overfitting.&lt;/li&gt;
  &lt;li&gt;unable to handle unknown words&lt;/li&gt;
  &lt;li&gt;independence assumption (which is untrue, and a very strong assumption to make)&lt;/li&gt;
  &lt;li&gt;Dense vectors may also do a better job of capturing semantics, transfer learning, reasoning and other tasks that were previously hard to do.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dense embeddings like word2vec actually have an elegant mathematical relationship with sparse embeddings like PPMI, in which word2vec can be seen as implicitly optimizing a shifted version of a PPMI matrix &lt;a href=&quot;https://proceedings.neurips.cc/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf&quot;&gt;(Levy and Goldberg, 2014c)&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 13 Jul 2023 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/llm/ml/deep-learning/2023/07/13/ml-to-llm.html</link>
        <guid isPermaLink="true">http://localhost:4000/llm/ml/deep-learning/2023/07/13/ml-to-llm.html</guid>
        
        <category>llm</category>
        
        <category>ml</category>
        
        <category>deep-learning</category>
        
        
        <category>llm</category>
        
        <category>ml</category>
        
        <category>deep-learning</category>
        
      </item>
    
      <item>
        <title>Probability Theory for Natural Language Processing</title>
        <description>&lt;p&gt;A lot of work in &lt;a href=&quot;https://rutumulkar.com/blog/2017/what-is-nlp/&quot;&gt;Natural Language Processing (NLP)&lt;/a&gt; such a creation of &lt;a href=&quot;https://rutumulkar.com/blog/2021/language-models/&quot;&gt;Language Models&lt;/a&gt; is based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Probability_theory&quot;&gt;probability theory&lt;/a&gt;. For the purpose of NLP, knowing about probabilities of words can help us predict the next word, understanding the rarity of words, analyzing and knowing when to ignore common words with respect to a given context - e.g. articles such as “the” and “a” have a very high probability of occurring in any document and add less information to the overall semantics, where as the probability of “supercalifragilisticexpialidocious” is incredibly low, but having it in a sentence provides much more semantics.&lt;/p&gt;

&lt;p&gt;Probability theory deals with predicting how likely something will happen. Below are some concepts to know and understand about probability. 
In this post I will discuss the following:&lt;/p&gt;

&lt;!-- - [Experiment (Trial)](#experiment-trial)
- [Foundations](#foundations)
  - [Sample space](#sample-space)
  - [Event Space](#event-space)
  - [Disjoint Sets](#disjoint-sets)
  - [Well Founded Probability Space](#well-founded-probability-space)
- [Probability Theory](#probability-theory)
  - [Conditional Probability and Independence](#conditional-probability-and-independence)
  - [Chain Rule](#chain-rule)
  - [Independence of Events](#independence-of-events)
  - [Bayes Theorem](#bayes-theorem)
  - [Random Variables](#random-variables)
  - [Probability Mass Function](#probability-mass-function)
  - [Expectation](#expectation)
  - [Variance](#variance)
  - [Standard Deviation](#standard-deviation)
  - [Joint Probability Mass Function](#joint-probability-mass-function)
  - [Marginal Distribution](#marginal-distribution)
  - [Relative Frequency](#relative-frequency)
- [Distributions](#distributions)
  - [Continuous distribution](#continuous-distribution)
- [Maximum Likelihood Estimate](#maximum-likelihood-estimate)
  - [Bayesian Updating](#bayesian-updating)
  - [Likelihood Ratio](#likelihood-ratio)
- [Derivation](#derivation)
- [Bayes Optimal Decision](#bayes-optimal-decision) --&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;experiment-trial&quot;&gt;Experiment (Trial)&lt;/h1&gt;
&lt;p&gt;An experiment (also known as a trial) is a process by which an observation is made. Rolling a dice is a trial, observing the weather is a trial. The outcome of a trial is called an event.&lt;/p&gt;

&lt;h1 id=&quot;foundations&quot;&gt;Foundations&lt;/h1&gt;
&lt;h2 id=&quot;sample-space&quot;&gt;Sample space&lt;/h2&gt;
&lt;p&gt;The sample space is a collection of all the basic outcomes or events of our experiment.  Sample space can be discrete or continuous. In NLP, the sample space of a given dataset can be the exhausting combinations of words that can occur together as bigrams. In a trial with 2 dice, the sample space is all the combinations in which the dice can have an outcome. A sample space is represented as $\Omega$. $\phi$ represents an event that can never happen.&lt;/p&gt;

&lt;h2 id=&quot;event-space&quot;&gt;Event Space&lt;/h2&gt;
&lt;p&gt;An event space is the set of all the possible subsets of the sample space. The size of an event space is $2^{\Omega}$. An event space is represented as $\mathcal{F}$.&lt;/p&gt;

&lt;h2 id=&quot;disjoint-sets&quot;&gt;Disjoint Sets&lt;/h2&gt;
&lt;p&gt;Disjoint sets are sets that do not share any events with each other. E.g. in NLP if 2 datasets have no common vocabulary or vocabulary sequences, they are disjoint from one another. $A_j$ and $A_k$ are disjoint sets if they satisfy the following condition:&lt;/p&gt;

\[A_j \in \mathcal{F} (A_j \cap A_k = \phi, j \ne k)\]

\[P(\cup_{j=1}^{\inf}A_j) = \sum_{j=1}^{\inf} P(A_j)\]

&lt;h2 id=&quot;well-founded-probability-space&quot;&gt;Well Founded Probability Space&lt;/h2&gt;
&lt;p&gt;A well founded probability space contains:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sample space $\Omega$&lt;/li&gt;
  &lt;li&gt;a $\sigma$ field of events $\mathcal{F}$&lt;/li&gt;
  &lt;li&gt;A probability function (where all the individual probabilities sum to $1$)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;probability-theory&quot;&gt;Probability Theory&lt;/h1&gt;
&lt;h2 id=&quot;conditional-probability-and-independence&quot;&gt;Conditional Probability and Independence&lt;/h2&gt;
&lt;p&gt;Conditional Probability is the heart of &lt;a href=&quot;&quot;&gt;Naive Bayes&lt;/a&gt; algorithm. Conditional Probability measures the probability of an event given another event has occurred.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/cond_prob.png&quot; alt=&quot;Image of Conditional Probability&quot; width=&quot;400&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 1: Conditional Probability&lt;/em&gt;
&lt;/p&gt;

\[P(A|B) = \frac{P(A \cap B)}{P(B)}\]

&lt;p&gt;A simpler way of understanding conditional probability is the following: If event $B$ has definitely happened, how likely is it for event $A$ to also happen? This is answered by the fraction of times $A$ happens when $B$ happens.&lt;/p&gt;

&lt;h2 id=&quot;chain-rule&quot;&gt;Chain Rule&lt;/h2&gt;
&lt;p&gt;Chain rule of probabilities is used for Markov Models. According to chain rule, if we have 2 events $A$ and $B$, the probability of $A \cap B$ can be written as below:&lt;/p&gt;

\[P(A \cap B) = P(A|B) P(B)\]

&lt;p&gt;When events A and B occur together they are written as $P(A \cap B)$ or $P(A B)$&lt;/p&gt;

&lt;p&gt;For $n$ events, $A_1, A_2, \ldots, A_n$, the chain rule is:&lt;/p&gt;

\[P(A_1 \cap A_2 \ldots \cap A_n) = P(A_1) P(A_2|A_1) \ldots P(A_n|\cap_{i=1}^{n-1}A_i)\]

&lt;p&gt;Another notation is:&lt;/p&gt;

\[P(A_1 A_2 \ldots A_n) = P(A_1) P(A_2|A_1) \ldots P(A_n|\prod_{i=1}^{n-1}A_i)\]

&lt;p&gt;Applying this to NLP, we can compute the probability of the sentence “Pete is happy” by computing:&lt;/p&gt;

\[P(Pete, is, happy) = P(Pete) P(is| Pete) P(happy| is, Pete)\]

&lt;p&gt;For very long sentences, such joint probabilities are very hard to compute, e.g. the probability of the sentence “I am happy because I ate salad for lunch on Wednesday” depends on the frequency of this exact sentence in the corpus. It is quite possible that this exact sentence does not exist in our training dataset, cut the component words do exist. In which case it is helpful to make an independence assumption.&lt;/p&gt;

&lt;h2 id=&quot;independence-of-events&quot;&gt;Independence of Events&lt;/h2&gt;

&lt;p&gt;Two events $A$ and $B$ are independent of each other if $P(A B) = P(A) P(B)$. That means that there is no overlap between the two events with respect to Figure 1.&lt;/p&gt;

&lt;p&gt;When applying the independence assumption, $P(A|B) = P(A)$ because the presence of $B$ does not affect the probability of $A$ at all.&lt;/p&gt;

&lt;p&gt;Two events $A$ and $B$ are conditionally independent given an event $C$ where $P(C) &amp;gt; 0$ if:&lt;/p&gt;

\[P(A \cap B|C) = P(A|C) P(B|C)\]

&lt;h2 id=&quot;bayes-theorem&quot;&gt;Bayes Theorem&lt;/h2&gt;
&lt;p&gt;According to Bayes theorem:&lt;/p&gt;

\[P(B|A) = \frac{P(A|B) P(B)}{P(A)}\]

&lt;p&gt;The denominator is a normalizing factor, and it helps produce a probability function.&lt;/p&gt;

&lt;p&gt;If we are simply interested in which event is most likely given A, we can ignore the normalizing constant because it is the same value for all events.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bayes theorem is central to the noisy channel model&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;random-variables&quot;&gt;Random Variables&lt;/h2&gt;
&lt;p&gt;Random Variables map outcomes of random processes to numbers. Random variables are different from regular variables. Random variables can have many values, but regular variables can be solved for a small set of values. The probability $P(random|condition)$ helps with the mathematics notation of probability.&lt;/p&gt;

&lt;p&gt;The value held by a random variable can be discrete of continuous. Discrete is separate values e.g. 1, 2, 3, where as continuous is when the random variable can held any value within an interval e.g. between 0 and 1.&lt;/p&gt;

&lt;h2 id=&quot;probability-mass-function&quot;&gt;Probability Mass Function&lt;/h2&gt;
&lt;p&gt;Probability Mass Function (or PMF) of a discrete random variable $X$ provides the probabilities $P(X=x)$ for all the possible values of $x$.&lt;/p&gt;

&lt;h2 id=&quot;expectation&quot;&gt;Expectation&lt;/h2&gt;
&lt;p&gt;Expectation tells us what is the most likely outcome to expect. Expectation is the mean or average of the random variable:&lt;/p&gt;

\[E(X) = \sum_{x}xP(x)\]

&lt;p&gt;Calculating Expectation is central to Information Theory.&lt;/p&gt;

&lt;h2 id=&quot;variance&quot;&gt;Variance&lt;/h2&gt;
&lt;p&gt;The variance of a random variable is a measure of whether the values of the random variable tend to be consistent over trials or to vary a lot. Variance is represented as $\sigma^2$&lt;/p&gt;

\[\begin{eqnarray}
var(X) &amp;amp;=&amp;amp; E((X-E(X))^2 \nonumber\\
 &amp;amp;=&amp;amp; E(X^2)-E^2(X))
\end{eqnarray}\]

&lt;h2 id=&quot;standard-deviation&quot;&gt;Standard Deviation&lt;/h2&gt;
&lt;p&gt;Standard Deviation is the square root of variance. It is represented as $\sigma$.&lt;/p&gt;

&lt;h2 id=&quot;joint-probability-mass-function&quot;&gt;Joint Probability Mass Function&lt;/h2&gt;
&lt;p&gt;The joint probability of two events $x$ and $y$ is represented as:&lt;/p&gt;

\[P(x,y) = P(X=y, Y=y)\]

&lt;h2 id=&quot;marginal-distribution&quot;&gt;Marginal Distribution&lt;/h2&gt;
&lt;p&gt;Marginal PMFs total up the probability masses for the values of each variable separately.&lt;/p&gt;

\[P_X(x) = \sum_y P(x,y)\]

\[P_Y(y) = \sum_x P(x,y)\]

&lt;h2 id=&quot;relative-frequency&quot;&gt;Relative Frequency&lt;/h2&gt;
&lt;p&gt;The proportion of times a certain outcome occurs is called the relative frequency of the outcome.&lt;/p&gt;

&lt;p&gt;$P$ - probability function&lt;/p&gt;

&lt;p&gt;$p$ - probability mass function&lt;/p&gt;

&lt;p&gt;We take a parametric approach to estimate the probability function in language.&lt;/p&gt;

&lt;p&gt;Non-parametric approaches are used in classification, or when the underlying distribution of the data is unknown.&lt;/p&gt;

&lt;h1 id=&quot;distributions&quot;&gt;Distributions&lt;/h1&gt;
&lt;p&gt;The types of functions for probability mass functions are called distributions&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Discrete Distributions&lt;/strong&gt;: Binomial discrete distributions are a series of trails with 2 outcomes only.
    &lt;ul&gt;
      &lt;li&gt;Multi-nominal Distribution: A special case of binomial distribution is &lt;strong&gt;Multi-nominal Distribution&lt;/strong&gt; where each trial has more than 2 basic outcomes&lt;/li&gt;
      &lt;li&gt;Bernoulli distribution: A special case of discrete distributions where there is only one trial.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Continuous distributions&lt;/strong&gt; Continuous distributions are also known as Normal distributions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;continuous-distribution&quot;&gt;Continuous distribution&lt;/h2&gt;
&lt;p&gt;A continuous distribution is also known as a Normal Distribution or a Bell Curve. We also refer to them as Gaussians (often used in clustering). A Gaussian is represented as:&lt;/p&gt;

\[n(x; \mu, \sigma) = \frac{1}{\sigma \sqrt{2\pi}} e^{\frac{-(x-\mu)}{2\sigma^2}}\]

&lt;p&gt;Where $\sigma$ is the standard deviation and $\mu$ is the mean.&lt;/p&gt;

&lt;p&gt;It is also the probability density of observing a single data point $x$, that is generated from a gaussian distribution.&lt;/p&gt;

&lt;h1 id=&quot;maximum-likelihood-estimate&quot;&gt;Maximum Likelihood Estimate&lt;/h1&gt;
&lt;p&gt;Maximum Likelihood Estimate or MLE helps us identify which values of $\mu$ and $\sigma$ should we use that produce a curve that explains or covers all the data points. It is the way to determine the most likely outcome to a set of trials.&lt;/p&gt;

&lt;h2 id=&quot;bayesian-updating&quot;&gt;Bayesian Updating&lt;/h2&gt;
&lt;p&gt;The process of using prior to get posterior, posterior becomes the new prior, as new data comes.&lt;/p&gt;

\[P(\Theta|data) = P(data|\Theta) \text{ } P(\Theta)\]

&lt;p&gt;Here $\Theta$ is what we are interested in and what we are trying to estimate. It represented the set of parameters. If we are trying to estimate the parameters of a gaussian distribution then $\Theta$ represents both the mean $\mu$ and the standard deviation $\sigma$ and $\Theta = \{\mu, \sigma\}$&lt;/p&gt;

&lt;p&gt;Here $P(\Theta|data)$ is the posterior and $P(\Theta)$ is the prior.&lt;/p&gt;

&lt;p&gt;Prior belief is computed as the following:&lt;/p&gt;

\[P(s|\mu_m) = m^i (1-m)^j\]

&lt;p&gt;Here:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$i$ = outcome of 1 counts&lt;/li&gt;
  &lt;li&gt;$j$ = outcome of 2 counts&lt;/li&gt;
  &lt;li&gt;$\mu_m$ is the model that asserts the outcome&lt;/li&gt;
  &lt;li&gt;$s$ is a sequence of observations&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;likelihood-ratio&quot;&gt;Likelihood Ratio&lt;/h2&gt;
&lt;p&gt;Ratio computed between 2 models to see which of them is more likely to occur.&lt;/p&gt;

&lt;p&gt;People often take log likelihood ratio and see it the result is $&amp;gt;$ 1 or $&amp;lt;$ 1&lt;/p&gt;

&lt;p&gt;The ratio to determine which theory is most likely to occur given a sequence of events $s$.&lt;/p&gt;

&lt;p&gt;Let $v$ be the first theory and $\mu$ be the seond theory.&lt;/p&gt;

\[\frac{P(\mu|s)}{P(v|s)} = \frac{P(s|\mu) P(\mu)}{P(s|v)P(v)}\]

&lt;p&gt;If the ratio is $&amp;gt;1$ we prefer theory $\mu$ (the numerator).
If the ratio is $&amp;lt;1$ we prefer theory $v$ (the denominator).&lt;/p&gt;

&lt;h1 id=&quot;derivation&quot;&gt;Derivation&lt;/h1&gt;
&lt;p&gt;Derivation is the process of finding the maxima and minima of functions.&lt;/p&gt;

&lt;p&gt;Computing the partial derivative WRT $\mu$ and then setting the equation to $0$ gives us the MLE fo $\mu$&lt;/p&gt;

&lt;h1 id=&quot;bayes-optimal-decision&quot;&gt;Bayes Optimal Decision&lt;/h1&gt;
&lt;p&gt;When we pick the best decision theory out of all the available theories that could explain the data, we make the Bayes Optimal Decision.&lt;/p&gt;
</description>
        <pubDate>Sat, 08 May 2021 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/probability/2021/05/08/probability-theory.html</link>
        <guid isPermaLink="true">http://localhost:4000/probability/2021/05/08/probability-theory.html</guid>
        
        <category>nlp</category>
        
        <category>classification</category>
        
        
        <category>probability</category>
        
      </item>
    
      <item>
        <title>The Foundations of Language Models</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;&gt;Language Models&lt;/a&gt; are models that are trained to predict the next word, given a set of words that are already uttered or written. e.g. Consider the sentence:  &lt;em&gt;“Don’t eat that because it looks…“&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The next word following this will most likely be “disgusting”, or “bad”, but will probably not be “table” or “chair”. &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;&gt;Language Models&lt;/a&gt; are models that assign probabilities to sequences of words to be able to predict the next word given a sequence of words.&lt;/p&gt;

&lt;p&gt;The probability of a word $w$ given some history $h$ is $p(w|h)$.&lt;/p&gt;

&lt;h2 id=&quot;n-grams&quot;&gt;N-Grams&lt;/h2&gt;
&lt;p&gt;The probability of a word $w$ given the history $h$ is defined as:&lt;/p&gt;

\[p(w|h)\]

&lt;p&gt;as $h$ is several tokens long, we can rephrase it as the probability of the ${n+1}^{th}$ word $w_{n+1}$ depends on the words $w_1, w_2 \cdots w_n$.&lt;/p&gt;

\[p(w_{n+1}|w_1, w_2, w_3 \cdots w_n)\]

&lt;p&gt;One way to answer this is using relative frequency counts. i.e. count the number of times we see $w_1, w_2, w_3 \cdots w_n$ and the number of times we see $w_1, w_2, w_3 \ldots w_n$ followed by $w_{n+1}$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.mathsisfun.com/data/relative-frequency.html&quot;&gt;Relative frequency&lt;/a&gt; is defined as the ratio of an observed sequence to the observed sequence followed by a suffix. Using the concept of relative frequency we can get:&lt;/p&gt;

\[P(w_{n+1}|w_1 w_2 w_3 \ldots w_n) = \frac{C(w_1 w_2 w_3 \cdots w_n w_{n+1})}{C(w_1 w_2 w_3 \ldots w_n)}\]

&lt;p&gt;Where $C(x_1, x_2)$ is the count or the number of times we see a pattern of token $x_1$ followed by $x_2$.&lt;/p&gt;

&lt;p&gt;This approach of getting probabilities from counts works well in many cases, but if we wanted to know the joint probability of an entire sequence of words $p(w_1, w_2, w_3 \ldots w_n)$, we would have to compute - out of all the possible combinations of size $n$ how many are this exact sequence $w_1, w_2, w_3 \ldots w_n$. It fails when the size of the sequence is very long.&lt;/p&gt;

&lt;p&gt;Even the entire web isn’t big enough to compute such probabilities, because there are not enough examples for every word combination even on the world wide web). For this reason, we will have to introduce clever ways of computing probability.&lt;/p&gt;

&lt;p&gt;Let’s decompose this joint probability into a conditional probabilities using the chain rule of probability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Chain_rule_(probability)&quot;&gt;Chain Rule of Probability&lt;/a&gt; helps us decompose a joint probability into a conditional probability of a word given previous words. &lt;a href=&quot;https://machinelearningmastery.com/joint-marginal-and-conditional-probability-for-machine-learning/&quot;&gt;Joint Probability&lt;/a&gt; of $n$ words is the probability of $n$ words occurring together. Using the chain rule, we can break down Equation the joint probability of a sequence of tokens into conditional probabilities. Here is how we do it:&lt;/p&gt;

\[\begin{eqnarray}
P(w_1 w_2 w_3 \cdots w_n w_{n+1}) &amp;amp;=&amp;amp; P(w_1) P(w_2|w_1) P(w_3|w_1 w_2) \ldots P(w_n|w_1\ldots w_{n-1}) \nonumber \\ 
&amp;amp;=&amp;amp; \prod_{k=1}^{n+1} P(w_k|w_1 \ldots w_k)
\end{eqnarray}\]

&lt;p&gt;The chain rule still has the constraint of needing the probability of a long sequence of previous words. One idea is to approximate this using the Markov Assumption.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_property&quot;&gt;Markov Assumption&lt;/a&gt; says that the probability of a word only depends on the previous word and not the entire sequence of tokens preceding it. According to this assumption, we can predict a word without looking too much into the previous history of words. So, instead of working with the exact probabilities of a long sequence of preceding words, we can use a small window of preceding words. This is where we introduce a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bigram&quot;&gt;bigram model&lt;/a&gt; (that uses the preceding word only) a &lt;a href=&quot;https://en.wikipedia.org/wiki/Trigram&quot;&gt;trigram model&lt;/a&gt; (that uses the preceding two words) to predict the probability of a sequence of tokens.&lt;/p&gt;

&lt;p&gt;Using the bigram model we can simplify the probability of a sequence of tokens to the following:&lt;/p&gt;

\[P(w_1, w_2, w_3 \cdots w_n, w_{n+1}) = \prod_{k=1}^n P(w_k|w_{k-1})\]

&lt;p&gt;Similarly using the trigram model we can simplify the probability of a sequence of tokens to the following:&lt;/p&gt;

\[P(w_1, w_2, w_3 \cdots w_n, w_{n+1}) = \prod_{k=1}^n P(w_k|w_{k-1} w_{k-2})\]

&lt;p&gt;Now that we have simplified the RHS, we need to compute the probabilities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#:~:text=In%20statistics%2C%20maximum%20likelihood%20estimation,observed%20data%20is%20most%20probable.&quot;&gt;Maximum Likelihood Estimation (MLE)&lt;/a&gt; is an intuitive way of measuring the parameters of an N-gram model, by computing the counts of words or tokens that exist together, normalized by the total counts so that the output is a probability that is between 0 and 1. Relative frequency is one way of measuring the MLE. Relative frequency can be computed using equation (7) below and Figure (1) explains the equation:&lt;/p&gt;

\[P(w_n|w_{n-1}) = \frac{C(w_{n-1}w_n)}{\sum_w C(w_{n-1}w)}\]

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://localhost:4000/assets/images/relative-frequency.png&quot; alt=&quot;Relative Frequency&quot; width=&quot;500&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 1: Relative Frequency Explained&lt;/em&gt;
&lt;/p&gt;

&lt;h3 id=&quot;working-example&quot;&gt;Working Example&lt;/h3&gt;
&lt;p&gt;Let’s build a small language model using bigrams, and use our model to predict the probability of a new sentence. For our toy example, consider a tiny corpus of the following sentences:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&amp;lt;s&amp;gt; I am Sam &amp;lt;\s&amp;gt;&lt;/p&gt;

  &lt;p&gt;&amp;lt;s&amp;gt; Sam I am &amp;lt;\s&amp;gt;&lt;/p&gt;

  &lt;p&gt;&amp;lt;s&amp;gt; I do not like that Sam I am &amp;lt;\s&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here &amp;lt;s&amp;gt; represents the start of a sentence and &amp;lt;\s&amp;gt; represents the end of the sentence.&lt;/p&gt;

&lt;p&gt;This corpus has the following lexicon or unique words:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;token&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;frequency&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;I&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;am&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sam&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;do&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;not&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;like&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;that&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Computing the conditional probabilities&lt;/p&gt;

\[P(I|&amp;lt;s&amp;gt;) = \frac{2}{4}\]

\[P(Sam|&amp;lt;s&amp;gt;) = \frac{1}{3}\]

\[P(am|I) = \frac{3}{4}\]

&lt;p&gt;We can continue computing probabilities for all different possibilities of bigrams, then for any new sentence such as  the following:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I like Sam&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We can compute the join probability of the tokens of this sentence from the conditional probability of the bigrams $P(like|I)$ and $P(Sam|like)$.&lt;/p&gt;

&lt;h3 id=&quot;practical-issues&quot;&gt;Practical Issues&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Always use Log probabilities. Multiplication in linear space is addition in log space, and we will avoid numerical underflow&lt;/li&gt;
  &lt;li&gt;It is typical to use trigrams instead of bigrams (although we illustrated bigrams in the example above)&lt;/li&gt;
  &lt;li&gt;There are often unknown words in the sentence we want to predict the probability of, and we need to handle that.&lt;/li&gt;
  &lt;li&gt;We often won’t find instances of joint probability in our corpus, and we need to account for that. E.g. in the above example we do not have an instance of ‘sam like’, and so the conditional probability of $P(Sam|like)$ will be $0$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;problems-with-language-models&quot;&gt;Problems with language Models&lt;/h2&gt;
&lt;p&gt;Language Models face the issue of &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;&gt;sparsity&lt;/a&gt; - which means that the training corpus is limited and some perfectly acceptable English word sequences are bound to be missing from it. This means that it is possible to have several n-grams with a probability of $0$, but should actually have a non-zero probability.&lt;/p&gt;

&lt;p&gt;Another issue with language models is that the vocabulary the the language model is trained on might not have seen words from the test dataset - introducing the issue of unknown words or &lt;a href=&quot;https://groups.csail.mit.edu/sls/publications/2000/03105.pdf&quot;&gt;out of vocabulary words(OOV)&lt;/a&gt;. One way to deal with OOV words is to replace all words with a frequency below a certain threshold by ‘UNK’. Other ways to deal with this is to use smoothing and discounting techniques.&lt;/p&gt;

&lt;h2 id=&quot;smoothing&quot;&gt;Smoothing&lt;/h2&gt;
&lt;p&gt;There will always be unknown words in the test dataset that the language model will have to work with that sparsity. To keep the language model from assigning zero probabilities to unseen events, we will shave off some probability mass from some more frequent events and give it to the events that we have never seen. This process is called &lt;a href=&quot;https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf&quot;&gt;smoothing&lt;/a&gt; or &lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1086/handouts/cs224n-lecture2-language-models-slides.pdf&quot;&gt;discounting&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A few types of smoothing are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;add-1 smoothing (or &lt;a href=&quot;****&quot;&gt;Laplace smoothing&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;add-k smoothing&lt;/li&gt;
  &lt;li&gt;backoff and interpolation&lt;/li&gt;
  &lt;li&gt;stupid backoff&lt;/li&gt;
  &lt;li&gt;Kneser ney smoothing&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;laplace-smoothing&quot;&gt;Laplace Smoothing&lt;/h3&gt;
&lt;p&gt;Laplace smoothing involves adding $1$ to all of the bigram (or n-gram) counts before we normalize them into probabilities.&lt;/p&gt;

\[P(w_i) = \frac{c_i}{N}\]

&lt;p&gt;If we add $1$ to each probability, and there are $V$ words in the vocabulary, we will need to add $V$ to the denominator as we add $1$ to each numerator.&lt;/p&gt;

\[P_{laplace}(w_i) = \frac{c_i}{N+V} \label{laplace}\]

&lt;p&gt;In equation \ref{laplace}, $w_i$ is the $i^{th}$ word, $N$ is a normalizer (total number of words) and $V$ is the vocabulary size.&lt;/p&gt;

&lt;p&gt;But instead of adding to the numerator and denominator, a better way to is change the numerator only, and show how it affects smoothing by describing an adjusted count $c^*$.&lt;/p&gt;

\[c_i^* = (c_i + 1) \frac{N}{N+V}\]

&lt;p&gt;Now we can convert each adjusted count into a probability by dividing it by $N$ (the total number of tokens)&lt;/p&gt;

&lt;p&gt;Although Laplace smoothing isn’t the best type of smoothing as it gives away a lot of probability mass to infrequent terms, it is still used and is practical for classification.&lt;/p&gt;

&lt;p&gt;** Question: How can you use Language Models for classification? **&lt;/p&gt;

\[P(w_n|w_{n-1}) = \frac{C(w_{n-1} w_n)}{C(w_{n-1})}\]

&lt;p&gt;Using Laplace Smoothing, this becomes:&lt;/p&gt;

\[p^*_{Laplace}(w_n|w_{n-1}) = \frac{C(w_{n-1} w_n) + 1}{\sum_w C(w_{n-1}w) + V}\]

&lt;h3 id=&quot;add-k-smoothing&quot;&gt;Add-k smoothing&lt;/h3&gt;
&lt;p&gt;Add-K smoothing is an alternative to add $1$ smoothing, where we move a bit less of the probability mass from the seen events to the unseen events.&lt;/p&gt;

\[p^*_{add-k}(w_n|w_{n-1}) = \frac{C(w_{n-1} w_n) + k}{\sum_w C(w_{n-1}w) + kV}\]

&lt;p&gt;Add-K requires us to have a method for choosing our k (0.5? 0.1? 0.05?) e.g. one can optimize over a dev set or some other data source.&lt;/p&gt;

&lt;h3 id=&quot;backoff-and-interpolation&quot;&gt;Backoff and Interpolation&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Katz%27s_back-off_model&quot;&gt;Backoff&lt;/a&gt; is an approach for smoothing using which we only backoff to a lower order n-gram when we have zero evidence for a higher level n-gram.&lt;/p&gt;

&lt;p&gt;So, we use a trigram if the evidence is sufficient, but if such a trigram does not exist, we backoff to a bigram, and if the bigram does not exist, we backoff to a unigram.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf&quot;&gt;Interpolation&lt;/a&gt; is an approach is using a mixture of probability estimates from all the n-gram estimators. For instance, if we are looking at trigrams, we would compute its probability by combining the trigram, bigram and unigram counts.&lt;/p&gt;

&lt;p&gt;Interpolation for a trigram can be defined by the following formula:&lt;/p&gt;

\[\hat{p}(w_n|w_{n-2}w_{n-1}) = \lambda_1 p(w_n|w_{n-2}w_{n-1}) + \lambda_2 p(w_n|w_{n-1}) + \lambda_3 p(w_n)\]

&lt;p&gt;where $\sum_i \lambda_i = 1$&lt;/p&gt;

&lt;p&gt;The values of $\lambda$ can be computed by optimizing over a heldout dataset. &lt;a href=&quot;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&quot;&gt;EM Algorithm (Expectation Maximization Algorithm)&lt;/a&gt; is an iterative learning algorithm that converges on locally optimal $\lambda$s&lt;/p&gt;

&lt;h2 id=&quot;evaluating-language-models&quot;&gt;Evaluating Language Models&lt;/h2&gt;
&lt;p&gt;There are 2 ways to evaluate language models:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;intrinsic evaluation: Evaluation of the model as a measure of how much it improves the application that it is used in.&lt;/li&gt;
  &lt;li&gt;extrinsic evaluation: Measure of the quality of the model independent of any application&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;intrinsic-evaluation&quot;&gt;Intrinsic Evaluation&lt;/h3&gt;
&lt;p&gt;For intrinsic evaluation of a language model we need to have:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;training dataset&lt;/li&gt;
  &lt;li&gt;test dataset&lt;/li&gt;
  &lt;li&gt;held out dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We use the language model to compute scores on the test dataset, and use the heldout and training dataset to optimize out language model.&lt;/p&gt;

&lt;h3 id=&quot;extrinsic-evaluation&quot;&gt;Extrinsic Evaluation&lt;/h3&gt;
&lt;p&gt;To compute extrinsic evaluation of a language model, we compute the effect of a new language model on the final end to end product that it is integrated with. Good scores during intrinsic evaluation does not always mean better scores during extrinsic evaluation, which is why both the types of evaluation are important.&lt;/p&gt;

&lt;h4 id=&quot;perplexity&quot;&gt;Perplexity&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Perplexity&quot;&gt;Perplexity&lt;/a&gt; is the measure of computation of the probabilities learned from the training dataset and applied on the test dataset. Perplexity is represented as $PP$ and is measured as the inverse probability of the test set, normalized by the number of words.&lt;/p&gt;

\[\begin{eqnarray}
PP(w) &amp;amp;=&amp;amp; P(w_1 w_2 w_3 \ldots w_n)^{-\frac{1}{n}} \nonumber \\
&amp;amp;=&amp;amp; \sqrt[n]{\frac{1}{P(w_1 w_2 w_3 \ldots w_n)}} \nonumber \\
&amp;amp;=&amp;amp; \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i|w_1 w_2 \ldots w_{i-1})}}
\end{eqnarray}\]

&lt;p&gt;To compute perplexity of a bigram, we can simplify Equation (11) to the following:&lt;/p&gt;

\[PP(w) = \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i|w_{i-1})}}\]

&lt;p&gt;Similarly, to compute perplexity of a bigram, we can simplify Equation (11) to the following:&lt;/p&gt;

\[PP(w) = \sqrt[n]{\frac{1}{\prod_{i=1}^nP(w_i|w_{i-1} w_{i-2})}}\]

&lt;blockquote&gt;
  &lt;p&gt;Minimizing perplexity is equivalent to maximizing the test set probability. If the perplexity is low, it means that the training data captures the probability of the test set really well.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another way to think about perplexity is to think of it as the weighted average branching factor of the language. The &lt;a href=&quot;https://towardsdatascience.com/perplexity-in-language-models-87a196019a94&quot;&gt;branching factor&lt;/a&gt; of a language is the number of possible next words that can follow any word.&lt;/p&gt;

&lt;p&gt;Intrinsic improvement in perplexity does not guarantee an extrinsic improvement in the performance of the language processing task.&lt;/p&gt;

&lt;h2 id=&quot;n-gram-efficiency-considerations&quot;&gt;N-Gram Efficiency considerations&lt;/h2&gt;
&lt;p&gt;When a language model uses large sets of n-grams, it is important to store the efficiently. Below are some ways to store LMs efficiently:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Words: storing words in 64 bit hash representations, and the actual words are stored on disc as string&lt;/li&gt;
  &lt;li&gt;Probabilities: 4-8 bits instead of 8 byte float&lt;/li&gt;
  &lt;li&gt;n-grams: Stored in reverse &lt;a href=&quot;https://en.wikipedia.org/wiki/Trie&quot;&gt;tries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Approximate language models can be created using techniques such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Bloom_filter&quot;&gt;bloom filters&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;n-grams can be shrunk by pruning i.e. only storing n-grams with counts greater than some threshold.&lt;/li&gt;
  &lt;li&gt;Efficient Language Models such as &lt;a href=&quot;https://github.com/kpu/kenlm&quot;&gt;KenLM&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Use sorted Arrays&lt;/li&gt;
      &lt;li&gt;Efficiently combined probabilities and backoffs into a single value&lt;/li&gt;
      &lt;li&gt;Use merge sorts to efficiently build probability tables in a minimal number of passes through a large corpus&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/3.pdf&quot;&gt;Chapter 3, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition&lt;/a&gt; by Daniel Jurafsky, James H. Martin&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;&gt;Language Models&lt;/a&gt;, Wikipedia&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1086/handouts/cs224n-lecture2-language-models-slides.pdf&quot;&gt;Lecture 2, Language Models&lt;/a&gt;, CS224n Stanford NLP&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf&quot;&gt;NLP Lunch Tutorial: Smoothing&lt;/a&gt;, Bill MacCartney, Stanford NLP&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mathsisfun.com/data/relative-frequency.html&quot;&gt;Relative Frequency&lt;/a&gt;, mathisfun.com&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 24 Apr 2021 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/language%20models/2021/04/24/language-models.html</link>
        <guid isPermaLink="true">http://localhost:4000/language%20models/2021/04/24/language-models.html</guid>
        
        <category>language_models</category>
        
        
        <category>language models</category>
        
      </item>
    
      <item>
        <title>The Comprehensive Guide to Logistic Regression</title>
        <description>&lt;p&gt;In &lt;a href=&quot;https://algorithmia.com/blog/introduction-natural-language-processing-nlp&quot;&gt;Natural Language Processing&lt;/a&gt; (NLP) &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;Logistic Regression&lt;/a&gt; is the baseline supervised ML algorithm for &lt;a href=&quot;https://en.wikipedia.org/wiki/Classification&quot;&gt;classification&lt;/a&gt;. It also has a very close relationship with &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_network&quot;&gt;neural networks&lt;/a&gt; (If you are new to neural networks, start with Logistic Regression to understand the basics.)&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Logistic Regression is a &lt;a href=&quot;https://medium.com/@akankshamalhotra24/generative-classifiers-v-s-discriminative-classifiers-1045f499d8cc#:~:text=An%20example%20of%20a%20discriminative,decision%20boundary%20for%20the%20model.&quot;&gt;discriminative classifier&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Discriminative models try to learn to &lt;strong&gt;distinguish&lt;/strong&gt; what different classes of data look like.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some examples of discriminative classifiers are &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;Logistic Regression&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_neural_network&quot;&gt;Neural Networks&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Conditional_random_field&quot;&gt;Conditional Random Fields&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Support-vector_machine&quot;&gt;Support Vector Machines&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Naive Bayes&lt;/a&gt;, is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_model&quot;&gt;generative classifier&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Generative models have the goal to &lt;strong&gt;understand&lt;/strong&gt; what different classes of data look like.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some examples of generative classifiers include &lt;a href=&quot;https://en.wikipedia.org/wiki/Naive_Bayes_classifier&quot;&gt;Naive Bayes&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_network&quot;&gt;Bayesian Networks&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_random_field&quot;&gt;Markov Random Fields&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Hidden_Markov_model&quot;&gt;Hidden Markov Models&lt;/a&gt;. &lt;a href=&quot;https://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/&quot;&gt;LDA&lt;/a&gt; (Latent Dirichlet Allocation is a generative statistical model for topic modeling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf&quot;&gt;Ng and Jordan, 2001&lt;/a&gt; provide a great analysis of generative vs discriminative models.&lt;/p&gt;

&lt;h1 id=&quot;components-of-a-classification-system&quot;&gt;Components of a Classification System&lt;/h1&gt;

&lt;p&gt;A Machine Learning system for classification has 4 components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Feature Representation: For each input observation $x^{(i)}$, this will be a vector of features $[x_1, x_2, x_3, … , x_n]$&lt;/li&gt;
  &lt;li&gt;A Classification Function: This gets us the probability of the output, given an input. This is denoted by $P(y|x)$. &lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;Sigmoid&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;&gt;Softmax&lt;/a&gt; are tools for classification for Logistic Regression.&lt;/li&gt;
  &lt;li&gt;Objective Function for Learning: This is the function that we want to optimize, usually involving minimizing error on training examples. Cross Entropy Loss is the objective function for Logistic Regression.&lt;/li&gt;
  &lt;li&gt;Algorithm for Optimizing the Objective Function: We use the Stochastic Gradient Descent Algorithm for optimizing over our Objective Function.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;logistic-regression-phases&quot;&gt;Logistic Regression Phases&lt;/h2&gt;

&lt;p&gt;Logistic Regression has two phases:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Training Phase: We train the system (specifically the weights $w$ and bias $b$) using Stochastic Gradient Descent and Cross Entropy Loss&lt;/li&gt;
  &lt;li&gt;Test Phase: Given a text example $x$, we compute $p(y|x)$ and return the higher probability label.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;feature-representation&quot;&gt;Feature Representation&lt;/h1&gt;

&lt;p&gt;A single input observation $x$ can be represented by a vector of features $[x_1, x_2, x_3, \ldots , x_n]$. For Logistic Regression this is usually done by &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_engineering&quot;&gt;feature engineering&lt;/a&gt; which is the process of manually identifying which features are relevant to solve the problem, and convert the text features into real numbers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Feature Interactions&lt;/strong&gt;: Feature Interaction is the combination of different features to form a more complex feature.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Feature Templates&lt;/strong&gt;: Feature Templates is when templates are used to automatically create features using abstract specification of features.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Representation Learning&lt;/strong&gt;: &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_learning&quot;&gt;Representation Learning&lt;/a&gt; is the process of learning features automatically in an unsupervised way from the input. In order to avoid the excessive human effort of feature design, recent NLP efforts are focused on representation learning&lt;/p&gt;

&lt;h1 id=&quot;classification-using-the-sigmoid-function&quot;&gt;Classification using the Sigmoid Function&lt;/h1&gt;

&lt;p&gt;The result of Logistic Regression is:&lt;/p&gt;

\[z = (\sum_{i=1}^n w_i x_i) + b\]

&lt;p&gt;Here, $w_i \cdot x_i$ is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dot_product&quot;&gt;dot product&lt;/a&gt; of vectors $x_i$ and $w_i$ and $z$ is a real number vector ranging from $-\infty$ to $+\infty$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dot Product: The dot product of 2 vectors $a$ and $b$ is written as $a \cdot b$ and is the sum of the products of the corresponding elements of each vector.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$z$ is not a legal probability. To make it a probability, $z$ will pass through the, written as $\sigma(z)$&lt;/p&gt;

\[y = \sigma(z) = \frac{1}{(1+e^{-z})}\]

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/sigmoid.jpg&quot; alt=&quot;Image of Sigmoid Function&quot; width=&quot;400&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 1: Sigmoid Function (Image Credit: Wikipedia)&lt;/em&gt;
&lt;/p&gt;

&lt;h2 id=&quot;characteristics-of-sigmoid-function&quot;&gt;Characteristics of Sigmoid Function&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Sigmoid takes a real valued number and maps it to the range [0, 1] (which is perfect to get a probability)&lt;/li&gt;
  &lt;li&gt;Sigmoid tends to squash outlier values towards 0 or 1&lt;/li&gt;
  &lt;li&gt;Sigmoid is differentiable - which makes it handy for learning (A function is not differentiable if it has a undefined slope or a vertical slope)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For classification into two classes:&lt;/p&gt;

\[\begin{eqnarray}
p(y=1) &amp;amp;=&amp;amp; \sigma(w \cdot x) + b \nonumber \\
    &amp;amp;=&amp;amp; \frac{1}{1+e^{w.x + b}}
\end{eqnarray}\]

\[\begin{eqnarray}
p(y=0) &amp;amp;=&amp;amp; 1 - \sigma(w \cdot x) + b \nonumber \\
&amp;amp;=&amp;amp; 1 - \frac{1}{1+e^{w.x + b}} \nonumber \\
&amp;amp;=&amp;amp;\frac{e^{w.x + b}}{1+e^{w.x + b}}
\end{eqnarray}\]

&lt;h1 id=&quot;learning-process-in-logistic-regression&quot;&gt;Learning Process in Logistic Regression&lt;/h1&gt;

&lt;h2 id=&quot;cost-function-cross-entropy-loss&quot;&gt;Cost Function: Cross Entropy Loss&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://machinelearningmastery.com/cross-entropy-for-machine-learning/&quot;&gt;Cross Entropy Loss&lt;/a&gt; is a function that determines for an observation $x$, how close the output of the classifier $\hat{y}$ is to the correct output $y$. This $Loss$ is expressed as: $ L(\hat{y}, y) $&lt;/p&gt;

&lt;p&gt;$ L(\hat{y}, y)$ is computed via a loss function that prefers the correct class labels of the training examples to be more likely. This is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;conditional maximum likelihood estimation&lt;/a&gt;. We choose parameters $w$ and $b$ that maximize the probability of the true $y$ labels in the training data given the observations $x$.&lt;/p&gt;

&lt;p&gt;Given a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bernoulli_distribution&quot;&gt;Bernoulli Distribution&lt;/a&gt; (a distribution that can only have 2 outcomes):&lt;/p&gt;

\[p(y|x) = \hat{y}^y (1-\hat{y})^{1-y}\]

&lt;p&gt;Taking log on both sides:&lt;/p&gt;

\[\log{p(y|x)} = y\log{\hat{y}} + (1-y)\log{(1-\hat{y})}\]

&lt;p&gt;Equation (6) is what we are trying to maximize. In order to turn this into a loss function (something that we need to minimize), we just flip the sign on Equation $(6)$. The result is Cross Entropy Loss $L_{CE}$.&lt;/p&gt;

\[\begin{eqnarray}
L_{CE}(\hat{y}, y) &amp;amp;=&amp;amp; -\log{ p(y|x)} \nonumber \\

                   &amp;amp;=&amp;amp; -[y \log{\sigma(w \cdot x + b) + (1-y)\log(1-\sigma(w \cdot x + b))}]
\end{eqnarray}\]

&lt;p&gt;Equation (7) is known as the cross entropy loss. It is also the formula for the cross entropy between the true probability distribution $y$ and the estimated distribution $\hat{y}$.&lt;/p&gt;

&lt;h3 id=&quot;cross-entropy&quot;&gt;Cross Entropy&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;&quot;&gt;Cross Entropy&lt;/a&gt; is the measure of the difference between two probability distributions for a given random variable.&lt;/p&gt;

&lt;h3 id=&quot;convex-optimization-problem&quot;&gt;Convex Optimization Problem&lt;/h3&gt;
&lt;p&gt;For logistic regression the loss function is convex, i.e. it has just one minimum. There is no local minima to get stuck in, so gradient descent will always find the minimum. The loss for multi-layer neural networks in non-convex, so it is possible to get stuck in local minima using neural networks.&lt;/p&gt;

&lt;h3 id=&quot;decision-boundary&quot;&gt;Decision Boundary&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;&quot;&gt;Decision Boundary&lt;/a&gt; is the threshold above which $\hat{y} = 1$, and below which $\hat{y} = 0$. This means that the decision boundary decides which class a given instance belongs to, based on the final probability of the item and the value of the decision boundary.&lt;/p&gt;

\[\hat{y} = \left\{
    \begin{array}{ll}
        1 &amp;amp; \mbox{if $p(y=1|x)$ $\gt$ 0.5}\\
        0 &amp;amp; \mbox{otherwise}
    \end{array}
\right.\]

&lt;p&gt;Here 0.5 is the decision boundary, and it is the threshold that decides which class an item belongs to.&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;Gradient Descent&lt;/a&gt; finds the gradient of the loss function at the current point (by taking a differential of it), and then moves in the opposite direction of gradient.&lt;/p&gt;

&lt;h3 id=&quot;learning-rate&quot;&gt;Learning Rate&lt;/h3&gt;
&lt;p&gt;The magnitude of the amount to move in gradient descent is the slope $ \frac{d}{dw} \;  f(x,w)$ weighted by the learning rate $\eta$.&lt;/p&gt;

\[w^{t+1} = w^{t} - \eta \; \frac{d}{dw} \; f(x; w)\]

&lt;p&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;: A parameter that needs to be adjusted:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the learning rate is too high, each step towards learning becomes too large and it overshoots the minimum&lt;/li&gt;
  &lt;li&gt;If the learning rate is too low, each step is too small and it takes a long time to get to the minimum&lt;/li&gt;
  &lt;li&gt;Start $\eta$ at a higher value and slowly decrease it, so that it is a function of the iteration $k$ in training&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic Gradient Descent&lt;/h3&gt;
&lt;p&gt;It is called Stochastic Gradient Descent because it chooses a single random example at a time. It moves weights so that it can improve the performance of that single instance.&lt;/p&gt;

&lt;h3 id=&quot;batch-training&quot;&gt;Batch Training&lt;/h3&gt;
&lt;p&gt;When we use Batch Training, we compute the gradient over the entire dataset. It looks at all the examples for each iteration to decide the next step.&lt;/p&gt;

&lt;h3 id=&quot;mini-batch-training&quot;&gt;Mini-Batch Training&lt;/h3&gt;
&lt;p&gt;We train a group of $m$ examples (where $m$ is 512, or 1024 or similar) that is less than the size of the entire dataset. When $m = 1$ it becomes Stochastic Gradient Descent again. Mini-Batch Training is more efficient than Batch Training and more accurate than Stochastic Gradient Descent. Mini-Batch Gradient Descent is the average of the individual gradients.&lt;/p&gt;

&lt;h2 id=&quot;regularization&quot;&gt;Regularization&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Regularization_(mathematics)&quot;&gt;Regularization&lt;/a&gt; is used to avoid overfitting and to generalize well for the test data. If the model fits the training data too well, it will not be able to handle new cases presented in the test data.&lt;/p&gt;

&lt;p&gt;A regularization term $R(\theta)$ is added to the objective function. i.e. the function that computes $\hat{\theta}$, where $\hat{\theta}$ is the next set of $w$ and $b$ parameters.&lt;/p&gt;

&lt;p&gt;Once we add regularization, we can find $\hat{\theta}$ as:&lt;/p&gt;

\[\hat{\theta} = argmax_{\theta} \sum_{i=0}^m log(p(y^{(i)}\|x^{(i)})) - \alpha R(\theta)\]

&lt;p&gt;Here we are maximizing the log probability instead of minimizing the loss (Equation (7)).&lt;/p&gt;

&lt;p&gt;$R(\theta)$ is used to penalize large weights. The intuition behind this is that if the model matches the training data perfectly, but uses large weights, then it will be penalized more than the case where the model matches the training data a little less, but does so using small weights.&lt;/p&gt;

&lt;h3 id=&quot;l2-regularization&quot;&gt;L2 Regularization&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Tikhonov_regularization&quot;&gt;L2 Regularization&lt;/a&gt; is the euclidean distance of the vector squared from the origin.&lt;/p&gt;

&lt;p&gt;$ R(\theta) = || \theta ||_2^2$ Is the notation of L2 Norm&lt;/p&gt;

&lt;p&gt;$ R(\theta) = \sum_{j=1}^n \theta_j^2$ Is how we can compute L2 Norm&lt;/p&gt;

&lt;p&gt;L2 regularization is easy to optimize because of it’s simple derivative. It prefers weight vectors in smaller weights.&lt;/p&gt;

&lt;h3 id=&quot;l1-regularization&quot;&gt;L1 Regularization&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization-How-does-it-solve-the-problem-of-overfitting-Which-regularizer-to-use-and-when&quot;&gt;L1 Regularization&lt;/a&gt; is the linear function named after L1 norm or Manhattan Distance, which is the sum of the absolute values of the weights.&lt;/p&gt;

&lt;p&gt;$R(\theta) = || \theta ||_1$ is the notation of L1 norm&lt;/p&gt;

&lt;p&gt;L1 Regularization is hard to differentiate as the derivative of $| \theta |$ is non continuous at 0. It prefers sparse solutions with larger weights.&lt;/p&gt;

&lt;h1 id=&quot;multinomial-logistic-regression&quot;&gt;Multinomial Logistic Regression&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_logistic_regression&quot;&gt;[Multinomial Logistic Regression](&lt;/a&gt;) is also known as Softmax Regression or Maxent Classifier. In Multinomial Logistic Regression the target variable $y$ ranges over more than two classes.&lt;/p&gt;

&lt;p&gt;In order to support more than 2 classes, Multinomial Logistic Regression uses the &lt;a href=&quot;https://en.wikipedia.org/wiki/Softmax_function&quot;&gt;softmax function&lt;/a&gt; instead of the sigmoid function.&lt;/p&gt;

&lt;p&gt;For a vector $Z$ of dimensionality $k$ the softmax function is defined as:&lt;/p&gt;

\[Softmax(Z_i) = \frac{e^{z_i}}{\sum_{j=1}^k e^{z_j}}\]

&lt;p&gt;In the above equation the denominator helps normalize the values into probabilities.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Like Sigmoid, Softmax has the property of squashing values towards 0 or 1&lt;/li&gt;
  &lt;li&gt;If one of the inputs is larger than the others, it will tend to push its probability towards 1, and suppress the probabilities of the smaller inputs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;loss-function-in-multinomial-logistic-regression&quot;&gt;Loss Function in Multinomial Logistic Regression&lt;/h2&gt;
&lt;p&gt;To compute the loss function in Multinomial Regression, we need to account for $k$ classes that a given item can belong to (instead of just 2 classes)
\(\begin{eqnarray}
L_{CE}(\hat{y}, y) &amp;amp;=&amp;amp; - \sum_{k=1}^k 1 \{y=k\} log p(y=k|x) \nonumber \\
&amp;amp;=&amp;amp; - \sum_{k=1}^k 1 \{y=k\} log \frac{e^{w_k \cdot x + b}}{\sum_{j=1}^k e^{w_j \cdot x + b_j}}
\end{eqnarray}\)&lt;/p&gt;

&lt;p&gt;In equation (11) $ 1{ }$ evaluates to 1 if the condition in the brackets is true, and $0$ otherwise.&lt;/p&gt;

&lt;h1 id=&quot;working-example-of-logistic-regression&quot;&gt;Working Example of Logistic Regression&lt;/h1&gt;
&lt;p&gt;Consider a simple scenario where we are doing sentiment analysis of a dataset, and we have 2 classes: positive and negative (there is no neutral class in our hypothetical scenario). In this example we will be using the sigmoid function (because we have only 2 classes, and for simplicity we will not be using any regularization.&lt;/p&gt;

&lt;p&gt;The following are the matrices representing the actual outcome $y$ and the features corresponding to it $x$.&lt;/p&gt;

&lt;p&gt;$y = \begin{pmatrix}y\\1 \\ 0\end{pmatrix} \;\; x = \begin{pmatrix}x1 &amp;amp; x2 \\ 3 &amp;amp; 2\\ 1 &amp;amp; 3\end{pmatrix}$&lt;/p&gt;

&lt;p&gt;In this example we have only 2 features $x_1$ and $x_2$. $x1$ is the number of positive words found in the sentence and $x2$ is the number of negative words found in the sentence. Also, $y=0$ represents negative (or bad sentiment), and $y=1$ represents positive (or good sentiment).&lt;/p&gt;

&lt;p&gt;This stage is feature extraction and representation of our data.&lt;/p&gt;

&lt;p&gt;The next state is to initialize out initial weights. For this example, we are setting 
$w_1 \; = \; w_2 \;= \; b \;= \; 0$.&lt;/p&gt;

&lt;p&gt;($w_1$ is the weight of feature $x_1$ and $w_2$ is the weight of feature $x_2$), $\beta$ is our bias term.&lt;/p&gt;

&lt;p&gt;$\eta \;= \; 0.1$&lt;/p&gt;

&lt;p&gt;Each step in learning for logistic regression is represented by the following formula:&lt;/p&gt;

\[\theta^{t+1} = \theta^t - \eta \; \delta_\theta \; L(f(x^{(i)}, \theta), y^{(i)})\]

&lt;p&gt;Here is a breakdown of each of the components of the formula.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://rutumulkar.com/assets/images/learning.png&quot; alt=&quot;Learning in Logistic Regression&quot; width=&quot;500&quot; /&gt;
    &lt;br /&gt;
    &lt;em class=&quot;image-label&quot;&gt;Fig 2: Each step for Learning in Logistic Regression&lt;/em&gt;
&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent-step-1&quot;&gt;Gradient Descent Step 1&lt;/h3&gt;
&lt;p&gt;Considering the first row, where $y=1$ finding the gradient for this example:&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} \frac{\partial L_{CE}(w, b)}{\partial_{w_1}} \\ \frac{\partial L_{CE}(w, b)}{\partial_{w_1}} \\ \frac{\partial L_{CE}(w, b)}{\partial_{b}} \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (\sigma (w \cdot x + b) - y)) \; x_1 \\ (\sigma (w \cdot x + b) - y)) \; x_2 \\ \sigma (w \cdot x + b) - y \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;We know that initial $w, b = 0$. Substituting these values:&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (\sigma (0) - 1)) \; x_1 \\ (\sigma (0) - y)) \; x_2 \\ \sigma (0) - y \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;Given that $\sigma(0) = 0.5$ (See the sigmoid image above) and $x_1 = 3$ and $x_2 = 2$ for the first example in matrix $x$ (first row in matrix)&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (0.5 - 1) * 3 \\ (0.5 - 1) * 2 \\ 0.5 - 1 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} -1.5 \\ -1.0 \\ -0.5 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;Now that we have the gradient, we can compute $\theta_1$ by moving in the opposite direction as the gradient.&lt;/p&gt;

&lt;p&gt;$ \theta_{1} = \begin{pmatrix} w_1 \\ w_2 \\ b \end{pmatrix} - \eta \begin{pmatrix} -1.5 \\ -1.0 \\ -0.5 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;Given that $w_1 = w_2 = b = 0$ and $\eta = 0.1$:&lt;/p&gt;

&lt;p&gt;$ \theta_{1} = \begin{pmatrix} 0.15 \\ 0.1 \\ 0.05 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;The weights after one step of gradient descent: $w_1 = 0.15$, $w_2 = 0.1$, $b = 0.05$.&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent-step-2&quot;&gt;Gradient Descent Step 2&lt;/h3&gt;
&lt;p&gt;This time we will consider row 2 of our matrix. The weights learned so far are : $w_1 = 0.15$, $w_2 = 0.1$, $b = 0.05$
The values from our matrix are $x_1 = 1$, $x_2=3$, $y=0$.&lt;/p&gt;

&lt;p&gt;From Equation (9) computing the gradient:&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (\sigma (w \cdot x + b) - 0)) \; x_1 \\ (\sigma (w \cdot x + b) - 0)) \; x_2 \\ \sigma (w \cdot x + b) - y \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (\sigma (0.15 * 1 + 0.1 * 3 + 0.05) - 0) \; 1 \\ (\sigma (0.15 * 1 + 0.1 * 3 + 0.05) - 0) \; 3 \\ \sigma (0.15 * 1 + 0.1 * 3 + 0.05) - 0 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (\sigma (0.15 + 0.3 + 0.05) - 0) \; 1 \\ (\sigma (0.15 + 0.3 + 0.05) - 0) \; 3 \\ \sigma (0.15 + 0.3 + 0.05) - 0 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;Sigmoid of 0.5 is 0.622. (&lt;a href=&quot;https://keisan.casio.com/exec/system/15157249643325&quot;&gt;Compute your own sigmoid&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} (0.622 - 0) \; 1 \\ (0.622 - 0) \; 3 \\ 0.622 - 0 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \delta_{wb} = \begin{pmatrix} 0.378 \\ 1.134 \\ 0.378 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;Now that we have the gradient, we can compute $\theta_2 $ by moving in the opposite direction as the gradient.&lt;/p&gt;

&lt;p&gt;$ \theta_{2} = \begin{pmatrix} w_1 \\ w_2  \\ b \end{pmatrix} - \eta \begin{pmatrix} 0.378 \\ 1.134 \\ 0.378 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \theta_{2} = \begin{pmatrix} 0.15 \\ 0.1  \\ 0.05 \end{pmatrix} - 0.1 \begin{pmatrix} 0.378 \\ 1.134 \\ 0.378 \end{pmatrix}$&lt;/p&gt;

&lt;p&gt;$ \theta_{2} = \begin{pmatrix} 0.15 - 0.0378 \\ 0.1 - 0.1134 \\ 0.05 - 0.0378 \end{pmatrix} $&lt;/p&gt;

&lt;p&gt;$ \theta_{2} = \begin{pmatrix} 0.1122 \\ -0.0134 \\ 0.4622 \end{pmatrix} $&lt;/p&gt;

&lt;p&gt;At the end of step 2, $w_1 = 0.1122$, $w_2 = -0.0134$ and $b = 0.04622$.&lt;/p&gt;

&lt;p&gt;We can continue this process for $k$ number of steps and iterate through the examples again and again till we find the global minimum.&lt;/p&gt;

&lt;h1 id=&quot;summary-of-logistic-regression&quot;&gt;Summary of Logistic Regression&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Each input is composed of a vector $x_1, x_2 \ldots x_n$&lt;/li&gt;
  &lt;li&gt;We compute $\hat{y} = \sigma(w \cdot x + b)&lt;/li&gt;
  &lt;li&gt;Compute loss = $ \hat{y} - y$. We use cross entropy loss to compute this value&lt;/li&gt;
  &lt;li&gt;Compute the gradient of the loss = $\frac{d}{dc}L_{CE}$&lt;/li&gt;
  &lt;li&gt;Sigmoid is replaced by Softmax when we do multinomial Logistic Regression&lt;/li&gt;
  &lt;li&gt;Regularization is used to avoid overfitting and make the model more generalized&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/2001/file/7b7a53e239400a13bd6be6c91c4f6c4e-Paper.pdf&quot;&gt;On discriminative vs. generative classifiers: a comparison of logistic regression and naive Bayes&lt;/a&gt; NIPS’01: Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic, January 2001 Pages 841–848&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/5.pdf&quot;&gt;Chapter 5, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition&lt;/a&gt; by Daniel Jurafsky, James H. Martin&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/&quot;&gt;Introduction to Latent Dirichlet Allocation&lt;/a&gt;, by Edwin Chen&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 23 Apr 2021 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/nlp/classification/2021/04/23/logistic-regression.html</link>
        <guid isPermaLink="true">http://localhost:4000/nlp/classification/2021/04/23/logistic-regression.html</guid>
        
        <category>nlp</category>
        
        <category>classification</category>
        
        
        <category>nlp</category>
        
        <category>classification</category>
        
      </item>
    
      <item>
        <title>What is Byte-Pair Encoding for Tokenization?</title>
        <description>&lt;p&gt;Tokenization is the concept of dividing text into tokens - words (unigrams), or groups of words (n-grams) or even characters. 
Morphology traditionally defines morphemes as the smallest semantic unit. e.g. The word &lt;strong&gt;Unfortunately&lt;/strong&gt; can be broken down as &lt;strong&gt;un - fortun - ate - ly&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;[[un [[fortun(e) ]\(_{ROOT}\) ate]\(_{STEM}\)]\(_{STEM}\) ly]\(_{WORD}\)&lt;/p&gt;

&lt;p&gt;Morphology is little studied with deep learning, but Byte Pair Encoding is a way to infer morphology from text. Byte-pair encoding allows us to define tokens automatically from data, instead of precpecifying character or word boundaries. This is especially useful in dealing with unkown words.&lt;/p&gt;

&lt;h2 id=&quot;modern-tokenizers&quot;&gt;Modern Tokenizers&lt;/h2&gt;

&lt;p&gt;Modern tokenizers often automatically induce tokens that include tokens smaller than words - called &lt;strong&gt;Subwords&lt;/strong&gt;. E.g. the subwords “-ly”, “-ing” give us an ideal about the type of the word - which is what subword tokenization aims to do.&lt;/p&gt;

&lt;p&gt;Most tokenizers have two parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;A token learner:&lt;/strong&gt; takes a raw training corpus and indices a vocabulary - a set of tokens.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;A token segmenter:&lt;/strong&gt; takes a raw test sentence and segments it into the tokens in the vocabulary.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Three algorithms are widely used :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Byte Pair Encoding (Sennrick et. al 2016)&lt;/li&gt;
  &lt;li&gt;Unigram Language Modeling (Kudo 2018)&lt;/li&gt;
  &lt;li&gt;Wordpiece (Schuster and Nakajima 2012) and Sentencepiece (Kudo and Richardson, 2018)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Byte Pair Encoding (BPE) is the simplest of the three.&lt;/p&gt;

&lt;h2 id=&quot;byte-pair-encoding-bpe-algorithm&quot;&gt;Byte Pair Encoding (BPE) Algorithm&lt;/h2&gt;

&lt;p&gt;BPE runs within word boundaries. &lt;strong&gt;&lt;em&gt;BPE Token Learning&lt;/em&gt;&lt;/strong&gt; begins with a vocabulary that is just the set of individual characters (tokens). It then runs over a training corpus ‘k’ times and each time, it merges 2 tokens that occur the most frequently in text. e.g. ‘e’ and ‘r’ are merged into a single token ‘er’ when they occur together in the same order.&lt;/p&gt;

&lt;p&gt;At the end of ‘k’ iterations, the algorithm produces a list of most frequent ‘k’ tokens along with the original set of characters.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;byte_pair_encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
	 &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;characters&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_list&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
	 &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	 	&lt;span class=&quot;n&quot;&gt;c_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_right&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;most&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frequent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pair&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;adjacent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokens&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;string_list&lt;/span&gt;
	 	&lt;span class=&quot;n&quot;&gt;c_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_left&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_right&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# create a new bigram
&lt;/span&gt;	 	&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_new&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# add the bigram to teh vocabulary
&lt;/span&gt;	 	&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;occurence&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_right&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_new&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# update the corpus
&lt;/span&gt;	 &lt;span class=&quot;nf&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once the &lt;strong&gt;token learner&lt;/strong&gt; learns the vocabulary, the &lt;strong&gt;token parser&lt;/strong&gt; is used to tokenize a test sentence from teh learned tokens that were leraned from teh training data.&lt;/p&gt;

&lt;p&gt;In real applications of BPE algorithms BPE is run with many thousands of merges such that most words are represented as tokens and only the rare words are represented by their parts.&lt;/p&gt;

&lt;p&gt;Byte-Pair Encoding was originally a compression algorithm where we replace the most frequent byte pair with a new byte - thereby compressing the data.&lt;/p&gt;

&lt;p&gt;For further reading check out this &lt;a href=&quot;https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture12-subwords.pdf&quot;&gt;NLP class slides from Stanford&lt;/a&gt; or this &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/2.pdf&quot;&gt;chapter on Text Normalization from the Jurafsky and Martin Textbook&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;T. Kudo. Subword Regularization: &lt;a href=&quot;https://arxiv.org/pdf/1804.10959.pdf&quot;&gt;Improving Neural Network Translation Models with Multiple Subword Candidates&lt;/a&gt;. 2018&lt;/li&gt;
  &lt;li&gt;T. Kudo and J. Richardson. &lt;a href=&quot;https://arxiv.org/pdf/1808.06226.pdf&quot;&gt;SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing&lt;/a&gt;. 2018&lt;/li&gt;
  &lt;li&gt;M. Schuster and K. Nakajima. &lt;a href=&quot;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37842.pdf&quot;&gt;Japanese and Korea Voice Search&lt;/a&gt;. 2012&lt;/li&gt;
  &lt;li&gt;R. Sennrich, B. Haddow and A. Birch. &lt;a href=&quot;http://aclweb.org/anthology/P16-1162&quot;&gt;Neural Machine Translation of Rare Words with Subword Units&lt;/a&gt;. ACL 2016&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 28 Jan 2021 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/tokenization/2021/01/28/byte-pair-encoding.html</link>
        <guid isPermaLink="true">http://localhost:4000/tokenization/2021/01/28/byte-pair-encoding.html</guid>
        
        <category>tokenization</category>
        
        
        <category>tokenization</category>
        
      </item>
    
      <item>
        <title>Managing Machine Learning Experiments</title>
        <description>&lt;p&gt;I run Machine Learning experiments for a living and I run an average of 50 experiments per stage of a project. For each experiment I write code for training models, identifying the right test cases and metrics, finding the right preprocessors - the list goes on.&lt;/p&gt;

&lt;p&gt;So how to manage these experiments? Here are a few criteria that I have:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Compatible with Git:&lt;/strong&gt; I manage all my code with Git and I want to make sure that experiment manager can keep track of how my code changes with time.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Version control for data:&lt;/strong&gt; I want to be able to work with multiple versions of my test and training datasets. I need to know if any 2 datasets are duplicates of each other, so that I am running my tests on the same datasets.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model Management:&lt;/strong&gt; When I run experiments and store models, I want to store models that are associated with an experiment, rather than a particular run. I need to have meta-data associated with the model that tells me information about how this model was created, data it was trained on etc. (This is also the experiment meta-data)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Metrics:&lt;/strong&gt; I want to be able to store the output metrics for each experiment, and create new metrics by running the same model over different test datasets.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(Optional) Running experiments:&lt;/strong&gt; I want to be able to run experiments with a single command - this can be bath experiments, or a single experiment. I don’t want to have to worry about containers and dockers and the logistics of it.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(Optional) Experiment optimization:&lt;/strong&gt; I have so many variations and tests to try out. If a system to automatically try out these different variations for me that go beyond optimizing hyper-parameters of an algorithm, I would love to try such a system out.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I went on a quest to find a solution to my problems. And while I was on my quest, I discovered some more criteria that I had previously not considered while evaluating tools.&lt;/p&gt;

&lt;p&gt;Here are some products that I have been looking at:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;th&gt;Product&lt;/th&gt;
    &lt;th&gt;Pricing&lt;/th&gt;
    &lt;!-- &lt;th&gt;Open Source&lt;/th&gt; --&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://www.comet.ml/&quot;&gt;Comet&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Paid&lt;/td&gt;
    &lt;!-- &lt;td&gt;No&lt;/td&gt; --&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://neptune.ml/&quot;&gt;neptune.ml&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Paid&lt;/td&gt;
    &lt;!-- &lt;td&gt;No&lt;/td&gt; --&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://tensordash.ai/&quot;&gt;Tensordash&lt;/a&gt; &lt;/td&gt;
    &lt;td&gt;Paid&lt;/td&gt;
    &lt;!-- &lt;td&gt;&lt;/td&gt; --&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://www.wandb.com/&quot;&gt;Weights and Biases&lt;/a&gt; &lt;/td&gt;
    &lt;td&gt;Free for individuals and academics, Paid&lt;/td&gt;
    &lt;!-- &lt;td&gt;No&lt;/td&gt; --&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://valohai.com/&quot;&gt;Valohai&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Paid&lt;/td&gt;
    &lt;!-- &lt;td&gt;&lt;/td&gt; --&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://www.floydhub.com/product/train&quot;&gt;FloydHub&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Paid&lt;/td&gt;
    &lt;!-- &lt;td&gt;No&lt;/td&gt; --&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://verta.ai/&quot;&gt;Verta.ai&lt;/a&gt;&lt;/td&gt;
    &lt;td colspan=&quot;2&quot;&gt;Not Launched Yet&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;http://www.sirio-ml.com/&quot;&gt;SirioML&lt;/a&gt;&lt;/td&gt;
    &lt;td colspan=&quot;2&quot;&gt;Not Launched Yet&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;
Below is an impressive list of opensource tools in this space for running, managing and analyzing experiments.&lt;/p&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://mlflow.org/&quot;&gt;MLFlow&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;dvc.org&quot;&gt;DVC&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;iterative.ai&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://guild.ai/&quot;&gt;Guild.ml&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;http://mlmodelscope.org/&quot;&gt;MLModelScope&lt;/a&gt; &lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;  
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/beringresearch/lab&quot;&gt;Machine Learning Lab&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt; 
  
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://modelchimp.com/&quot;&gt;ModelChimp&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/allegroai/trains&quot;&gt;Trains&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://mitdbg.github.io/modeldb/&quot;&gt;ModelDB&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/vivekratnavel/omniboard&quot;&gt;Omniboard&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/datmo/datmo&quot;&gt;Datmo&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;http://seba1511.net/randopt/&quot;&gt;Randopt&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/studioml/studio&quot;&gt;StudioML&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/kubeflow/kubeflow&quot;&gt;KubeFlow&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/instacart/lore&quot;&gt;Lore&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
    &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/machinalis/featureforge&quot;&gt;Featureforge&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/pachyderm/pachyderm&quot;&gt;pachyderm&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/polyaxon/polyaxon&quot;&gt;PolyAxon&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;Runway&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;paper: http://www.jsntsay.com/publications/tsay-sysml2018.pdf&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://github.com/IDSIA/sacred&quot;&gt;Sacred&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;http://neuralensemble.org/sumatra/&quot;&gt;Sumatra&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;Free&lt;/td&gt;
    &lt;td&gt;Yes&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

</description>
        <pubDate>Wed, 24 Jul 2019 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/machine%20learning/experiment%20management/2019/07/24/manageml.html</link>
        <guid isPermaLink="true">http://localhost:4000/machine%20learning/experiment%20management/2019/07/24/manageml.html</guid>
        
        <category>machine_learning</category>
        
        
        <category>machine learning</category>
        
        <category>experiment management</category>
        
      </item>
    
      <item>
        <title>What is Natural Language Processing (NLP)?</title>
        <description>&lt;p&gt;Last year &lt;a href=&quot;http://rutumulkar.com/blog/2016/NLP-ML&quot;&gt;I wrote a highly popular blog post about Natural Language Processing, Machine Learning, and Deep Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post, we will break down NLP further and talk about Rule-Based and Statistical NLP. I will discuss why everyone needs to know about NLP and AI (Artificial Intelligence), how Machine Learning (ML) fits into the NLP space (it is indispensable actually) and how we are using it in our daily life even without knowing about it.&lt;/p&gt;

&lt;h4 id=&quot;introduction-to-nlp&quot;&gt;Introduction to NLP&lt;/h4&gt;

&lt;p&gt;Natural Language Processing or NLP is a phrase that is formed from 3 components - &lt;em&gt;natural&lt;/em&gt; - as exists in nature, &lt;em&gt;language&lt;/em&gt; - that we use to communicate with each other, &lt;em&gt;processing&lt;/em&gt; - something that is done automatically. Putting these words together, we get &lt;em&gt;Natural Language Processing&lt;/em&gt; or &lt;em&gt;NLP&lt;/em&gt; - which stands for the approaches to “process” natural language or human language.&lt;/p&gt;

&lt;p&gt;This is a very generic term. What does this “processing” even mean?&lt;/p&gt;

&lt;p&gt;As a human, I understand English when someone talks to me, is that NLP? Yes! When done automatically, it is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Natural_language_understanding&quot;&gt;Natural Language Understanding (NLU)&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;http://www.stuartduncan.name/wp-content/uploads/2012/05/understanding-1024x834.jpg&quot; alt=&quot;Human Understanding. Image Credit: http://www.stuartduncan.name&quot; class=&quot;img-responsive center-image&quot; style=&quot;width: 400px;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Human Understanding. Image Credit: http://www.stuartduncan.name&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I translated some Hindi to English for my friend, is that NLP? Yes, it is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;&gt;Machine Translation (MT)&lt;/a&gt; when done automatically.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://cloud.google.com/images/products/artwork/hello-lead.png&quot; class=&quot;img-responsive center-image&quot; width=&quot;200px&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Machine Translation. Image Credit: Google Cloud&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As humans, we perform Natural Language Processing pretty well but we are not perfect; misunderstandings are pretty common among humans, and we often interpret the same language differently. So, language processing isn’t &lt;a href=&quot;https://en.wikipedia.org/wiki/Deterministic_system&quot;&gt;deterministic&lt;/a&gt; (which means that the same language doesn’t have the same interpretation, unlike math where 1 + 1 is deterministic and always equals 2) and something that might be funny to me, might not be funny to you.&lt;/p&gt;

&lt;p&gt;This inherent &lt;a href=&quot;https://en.wikipedia.org/wiki/Nondeterministic_algorithm&quot;&gt;non-deterministic&lt;/a&gt; nature of the field of Natural Language Processing makes it an interesting and an &lt;a href=&quot;https://en.wikipedia.org/wiki/NP-hardness&quot;&gt;NP-hard problem&lt;/a&gt;. In this sense, understanding NLP is like creating a new form of intelligence in an artificial manner that can understand how humans understand language; which is why NLP is a subfield of &lt;a href=&quot;https://en.wikipedia.org/wiki/Artificial_intelligence&quot;&gt;Artificial Intelligence&lt;/a&gt;. NLP experts say that if humans don’t agree 100% on NLP tasks (like language understanding, or language translation), it isn’t possible to model a machine to perform these tasks without some degree of error. Side note - if an NLP consultant ever tells you that they can create a model that is more precise than a human, be very wary of them. More about that in my post about &lt;a href=&quot;&quot;&gt;7 questions to ask before you hire your Data Science consultant&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;rule-based-nlp-vs-statistical-nlp&quot;&gt;Rule-Based NLP vs Statistical NLP&lt;/h4&gt;
&lt;p&gt;NLP separated into two different sets of ideologies and approaches.&lt;/p&gt;

&lt;p&gt;One set of scientists believe that it is impossible to completely do NLP without some inherent background knowledge that we take for granted in our daily lives such as &lt;em&gt;freezing temperatures cause hypothermia&lt;/em&gt;, &lt;em&gt;hot coffee will burn my skin&lt;/em&gt; and so on. This set of knowledge is collectively known as &lt;em&gt;commonsense knowledge&lt;/em&gt; and has brought about the field of &lt;em&gt;Commonsense Reasoning&lt;/em&gt; and very many &lt;a href=&quot;http://commonsensereasoning.org/&quot;&gt;conferences&lt;/a&gt; and companies (a special mention to &lt;a href=&quot;https://en.wikipedia.org/wiki/Cyc&quot;&gt;Cyc&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Encoding commonsense knowledge is a very time intensive and manual effort driven process as is considered to be in the space of &lt;a href=&quot;https://en.wikipedia.org/wiki/Rule-based_system&quot;&gt;Rules-Based NLP&lt;/a&gt;. It is hard because commonsense knowledge isn’t found in the written text (discourse), and we don’t know how many rules we need to create, before the work for encoding knowledge is complete. Here is an example: as humans, we inherently understand the concepts of &lt;em&gt;death&lt;/em&gt; and you will rarely find documents that describe it by explaining the existence and nonexistence of hydrocarbons. Similarly are the concepts of &lt;em&gt;moving&lt;/em&gt; and &lt;em&gt;dancing&lt;/em&gt; which usually do not require any explanation to a human, but a computer model requires the breakdown of &lt;em&gt;moving&lt;/em&gt; into the &lt;em&gt;origin&lt;/em&gt;, &lt;em&gt;destination&lt;/em&gt;, and the concept of not being at the origin after the move has happened. Dancing, on the other hand, is also a type of moving, but it is obviously very different from a traditional move, and requires more explanation because you can move a lot and still end up in your original location, so what is the point of a &lt;em&gt;dance move&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;Another set of scientists have taken a different (&lt;a href=&quot;https://www.quora.com/Why-are-rule-based-methods-becoming-unpopular-in-NLP-Are-rule-based-methods-still-in-use-If-yes-where-should-I-look-for-them&quot;&gt;now deceptively mainstream&lt;/a&gt;) approach to NLP. Instead of creating commonsense data that is missing in textual discourse, their idea is to leverage large amounts of already existing data for NLP tasks. This approach is statistical and inductive in nature and the idea is that if we can find enough number of examples of a given problem, we could potentially solve it using the power of &lt;a href=&quot;https://en.wikipedia.org/wiki/Mathematical_induction&quot;&gt;induction&lt;/a&gt;. Statistical NLP makes heavy use of &lt;em&gt;Machine Learning&lt;/em&gt; for developing models and deriving insights from a labeled text.&lt;/p&gt;

&lt;p&gt;Rule-Based NLP and Statistical NLP use different approaches for solving the same problems. Here are a couple of examples:&lt;/p&gt;

&lt;h5 id=&quot;parsing&quot;&gt;Parsing:&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;Rules-Based uses Linguistic rules and patterns. E.g English has the structure of SVO (Subject Verb Object), Hindi has SOV (Subject Object Verb).&lt;/li&gt;
  &lt;li&gt;Statistical NLP induces linguistic rules from the text (so our models are only as good as our text), along with lots of labeling of the text to predict the most likely parse tree of a new data source.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;http://www.cs.cornell.edu/courses/cs2112/2014fa/lectures/lec_parsing/simple-sentence.png&quot; class=&quot;img-responsive center-image&quot; width=&quot;300px&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Natural Language Parsing. Image Credit: cs.cornell.edu&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;synonym-extraction&quot;&gt;Synonym extraction&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;Rules-Based approaches use thesaurus and lists. Data sources such as &lt;a href=&quot;https://wordnet.princeton.edu/&quot;&gt;Wordnet&lt;/a&gt; are very useful for deterministic rule-based approaches&lt;/li&gt;
  &lt;li&gt;Statistical NLP approaches use statistics to induce thesaurus based on &lt;a href=&quot;https://code.google.com/archive/p/word2vec/&quot;&gt;how similar words have similar contexts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;https://camo.githubusercontent.com/bc16ea3ea66a7f696fa17d656288d07c8dcd1254/687474703a2f2f692e696d6775722e636f6d2f4443466367784e2e706e67&quot; class=&quot;img-responsive center-image&quot; width=&quot;800px&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Synonyms using Word2Vec. Image Credit: oscii-lab&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;sentiment-analysis&quot;&gt;Sentiment Analysis&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;Rules-based approaches look for linguistic terms such as “love”, and “hate”, “like” and “dislike” etc. and deterministically classify text as positive and negative&lt;/li&gt;
  &lt;li&gt;Statistical NLP approaches use Machine Learning and do some feature engineering to provide weights to linguistic terms to determine the positive and negative nature of texts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Which approach is better? Both the approaches have their advantages. Rules-based approaches mimic the human mind and present highly precise results, however, they are limited by what we provide as rules. Statistical approaches are less precise, but they have a much higher coverage than rules-based systems as they are able to account for cases that are not explicitly specified in the rules.&lt;/p&gt;

&lt;p&gt;Most institutions prefer to use a hybrid approach to NLP, using Rule-Based along with Statistical Systems.&lt;/p&gt;

&lt;h4 id=&quot;solving-nlp-problems&quot;&gt;Solving NLP Problems&lt;/h4&gt;

&lt;p&gt;We use NLP every day when we do a google search. Search engines use &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/irbook.html&quot;&gt;Information Retrieval&lt;/a&gt; in their backend, which is one of the subfields of NLP.&lt;/p&gt;

&lt;p&gt;NLP had earned its popularity because of several mainstream types of language-based problems - such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_summarization&quot;&gt;text summarization&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Sentiment_analysis&quot;&gt;sentiment analysis&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Keyword_extraction&quot;&gt;keyword extraction&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Question_answering&quot;&gt;question answering&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Chatbot&quot;&gt;conversational interfaces and chatbots&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_translation&quot;&gt;machine translation&lt;/a&gt;, to name a few.&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Today NLP is largely statistical because of the availability of massive amounts of data. We can use tools like &lt;a href=&quot;https://code.google.com/archive/p/word2vec/&quot;&gt;Word2Vec&lt;/a&gt; to get us similarly occurring concepts, and search engines like &lt;a href=&quot;https://www.elastic.co/&quot;&gt;Elastic Search&lt;/a&gt; to organize our text to make it searchable. We are able to use off the shelf tools like &lt;a href=&quot;https://stanfordnlp.github.io/CoreNLP/&quot;&gt;Stanford Core NLP&lt;/a&gt; to parse our data for us and other algorithms like &lt;a href=&quot;http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/&quot;&gt;Latent Dirichlet Allocation (LDA)&lt;/a&gt; to discover clusters and topics in the text.&lt;/p&gt;

&lt;p&gt;As a consumer, we use NLP every day - from your first google search of the day, to your curated daily news articles delivered to you, your online shopping experience and reading reviews, and your conversational assistants such as &lt;a href=&quot;http://ok-google.io/&quot;&gt;OK Google&lt;/a&gt;, &lt;a href=&quot;https://developer.amazon.com/alexa&quot;&gt;Alexa&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Siri&quot;&gt;Siri&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;NLP is embedded in our everyday lives, and we use it even without realizing it. The latest wave of &lt;a href=&quot;https://en.wikipedia.org/wiki/Chatbot&quot;&gt;conversational interfaces or chatbots&lt;/a&gt; are adding a human component to conversation, and we are finally blending the 2 approaches to NLP - Rule-Based and Statistical NLP.&lt;/p&gt;

&lt;p&gt;Where do we go from here? I am excited for the future of NLP, and although we are &lt;a href=&quot;https://hbr.org/2016/11/what-artificial-intelligence-can-and-cant-do-right-now&quot;&gt;very far from NLP/AI taking over the world&lt;/a&gt; I am optimistic about computational power being more ingrained in our lives to make the world a better and easier place for us.&lt;/p&gt;

&lt;p&gt;To learn more about NLP and for additional NLP resources &lt;a href=&quot;https://blog.algorithmia.com/introduction-natural-language-processing-nlp/&quot;&gt;check out this cool blog post from Algorithmia&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Dec 2017 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/nlp%20introduction/2017/12/12/what-is-nlp.html</link>
        <guid isPermaLink="true">http://localhost:4000/nlp%20introduction/2017/12/12/what-is-nlp.html</guid>
        
        <category>nlp</category>
        
        <category>introduction</category>
        
        
        <category>nlp introduction</category>
        
      </item>
    
      <item>
        <title>Natural Language Processing vs. Machine Learning vs. Deep Learning</title>
        <description>&lt;p&gt;NLP, Machine Learning and Deep Learning are all parts of Artificial Intelligence, which is a part of the greater field of Computer Science. The following image visually illustrates CS, AI and some of the components of AI -&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.bostondynamics.com/&quot;&gt;Robotics&lt;/a&gt; (AI for motion)&lt;/li&gt;
  &lt;li&gt;Vision (AI for visual space - videos, images)&lt;/li&gt;
  &lt;li&gt;NLP (AI for text)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://rutumulkar.com/assets/images/cs_ai.png&quot; alt=&quot;Overview of Computer Science, Artificial Intelligence and its sub-fields&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are other aspects of AI too which are not highlighted in the image - such as speech, which is beyond the scope of this post. Here is what I discuss in this post:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-natural-language-processing&quot;&gt;What is Natural Language Processing?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#machine-learning&quot;&gt;Machine Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#deep-learning&quot;&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#relationship-between-nlp-ml-and-deep-learning&quot;&gt;Relationship between NLP, ML and Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-is-natural-language-processing&quot;&gt;What is Natural Language Processing?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://rutumulkar.com/blog/2017/what-is-nlp/&quot;&gt;Natural Language Processing&lt;/a&gt; (or NLP) is an area that is a confluence of Artificial Intelligence and linguistics. It involves intelligent analysis of &lt;strong&gt;written language&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If you have a lot of data written in plain text and you want to automatically get some insights from it, you need to use NLP.&lt;/p&gt;

&lt;p&gt;Some applications of NLP are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sentiment Analysis : Classification of emotion behind text content. e.g. movie reviews are good or bad. How can humans tell if a review is good or bad? Can use use the same features that humans use - presence of describing words (adjectives) such as “great” or “terrible” etc.?&lt;/li&gt;
  &lt;li&gt;Information extraction : Extracting structured data from text. e.g. relationships between country and name of president, acquisition relationship between buyer and seller etc.&lt;/li&gt;
  &lt;li&gt;Information retrieval : This is a synonym of &lt;strong&gt;search&lt;/strong&gt;. It is the concept of retrieving the correct document given a query - like Google! For the curious, here is info on &lt;a href=&quot;https://rutumulkar.com/blog/2014/build-your-own-search-engine/&quot;&gt;how to build your own search engine&lt;/a&gt; and some more details on &lt;a href=&quot;https://rutumulkar.com/blog/2014/core-of-lucene/&quot;&gt; the internals of Lucene&lt;/a&gt; (&lt;a href=&quot;https://lucene.apache.org/&quot;&gt;Apache Lucene&lt;/a&gt; is an open source search engine that is used in Elastic Search)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is a more detailed post about NLP - &lt;a href=&quot;https://rutumulkar.com/blog/2017/what-is-nlp/&quot;&gt;What is Natural Language Processing?&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;machine-learning&quot;&gt;Machine Learning&lt;/h3&gt;

&lt;p&gt;Machine Learning (or ML) is an area of Artificial Intelligence (AI) that is a set of statistical techniques for problem solving.&lt;/p&gt;

&lt;p&gt;Machine Learning by itself is a set of algorithms that is used to do better NLP, better vision, better robotics etc. It is not an AI field in itself, but a way to solve real AI problems.&lt;/p&gt;

&lt;p&gt;Today ML is used for self driving cars (vision research from graphic above), fraud detection, price prediction, and even NLP.&lt;/p&gt;

&lt;p&gt;In order to apply ML techniques to NLP problems, we need to usually convert the unstructured text into a structured format, i.e. tabular format.&lt;/p&gt;

&lt;h3 id=&quot;deep-learning&quot;&gt;Deep Learning&lt;/h3&gt;

&lt;p&gt;Deep Learning (which includes Recurrent Neural Networks, Convolution neural Networks and others) is a type of Machine Learning approach.&lt;/p&gt;

&lt;p&gt;Deep Learning is an extension of Neural Networks - which is the closest imitation of how the human brains work using neurons. Mathematically it involves running data through a large networks of neurons - each of which has an activation function - the neuron is activated if that threshold is reached - and that value is propagated through the network.&lt;/p&gt;

&lt;p&gt;Deep Learning is used quite extensively for vision based classification (e.g. distinguishing images of airplanes from images of dogs).&lt;/p&gt;

&lt;p&gt;Deep Learning can be used for NLP tasks as well. However it is important to note that Deep Learning is a broad term used for a series of algorithms and it is just another tool to solve core AI problems that are highlighted above.&lt;/p&gt;

&lt;h3 id=&quot;relationship-between-nlp-ml-and-deep-learning&quot;&gt;Relationship between NLP, ML and Deep Learning&lt;/h3&gt;

&lt;p&gt;The image below shows graphically how NLP is related ML and Deep Learning. Deep Learning is one of the techniques in the area of Machine Learning - there are several other techniques such as Regression, K-Means, and so on.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://rutumulkar.com/assets/images/nlp-ml.png&quot; alt=&quot;Relationship between NLP, ML and Deep Learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ML and NLP have some overlap, as Machine Learning as a tool is often used for NLP tasks. There are several other things that you need for NLP - NER (named entity recognizer), POS Tagged (Parts of peech tagger identifies Nouns, verbs and other part of speech tags in text).&lt;/p&gt;

&lt;p&gt;NLP has a strong linguistics component (not represented in the image), that requires an understanding of how we use language. The art of understanding language involves understanding humor, sarcasm, subconscious bias in text, etc.&lt;/p&gt;

&lt;p&gt;Once we can understand that is means to to be sarcastic (yeah right!) we can encode it into a machine learning algorithm to automatically discover similar patterns for us statistically.&lt;/p&gt;

&lt;p&gt;To summarize, in order to do any NLP, you need to understand language. Language is different for different genres (research papers, blogs, twitter have different writing styles), so there is a strong need of looking at your data manually to get a feel of what it is trying to say to you, and how you - as a human would analyze it.&lt;/p&gt;

&lt;p&gt;Once you figure out what you are doing as a human reasoning system (ignoring hash tags, using smiley faces to imply sentiment), you can use a relevant ML approach to automate that process and scale it.&lt;/p&gt;
</description>
        <pubDate>Wed, 08 Jun 2016 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/nlp%20introduction/machine%20learning/deep%20learning/2016/06/08/NLP-ML.html</link>
        <guid isPermaLink="true">http://localhost:4000/nlp%20introduction/machine%20learning/deep%20learning/2016/06/08/NLP-ML.html</guid>
        
        <category>nlp</category>
        
        <category>machine_learning</category>
        
        <category>deep_learning</category>
        
        
        <category>nlp introduction</category>
        
        <category>machine learning</category>
        
        <category>deep learning</category>
        
      </item>
    
      <item>
        <title>Online Word2Vec for Gensim</title>
        <description>&lt;p&gt;Word2Vec [1] is a technique for creating vectors of word representations to capture the syntax and semantics of words. The vectors used to represent the words have several interesting features, here are a few:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Addition and subtraction of vectors show how word semantics are captured:
 e.g. \(king - man + woman = queen\)
This example captures the fact that the semantics of $king$ and $queen$ are nicely captured by the word vectors&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Similar words have similar word vectors: E.g. $king$ is most similar to - $queen$, $duke$, $duchess$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the description of &lt;a href=&quot;http://radimrehurek.com/gensim/&quot;&gt;Gensim&lt;/a&gt; Word2Vec, and a few blogs that describe how to use it: &lt;a href=&quot;https://radimrehurek.com/gensim/models/word2vec.html&quot;&gt;Deep Learning with Word2Vec&lt;/a&gt;, &lt;a href=&quot;http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/&quot;&gt;Deep learning with word2vec and gensim&lt;/a&gt;, &lt;a href=&quot;http://rare-technologies.com/word2vec-tutorial/&quot;&gt;Word2Vec Tutorial&lt;/a&gt;, &lt;a href=&quot;http://rare-technologies.com/word2vec-in-python-part-two-optimizing/&quot;&gt;Word2vec in Python, Part Two: Optimizing&lt;/a&gt;, &lt;a href=&quot;https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors&quot;&gt;Bag of Words Meets Bags of Popcorn&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the issues of the Word2Vec algorithm is that it is not able to add more words to vocabulary after an initial training. This approach to ‘freeze vocabulary’ might not work for several situations where we need to train the model in an &lt;em&gt;online manner&lt;/em&gt;, by adding and training on new words as they are encountered. Here is a quick description of an &lt;a href=&quot;https://en.wikipedia.org/wiki/Online_algorithm&quot;&gt;online algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this post, I will discuss an &lt;a href=&quot;https://github.com/rutum/gensim&quot;&gt;online word2vec&lt;/a&gt; implementation that I have developed and how to use it to update the vocabulary and learn new word vectors in an online manner. I maintain the code here: &lt;a href=&quot;https://github.com/rutum/gensim&quot;&gt;https://github.com/rutum/gensim&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;How to use online word2vec:&lt;/p&gt;

&lt;p&gt;1) Download the source code from here: &lt;a href=&quot;https://github.com/rutum/gensim&quot;&gt;https://github.com/rutum/gensim&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2) On your local machine, browse to the location of the downloaded code, and install it by typing:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;#clean already existing install
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rf&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;build&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gensim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pyc&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#installation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sudo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;python&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;install&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;3) Now run the following lines of code from ipython or a seperate python file:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gensim.models&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# setup logging
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;basicConfig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;%(asctime)s : %(levelname)s : %(message)s&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logging&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# train the basic model with text8-rest, which is all the sentences
# without the word - queen
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gensim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Word2Vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gensim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word2vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;LineSentence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;text8-rest&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;build_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Evaluation
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;n_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;king&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;duke&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.68208604377750204&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;n_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;king&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;queen&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;KeyError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;queen&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# text8-rest:
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;questions-words.txt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;49&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;781&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precomputing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norms&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;346&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capital&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;common&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;countries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;33.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;168&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;506&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;728&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capital&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;world&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;17.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;254&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1452&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;807&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;268&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;402&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;city&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;15.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;235&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1571&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;35&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;197&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;family&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;50.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;136&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;272&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;378&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adjective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adverb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;756&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;46&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;406&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opposite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;12.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;38&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;306&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;59&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;972&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comparative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;34.6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;436&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1260&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;58&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;865&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;superlative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;13.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;67&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;506&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;58&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;17&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;331&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;present&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;participle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;18.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;181&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;992&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;58&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;446&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nationality&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adjective&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;37.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;514&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1371&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;58&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;533&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;past&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;18.3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;244&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1332&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;58&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;55&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;660&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plural&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;30.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;992&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;59&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;508&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plural&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;20.3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;132&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;650&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;59&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;509&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;22.6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2766&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12234&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;OK. So far so good.&lt;/p&gt;

&lt;p&gt;You will notice that I did some more evaluation on this data, by testing it against the same dataset that Google released, to compute the sysntactic and semantic relationships between words. As text8 is a small dataset, we don’t expect it to achieve very high levels of accuracy on this task, however, it will help us discern the difference in learning words in an online manner, vs learning it all in one sitting. You can download the script that I ran from &lt;a href=&quot;http://rutumulkar.com/data/onlinew2v/word2vec_wrapper.py&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now lets update the model with all the sentences containing queen and see if the vector for $queen$ is similar to that of $king$ and $duke$. Notice that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;build_vocab&lt;/code&gt; function now has an additional argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update=True&lt;/code&gt; that add more words to the existing vocabulary.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sentences2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gensim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word2vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;LineSentence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;text8-queen&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;build_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentences2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentences2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Evaluation
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;n_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;king&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;duke&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.47693305301957223&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;n_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;king&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;queen&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.68197327708244115&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# text8-rest + text8-queen (using update model)
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;questions-words.txt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;571&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precomputing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norms&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;00&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;892&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capital&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;common&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;countries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;23.3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;118&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;506&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;583&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capital&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;world&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;14.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;205&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1452&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;521&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;268&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;348&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;city&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;13.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;208&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1571&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;349&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;family&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;46.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;142&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;306&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;891&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adjective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adverb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;756&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;34&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;925&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opposite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;13.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;41&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;306&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;631&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comparative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;32.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;408&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1260&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;52&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;768&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;superlative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;11.7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;59&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;506&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;831&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;present&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;participle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;18.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;179&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;992&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;823&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nationality&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adjective&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;35.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;483&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1371&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;937&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;past&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;17.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;228&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1332&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;960&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plural&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;26.8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;266&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;992&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;49&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;822&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plural&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;19.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;125&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;650&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;49&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;823&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;20.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2521&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12268&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;BINGO! Looks like it learned the weights of the vector $queen$ quite well.&lt;/p&gt;

&lt;p&gt;NOTE: &lt;em&gt;text8-rest&lt;/em&gt;, and &lt;em&gt;text8-queen&lt;/em&gt;, and &lt;em&gt;text8-all&lt;/em&gt; can be downloaded here: &lt;a href=&quot;http://rutumulkar.com/data/onlinew2v/text8-files.zip&quot;&gt;http://rutumulkar.com/data/onlinew2v/text8-files.zip&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is how the files are divided: All sentences from text8 that have &lt;em&gt;queen&lt;/em&gt; in them are in &lt;em&gt;text8-queen&lt;/em&gt;, and the remaining sentences are in &lt;em&gt;text8-rest&lt;/em&gt;. The file &lt;em&gt;text8-all&lt;/em&gt;, is a concatenation of &lt;em&gt;text8-rest&lt;/em&gt; and &lt;em&gt;text8-queen&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Here are the output accuracies that were achieve if we were to train the entire model in one go, as opposed to piecemeal in an online manner. Note that as the amount of data we are using is very little, the accuracy will vary a little due to the initialization parameters.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gensim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word2vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;LineSentence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;text8-all&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;build_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# text8-all
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;questions-words.txt&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;07&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;53&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;811&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precomputing&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norms&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;07&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;58&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;595&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capital&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;common&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;countries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;36.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;182&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;506&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;343&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;capital&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;world&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;18.9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;275&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1452&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;757&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currency&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;268&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;813&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;city&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;16.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;257&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1571&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;31&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;542&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;family&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;48.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;148&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;306&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;38&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;486&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adjective&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adverb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;6.9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;52&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;756&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;41&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;268&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opposite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;16.7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;51&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;306&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;52&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;507&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comparative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;34.4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;434&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1260&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;148&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;superlative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;12.8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;65&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;506&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;09&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;06&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;475&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;present&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;participle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;19.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;189&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;992&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;09&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;681&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nationality&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adjective&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;40.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;548&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1371&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;09&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;722&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;past&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;18.2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;243&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1332&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;09&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;39&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;516&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plural&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;32.7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;324&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;992&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;09&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;498&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gram9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plural&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;17.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;111&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;650&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2015&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;08&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;09&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;45&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;499&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;INFO&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;23.6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2892&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12268&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As you can see, the output score does drop a little, when the model is updated in an online manner, as opposed to training everything in one go. The PR for my code can be found here: &lt;a href=&quot;https://github.com/piskvorky/gensim/pull/435&quot;&gt;https://github.com/piskvorky/gensim/pull/435&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;###References:###
[1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. &lt;a href=&quot;http://arxiv.org/pdf/1301.3781v3.pdf&quot;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Aug 2015 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/word2vec/representation%20learning/2015/08/22/word2vec.html</link>
        <guid isPermaLink="true">http://localhost:4000/word2vec/representation%20learning/2015/08/22/word2vec.html</guid>
        
        <category>word2vec</category>
        
        
        <category>word2vec</category>
        
        <category>representation learning</category>
        
      </item>
    
      <item>
        <title>Understanding your Data - Basic Statistics</title>
        <description>&lt;p&gt;Have you ever had to deal with a lot of data, and don’t know where to start? If yes, then this post is for you. In this post I will try to guide you through some basic approaches and operations you can perform to analyze your data, make some basic sense of it, and decide on your approach for deeper analysis of it. I will use &lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;python&lt;/a&gt; and a &lt;a href=&quot;https://raw.githubusercontent.com/rutum/basic-statistics/master/bikesharing.csv&quot;&gt;small subset of  data&lt;/a&gt; from the &lt;a href=&quot;https://www.kaggle.com/c/bike-sharing-demand&quot;&gt;Kaggle Bikesharing Challenge&lt;/a&gt; to illustrate my examples. The code for this work can be found at &lt;a href=&quot;https://github.com/rutum/basic-statistics&quot;&gt;this location&lt;/a&gt;. Please take a minute to download &lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;python&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/rutum/basic-statistics/master/bikesharing.csv&quot;&gt;sample data&lt;/a&gt; before we proceed.&lt;/p&gt;

&lt;p&gt;Below are the topics that I discuss in this post:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#description-of-dataset&quot;&gt;DESCRIPTION OF DATASET&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#min-max-and-mean&quot;&gt;MIN MAX AND MEAN&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#variability-in-data&quot;&gt;VARIABILITY IN DATA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#variance&quot;&gt;VARIANCE&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#standard-deviation&quot;&gt;STANDARD DEVIATION&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#standard-error-of-the-mean-sem&quot;&gt;STANDARD ERROR OF THE MEAN (SEM)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;description-of-dataset&quot;&gt;DESCRIPTION OF DATASET&lt;/h2&gt;

&lt;p&gt;The data provided is a CSV file &lt;a href=&quot;https://raw.githubusercontent.com/rutum/basic-statistics/master/bikesharing.csv&quot;&gt;bikesharing.csv&lt;/a&gt;, with 5 columns - datetime, season, holiday, workingday, and count.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;datetime: The date and time when the statistics were captured&lt;/li&gt;
  &lt;li&gt;season: 1 = spring, 2 = summer, 3 = fall, 4 = winter&lt;/li&gt;
  &lt;li&gt;holiday: whether the day is considered a holiday&lt;/li&gt;
  &lt;li&gt;workingday: whether the day is neither a weekend nor holiday&lt;/li&gt;
  &lt;li&gt;count: the total number of bikes rented on that day&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;min-max-and-mean&quot;&gt;MIN MAX AND MEAN&lt;/h2&gt;

&lt;p&gt;One of the first analyses one can do with their data, is to find the minimum,  maximum and the mean. The mean (or average) number of bikes per day rented in this case is the sum of all bikes rented per day divided by the total number of days:&lt;/p&gt;

&lt;p&gt;$\bar{f} = \frac{\sum_{i=1}^{n} b_i}{n}$&lt;/p&gt;

&lt;p&gt;where $\bar{f}$ is the mean, $b_i$ is the number of bikes rented on day $i$ and $n$ are the total number of days. We can compute these in python using the following code:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;defaultdict&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;defaultdict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prevdate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;bikesharing.csv&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;csvreader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csvreader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csvreader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;date_object&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;strptime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;%m/%d/%y&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;currdate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;month&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;day&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Computing the total number of bikes rented in a day
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currdate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;iteritems&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2752&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#min
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13446&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#max
&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;9146.82&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#mean&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this case, the mean is 9146.82. It looks like there are several large values in the data, because the mean is closer to the max than to the min. Maybe the data will provive more insight if we compute the min, max and mean per day, grouped by another factor, like season or weather. Here is some code to compute mean per day grouped by the season:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;bikesharing.csv&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;csvreader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csvreader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csvreader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;date_object&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;strptime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;%m/%d/%y&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;currdate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;month&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;day&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currdate&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currdate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]].&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currdate&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])})&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currdate&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;iteritems&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;iteritems&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#season
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2752&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#min
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10580&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#max
&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5482.42105263&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#mean
&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;6252&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;13088&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;10320.7368421&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;7818&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;13446&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;11239.6842105&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;5713&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;12982&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;9544.45614035&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As you can see, the mean varies significantly with the season. It intuitively makes sense because we would expect more people to ride a bike in the summer as compared to the winter, which means a higher mean in the summer than the winter, this also means higher min and max values in the summer than the winter. This data also helps us intuitive guess that season 1, is most likely winter, and season 3 is most likely summer.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;variability-in-data&quot;&gt;VARIABILITY IN DATA&lt;/h2&gt;

&lt;p&gt;The next thing we would like to know, is the variability of the data provided. It is good to know if the data is skewed in a particular direction, or how varied it is. If the data is highly variable, it is hard to determine if the mean changes with different samples of data. Reducing variability is a common goal of designed experiments, and this can be done by finding subsets of data that have low variablity such that samples from each of the subsets produce similar mean value. We already did a little bit of that in the second example above.&lt;/p&gt;

&lt;p&gt;There are 2 ways of measuring variability: variance and standard deviation.&lt;/p&gt;

&lt;h2 id=&quot;variance&quot;&gt;VARIANCE&lt;/h2&gt;

&lt;p&gt;Variance is defined as the average of the squared differences from the mean. In most experiments, we take a random sample from a population. In this case, we will compute the population variance, which uses all possible data provided. Population variance can be computed as:&lt;/p&gt;

\[\sigma^2 = \frac{\sum_{i=1}^{n} (f_i - \bar{x})^2}{N}\]

&lt;p&gt;If you needed to compute the sample variance, you can use the following formula:&lt;/p&gt;

\[variance = \frac{\sum_{i=1}^{n} (f_i - \bar{x})^2}{N-1}\]

&lt;p&gt;where $x_i $ is each instance, $\bar{x}$ is the mean, and $N$ is the total number of features. Dividing by n−1 gives a better estimate of the population standard deviation for the larger parent population than dividing by n, which gives a result which is correct for the sample only. This is known as Bessel’s correction.&lt;/p&gt;

&lt;p&gt;In our case we will compute population variance using most of the same code as that above, except adding the following line to it:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#season
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2752&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#min
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10580&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#max
&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5482.42105263&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#mean
&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2812044.87535&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#variance
&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;6252&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;13088&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;11239.6842105&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;2953435.21145&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;7818&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;13446&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;11239.6842105&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;1368005.2687&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;5713&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;12982&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;9544.45614035&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;2552719.23053&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;standard-deviation&quot;&gt;STANDARD DEVIATION&lt;/h2&gt;

&lt;p&gt;Variance by itself is not particularly insightful, as its units are feature squared and it is not possible to plot it on a graph and compare it with the min, max and mean values. The square root of variance is the standard deviation, and it is a much more insightful metric.&lt;/p&gt;

&lt;p&gt;The population standard deviation, $\sigma$, is the square root of the variance, $\sigma^2$. In python you can compute variance by adding the following line to the above code:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;var&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;totals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#season
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2752&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#min
&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10580&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#max
&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5482.42105263&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#mean
&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1676.91528568&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#standard deviation
&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;6252&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;13088&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;10320.7368421&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;1718.55614149&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;7818&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;13446&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;11239.6842105&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;1169.6175737&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;5713&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;12982&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;9544.45614035&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;1597.72313951&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;A standard deviation close to 0 indicates that the data points tend to be very close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values. In our case, the data is very spread out. Three standard deviations from the mean account for 99.7% of the sample population being studied, assuming the distribution is normal (bell-shaped).&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;standard-error-of-the-mean-sem&quot;&gt;STANDARD ERROR OF THE MEAN (SEM)&lt;/h2&gt;

&lt;p&gt;In this post, we have computed the population mean, however, if one has to compute the sample mean, it is useful to know how accurate this value is in estimating the population mean. SEM is the error in estimating $\mu$.&lt;/p&gt;

\[SEM = \frac{\sigma}{\sqrt{N}}\]

&lt;p&gt;however, as we often are unable to compute the population standard deviation, we will use teh sample standard deviation instead:&lt;/p&gt;

\[SEM = \frac{s}{\sqrt{N}}\]

&lt;p&gt;The mean of any given sample is an estimate of the population mean number of features. Two aspects of the population and the sample could affect the variability of the mean number of features of those samples.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If the population of number of features has very small standard deviation, then the samples from that population will have small sample standard deviation the sample means will be close to the population mean and we will have a small standard error of the mean&lt;/li&gt;
  &lt;li&gt;If the population of number of features has a large standard deviation, then the samples from that population will have large sample standard deviation the sample means may be far from the population mean and we will have a large standard error of the mean&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So large population variability causes a large standard error of the mean. The estimate of the population mean using 2 observations is less reliable than the estimate using 20 observations, and much less reliable than the estimate of the mean using 100 observations. As N gets bigger, we expect our error in estimating the population mean to get smaller.&lt;/p&gt;
</description>
        <pubDate>Sun, 10 May 2015 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/statistics/2015/05/10/statistics-for-everyone.html</link>
        <guid isPermaLink="true">http://localhost:4000/statistics/2015/05/10/statistics-for-everyone.html</guid>
        
        <category>statistics</category>
        
        
        <category>statistics</category>
        
      </item>
    
  </channel>
</rss>
